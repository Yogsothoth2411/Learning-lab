{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8584346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import gc\n",
    "import evaluate\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm # 顯示進度條\n",
    "import trackio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "752d3acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # True 表示可以使用 GPU\n",
    "print(torch.cuda.get_device_name(0))  # 顯示 GPU 名稱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "828180ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 28 22:21:30 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 581.57                 Driver Version: 581.57         CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   46C    P8             18W /  130W |     818MiB /   8192MiB |      9%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            5456    C+G   ...App_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A            6520    C+G   ...al\\Programs\\Notion\\Notion.exe      N/A      |\n",
      "|    0   N/A  N/A            6808    C+G   ...IA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           10960    C+G   ...em32\\ApplicationFrameHost.exe      N/A      |\n",
      "|    0   N/A  N/A           11364    C+G   ...IA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           16124    C+G   ...Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A           19832    C+G   ...Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A           21576    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           22752    C+G   ...crosoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A           23228    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           24252    C+G   ...lpaper_engine\\wallpaper32.exe      N/A      |\n",
      "|    0   N/A  N/A           24712    C+G   ...ms\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A           27756    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           29912    C+G   ...2txyewy\\CrossDeviceResume.exe      N/A      |\n",
      "|    0   N/A  N/A           31656    C+G   ....0.3537.99\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           31712    C+G   ...ntrolPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A           32688    C+G   ...xyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           34332    C+G   ...Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A           34356    C+G   ...indows\\System32\\ShellHost.exe      N/A      |\n",
      "|    0   N/A  N/A           34556    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           37564    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           40812    C+G   ...cal\\LINE\\bin\\current\\LINE.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de9ccef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# --- 清理環境 --- #\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# --- 清理環境 --- #\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8745446c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source', 'messages', 'num_turns'],\n",
      "        num_rows: 15806\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source', 'messages', 'num_turns'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n",
      "Train samples: 15806\n",
      "Test samples: 200\n",
      "[{'content': 'Recommend a movie to watch.\\n', 'role': 'user'}, {'content': 'I would recommend the movie, \"The Shawshank Redemption\" which is a classic drama film starring Tim Robbins and Morgan Freeman. This film tells a powerful story about hope and resilience, as it follows the story of a young man who is wrongfully convicted of murder and sent to prison. Amidst the harsh realities of prison life, the protagonist forms a bond with a fellow inmate, and together they navigate the challenges of incarceration, while holding on to the hope of eventual freedom. This timeless movie is a must-watch for its moving performances, uplifting message, and unforgettable storytelling.', 'role': 'assistant'}, {'content': 'Describe the character development of Tim Robbins\\' character in \"The Shawshank Redemption\".', 'role': 'user'}, {'content': 'In \"The Shawshank Redemption\", Tim Robbins plays the character of Andy Dufresne, a banker who is wrongfully convicted of murdering his wife and her lover. Over the course of the film, we see a significant transformation in Andy\\'s character.\\n\\nAt the beginning of the movie, Andy is a quiet, reserved man who maintains his innocence but is largely passive in the face of his unjust conviction. He seems almost aloof, detached from the harsh reality of his situation. However, beneath this seemingly impassive exterior, Andy is a man of deep intelligence and inner strength.\\n\\nAs the story progresses, Andy begins to adapt to his new environment. He uses his banking skills to gain favor with the prison guards and the warden, providing them with financial advice and even helping them with tax evasion. This not only earns him certain privileges but also gives him a measure of protection within the prison walls.\\n\\nDespite the grim circumstances, Andy never loses hope. He befriends a fellow inmate, Red, and shares with him his dream of living a quiet life in Zihuatanejo, a small town on the Pacific coast of Mexico. This dream keeps him going and becomes a symbol of hope for other inmates as well.\\n\\nPerhaps the most significant development in Andy\\'s character comes towards the end of the film, when it is revealed that he has been digging a tunnel through his cell wall for the past 19 years. This act of defiance, carried out with nothing but a small rock hammer, is a testament to Andy\\'s resilience and determination. It also shows his cunning, as he manages to keep his plan a secret from everyone, including Red.\\n\\nIn the end, Andy\\'s character arc is one of hope, resilience, and redemption. Despite being placed in a situation that seems hopeless, he never gives up. He maintains his dignity, uses his intelligence to his advantage, and ultimately, manages to escape from Shawshank, proving his innocence in the process. His character serves as a powerful symbol of the human spirit\\'s ability to endure and overcome even the most challenging circumstances.', 'role': 'assistant'}, {'content': \"Explain the significance of the friendship between Andy and Red in shaping Andy's character development.\", 'role': 'user'}, {'content': 'The friendship between Andy Dufresne (Tim Robbins) and Ellis \"Red\" Redding (Morgan Freeman) is central to the narrative of \"The Shawshank Redemption\", and plays a significant role in shaping Andy\\'s character development.\\n\\nWhen Andy first arrives at Shawshank, he is a quiet, introverted man, largely keeping to himself. Red, who is the long-term inmate, takes an interest in him and they slowly develop a friendship. Red\\'s wisdom, street-smart attitude, and his ability to procure items become instrumental in Andy\\'s survival and eventual escape.\\n\\nRed is initially skeptical of Andy\\'s innocence and his hopeful outlook on life. However, as their friendship grows, Red becomes more receptive to Andy\\'s perspective. This friendship provides Andy with a confidant, a sounding board, and a supportive ally amidst the harsh realities of prison life. \\n\\nAndy\\'s influence on Red is equally profound. Andy\\'s unyielding hope and resilience slowly chip away at Red\\'s hardened cynicism. Andy shares his dreams of freedom and his plans for the future with Red, which initially seem unrealistic to Red, but over time, Andy\\'s unwavering belief in hope begins to influence Red\\'s outlook on life.\\n\\nIn many ways, their friendship is a beacon of hope and humanity in an otherwise oppressive environment. It\\'s through this friendship that Andy finds the strength to maintain his dignity, persevere, and ultimately, to engineer his daring escape. It\\'s also through this friendship that Red finds hope for redemption and a life beyond the prison walls.\\n\\nIn conclusion, the friendship between Andy and Red is a pivotal element in shaping Andy\\'s character development. It\\'s through this bond that Andy finds the strength to endure his unjust imprisonment and to hold onto hope, ultimately leading to his redemption.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./PEFT-OPT350M-Capybara-QLoRA-Demo\"\n",
    "# 資料集備選：\n",
    "# \"trl-lib/Capybara\" \n",
    "# \"google/boolq\"\n",
    "# \"Abzu/dolly_hhrlhf\"\n",
    "# \"stanfordnlp/imdb\"\n",
    "dataset_all = load_dataset(\"trl-lib/Capybara\")\n",
    "\n",
    "# 查看樣本數\n",
    "print(dataset_all)\n",
    "print(\"Train samples:\", len(dataset_all['train']))\n",
    "print(\"Test samples:\", len(dataset_all['test']))\n",
    "print(dataset_all['train'][0][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1271076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x00000283F49C7BA0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Learning-lab\\.venv\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"d:\\Learning-lab\\.venv\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "    ^^^^^^^^^\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/opt-350m\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# 模型 (量化為 4bit)\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,      # 或 load_in_8bit=True\n",
    "#     bnb_4bit_use_double_quant=True,  # optional: 提升 4-bit 精度\n",
    "#     bnb_4bit_quant_type=\"nf4\",       # nf4 或 fp4\n",
    "#     bnb_4bit_compute_dtype=torch.float16 # 計算用 dtype\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    dtype = torch.float16,\n",
    ")\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3180d700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
      "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model) # 查看模型結構，使用神奇妙妙工具微調時需要注意力層投影名稱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be06b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(prompts, pipeline_gen=None, batch_size=2):\n",
    "    outputs = []\n",
    "    # pipeline 批次生成\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Pipeline generating\"):\n",
    "        batch_prompts = prompts[i:i+batch_size]\n",
    "        batch_outputs = pipeline_gen(batch_prompts, max_new_tokens=100, num_return_sequences=1,truncation=True,do_sample=False)\n",
    "        for out in batch_outputs:\n",
    "            if isinstance(out, list):\n",
    "                outputs.append(out[0][\"generated_text\"])\n",
    "            else:\n",
    "                outputs.append(out[\"generated_text\"])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "965e2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for messages in examples[\"messages\"]:\n",
    "        full_text = \"\"\n",
    "        role_spans = []\n",
    "\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"].strip()\n",
    "\n",
    "            if role == \"user\":\n",
    "                prefix = \"User: \"\n",
    "                start_idx = len(full_text)\n",
    "                full_text += prefix + content + \"\\n\"\n",
    "                end_idx = len(full_text)\n",
    "                role_spans.append((start_idx, end_idx))\n",
    "            else:\n",
    "                full_text += \"Assistant: \" + content + \"\\n\"\n",
    "\n",
    "        # tokenize 並取得 offset\n",
    "        tokenized = tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        labels = input_ids.copy()\n",
    "\n",
    "        # mask user 區段\n",
    "        for start, end in role_spans:\n",
    "            for idx, (token_start, token_end) in enumerate(tokenized[\"offset_mapping\"]):\n",
    "                # 只對實際文字做 mask，跳過 padding\n",
    "                if token_start == token_end:\n",
    "                    continue\n",
    "                if token_start >= start and token_end <= end:\n",
    "                    labels[idx] = -100\n",
    "                if token_start < end and token_end > start:\n",
    "                    labels[idx] = -100\n",
    "\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        labels_list.append(labels)\n",
    "    assert len(input_ids) == 512\n",
    "    assert len(labels) == 512\n",
    "\n",
    "    return {\"input_ids\": input_ids_list, \"labels\": labels_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea284317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e615ebbec49d491b9b87ccd7f5ec5e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15806 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd914b71d2b46c59a197e8f6a002ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0366f8ffd04ee1a48d75356233bacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/15806 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbee70759194981ba7b49547346befc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering:\n",
      "Train samples: 4604\n",
      "Test samples: 60\n"
     ]
    }
   ],
   "source": [
    "max_len = 512\n",
    "\n",
    "dataset_all = load_dataset(\"trl-lib/Capybara\")\n",
    "\n",
    "def filter_long_samples(example):\n",
    "    # 把 messages 展平為純文字\n",
    "    full_text = \"\"\n",
    "    for msg in example[\"messages\"]:\n",
    "        role = msg[\"role\"]\n",
    "        content = msg[\"content\"].strip()\n",
    "        prefix = \"User: \" if role == \"user\" else \"Assistant: \"\n",
    "        full_text += prefix + content + \"\\n\"\n",
    "    # 用 tokenizer 計算長度\n",
    "    tokenized = tokenizer(full_text, truncation=False, add_special_tokens=False)\n",
    "    return len(tokenized[\"input_ids\"]) <= max_len\n",
    "\n",
    "# 用 .filter() 過濾整個資料集\n",
    "dataset_all = dataset_all.map(preprocess_function, batched=True)\n",
    "dataset_all = dataset_all.filter(filter_long_samples)\n",
    "\n",
    "print(\"After filtering:\")\n",
    "print(\"Train samples:\", len(dataset_all[\"train\"]))\n",
    "print(\"Test samples:\", len(dataset_all[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4df2a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data len: 3683\n",
      "validation data len: 921\n",
      "test data len: 60\n"
     ]
    }
   ],
   "source": [
    "train_val = dataset_all[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "dataset = {\n",
    "    \"train\": train_val[\"train\"],\n",
    "    \"validation\": train_val[\"test\"],\n",
    "    \"test\": dataset_all[\"test\"],\n",
    "}\n",
    "print(f\"train data len: {len(dataset['train'])}\")\n",
    "print(f\"validation data len: {len(dataset['validation'])}\")\n",
    "print(f\"test data len: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a81b198f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token ID: 50254\n",
      "Tokenizer vocab size: 50265\n"
     ]
    }
   ],
   "source": [
    "# 確認範圍，避免索引錯誤\n",
    "max_id = max([max(ids) for ids in dataset[\"train\"][\"input_ids\"]])\n",
    "print(\"Max token ID:\", max_id)\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb72e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalmodel(generator):\n",
    "    # 選擇評估指標\n",
    "    metric = evaluate.load(\"bertscore\")  # 也可換成 \"rouge\", \"bleu\"\n",
    "\n",
    "    prompts = []\n",
    "    labels = []\n",
    "\n",
    "    for example in dataset[\"test\"]:\n",
    "        messages = example['messages']\n",
    "        user_prompt = \"\".join([m['content']+\"\\n\" for m in messages if m['role']=='user'])\n",
    "        label = next((m['content'] for m in reversed(messages) if m['role']=='assistant'), \"\")\n",
    "        \n",
    "        prompts.append(user_prompt.strip())\n",
    "        labels.append(label.strip())\n",
    "\n",
    "    for i, p in enumerate(prompts):\n",
    "        tokenized = tokenizer(p, add_special_tokens=True)\n",
    "        if max(tokenized['input_ids']) >= tokenizer.vocab_size:\n",
    "            print(f\"Prompt {i} has token id out of range: {max(tokenized['input_ids'])}\")\n",
    "        if len(tokenized['input_ids']) > 512:\n",
    "            print(f\"Prompt {i} too long: {len(tokenized['input_ids'])} tokens\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # 選擇生成方式\n",
    "    outputs = generate_text(prompts, pipeline_gen=generator,batch_size=2)\n",
    "\n",
    "    predictions.extend(outputs)\n",
    "    references.extend(labels)\n",
    "\n",
    "    results = metric.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    print(results)\n",
    "    import numpy as np\n",
    "\n",
    "    precision_avg = np.mean(results['precision'])\n",
    "    recall_avg = np.mean(results['recall'])\n",
    "    f1_avg = np.mean(results['f1'])\n",
    "\n",
    "    print(f\"Precision: {precision_avg:.4f}\")\n",
    "    print(f\"Recall:    {recall_avg:.4f}\")\n",
    "    print(f\"F1:        {f1_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a545d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipeline generating:   0%|          | 0/30 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Pipeline generating:  33%|███▎      | 10/30 [00:19<00:31,  1.56s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Pipeline generating: 100%|██████████| 30/30 [00:49<00:00,  1.64s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': [0.8217451572418213, 0.8107905387878418, 0.830459713935852, 0.7366594076156616, 0.7564651966094971, 0.8497792482376099, 0.8497246503829956, 0.8837918043136597, 0.8247810006141663, 0.7553496360778809, 0.7829270362854004, 0.8639145493507385, 0.7495079040527344, 0.8070576190948486, 0.885560154914856, 0.710041344165802, 0.8720864057540894, 0.8701018691062927, 0.7506201267242432, 0.7543113827705383, 0.8730806112289429, 0.8401428461074829, 0.774517297744751, 0.8213115930557251, 0.746679425239563, 0.7796310782432556, 0.840653657913208, 0.8737272024154663, 0.7590190172195435, 0.7763035297393799, 0.8394687175750732, 0.863319456577301, 0.7249789237976074, 0.8152983784675598, 0.6698594093322754, 0.8195995092391968, 0.7384986877441406, 0.8610401153564453, 0.7867806553840637, 0.8155019283294678, 0.8218244314193726, 0.83235764503479, 0.7859389781951904, 0.8336321711540222, 0.7997800707817078, 0.8062293529510498, 0.818127453327179, 0.7522761821746826, 0.8106005191802979, 0.7734717130661011, 0.8380314111709595, 0.8286683559417725, 0.8435494899749756, 0.8592748641967773, 0.7755780220031738, 0.7732462882995605, 0.7636409997940063, 0.839716911315918, 0.7872263193130493, 0.8569767475128174], 'recall': [0.7982983589172363, 0.784578263759613, 0.8445214033126831, 0.8898712396621704, 0.9158660173416138, 0.817261815071106, 0.8220927119255066, 0.8375818729400635, 0.8282057046890259, 0.9177953004837036, 0.7749077677726746, 0.7773470282554626, 0.8980034589767456, 0.7949143052101135, 0.892020583152771, 0.8431175947189331, 0.8842922449111938, 0.8581238985061646, 0.7892200350761414, 0.9042904376983643, 0.837343692779541, 0.8110356330871582, 0.9160223007202148, 0.7473695278167725, 0.8388633728027344, 0.909168541431427, 0.832079291343689, 0.8439397811889648, 0.8301620483398438, 0.7939245700836182, 0.8396141529083252, 0.8292020559310913, 0.8862310647964478, 0.8165609240531921, 0.9185792207717896, 0.7713295221328735, 0.9107755422592163, 0.8559948205947876, 0.7564868927001953, 0.8375093936920166, 0.8106366395950317, 0.8466635942459106, 0.747658371925354, 0.8209749460220337, 0.8325492143630981, 0.7480934858322144, 0.9008091688156128, 0.8018059730529785, 0.8731316328048706, 0.8479132652282715, 0.7711087465286255, 0.8039203882217407, 0.7722631692886353, 0.8549811840057373, 0.7957485914230347, 0.9073128700256348, 0.7958067655563354, 0.8098469376564026, 0.9145174026489258, 0.847014844417572], 'f1': [0.8098520636558533, 0.7974690198898315, 0.8374314904212952, 0.8060493469238281, 0.8285688757896423, 0.8332033753395081, 0.835680365562439, 0.8600666522979736, 0.8264897465705872, 0.828686535358429, 0.7788967490196228, 0.8183478116989136, 0.8170635104179382, 0.8009399771690369, 0.8887786865234375, 0.7708784341812134, 0.8781469464302063, 0.8640714287757874, 0.769436240196228, 0.8225200176239014, 0.8548388481140137, 0.8253326416015625, 0.8393474817276001, 0.7825978398323059, 0.7900915741920471, 0.8394318222999573, 0.8363444805145264, 0.8585752248764038, 0.7929980754852295, 0.7850151658058167, 0.8395414352416992, 0.8459168672561646, 0.7975358366966248, 0.8159291744232178, 0.7747468948364258, 0.7947322130203247, 0.8156394362449646, 0.8585100769996643, 0.7713364958763123, 0.826359212398529, 0.8161922097206116, 0.8394497036933899, 0.7663209438323975, 0.8272551894187927, 0.81583571434021, 0.776074230670929, 0.8574797511100769, 0.7762517929077148, 0.8407049179077148, 0.8089836239814758, 0.8031784296035767, 0.8161068558692932, 0.8063338398933411, 0.8571226596832275, 0.7855339050292969, 0.8349319696426392, 0.7793921232223511, 0.8245114684104919, 0.8461111783981323, 0.8519666790962219], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.57.1)'}\n",
      "Precision: 0.8064\n",
      "Recall:    0.8360\n",
      "F1:        0.8191\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "evalmodel(generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75b56414",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=output_path,\n",
    "    num_train_epochs=2,\n",
    "    max_length=512,\n",
    "    per_device_train_batch_size=4,# 每個 GPU (或 CPU) 裝置的訓練批次大小。\n",
    "    gradient_accumulation_steps=2,# 梯度累積步數。在不增加實際批次大小的情況下，模擬更大的批次。\n",
    "    logging_steps=10,# 每格多少step打印log，記錄一次訓練日誌 (例如損失值、學習率)\n",
    "    learning_rate=2e-4,# 小學習率，1e-5到5e-5\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,   # 如果 GPU 支援\n",
    "    push_to_hub=False,\n",
    "    hub_model_id=None,\n",
    "    hub_strategy=\"end\",\n",
    "    report_to=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a14174d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learning-lab\\.venv\\Lib\\site-packages\\peft\\mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "d:\\Learning-lab\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OPTForCausalLM(\n",
       "      (model): OPTModel(\n",
       "        (decoder): OPTDecoder(\n",
       "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "          (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x OPTDecoderLayer(\n",
       "              (self_attn): OPTAttention(\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,               # rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],  # 根據模型架構選\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # 也可傳入 TaskType物件\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, config)\n",
    "peft_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9fa8bdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model, # 傳入基礎模型\n",
    "    train_dataset=dataset[\"train\"], # 傳入訓練資料集 (Capybara)\n",
    "    eval_dataset=dataset[\"validation\"], # 傳入驗證資料集\n",
    "    args=training_args, # 傳入 SFTConfig 訓練參數\n",
    "    peft_config=config, # 傳入 LoRA 參數\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bbd3d89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== train start ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='922' max='922' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [922/922 17:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.797800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.745300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.855800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.774700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.680800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.908200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.756600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.839300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.705200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.659800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.670900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.674200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.638100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.676300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.736200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.763100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.787300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.840800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.517100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.663600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.689500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.795500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.864500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.757500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.637800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.592300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.785400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.630100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.582500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.567200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.744700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.787100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.639400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.728100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.772400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.797900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.683700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.701200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.755400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.825400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.725800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.585600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.781600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.577600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.755600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.591700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.591900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.759500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.663900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.713100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.683600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.807000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.905100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.779100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.599000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.669400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.600200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.581700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.627600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.676500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.812300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.669100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.710600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.547900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.585900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.649600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.664100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.532300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.596900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.654200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.622800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.663800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.844400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.788100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.740600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.546100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.670200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.609700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== train end & save model ==\n"
     ]
    }
   ],
   "source": [
    "print(\"== train start ==\")\n",
    "trainer.train()\n",
    "print(\"== train end & save model ==\")\n",
    "trainer.save_model(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b1c7eff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learning-lab\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合併後的模型儲存至./PEFT-OPT350M-Capybara-QLoRA-Demo。\n"
     ]
    }
   ],
   "source": [
    "# 從保存的路徑載入 LoRA 適配器\n",
    "lora_model = PeftModel.from_pretrained(model, output_path)\n",
    "\n",
    "# 將 LoRA 適配器與基礎模型權重合併，生成一個可獨立部署的模型\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# 儲存合併後的模型\n",
    "merged_model.save_pretrained(output_path)\n",
    "tokenizer.save_pretrained(output_path)\n",
    "print(f\"合併後的模型儲存至{output_path}。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "09a1c340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# --- 清理環境 --- #\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# --- 清理環境 --- #\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "60908f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x00000283F49C7BA0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Learning-lab\\.venv\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"d:\\Learning-lab\\.venv\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "    ^^^^^^^^^\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(output_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    output_path,\n",
    "    device_map=\"auto\",\n",
    "    dtype = torch.float16\n",
    ")\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a37afa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipeline generating:   0%|          | 0/30 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Pipeline generating: 100%|██████████| 30/30 [01:30<00:00,  3.00s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': [0.8370808362960815, 0.8342282176017761, 0.8342353105545044, 0.741121232509613, 0.7671545743942261, 0.8469597101211548, 0.8335655927658081, 0.8034133911132812, 0.8425979614257812, 0.7576103210449219, 0.8257039189338684, 0.839861273765564, 0.7483733296394348, 0.797203779220581, 0.865003228187561, 0.7870572805404663, 0.8556906580924988, 0.8352280855178833, 0.7687060236930847, 0.7582086324691772, 0.8459712862968445, 0.841443657875061, 0.7247333526611328, 0.8153818845748901, 0.7567036747932434, 0.772638201713562, 0.8307317495346069, 0.8273088932037354, 0.7825288772583008, 0.8128316402435303, 0.8844503164291382, 0.8146368265151978, 0.7249789237976074, 0.7950693964958191, 0.6846445202827454, 0.7938662767410278, 0.7288016080856323, 0.8580649495124817, 0.8054811954498291, 0.7829751968383789, 0.8642539978027344, 0.8059259653091431, 0.7803385257720947, 0.8263607025146484, 0.7688013315200806, 0.8171194791793823, 0.818127453327179, 0.7522761821746826, 0.8203299045562744, 0.7848988771438599, 0.8354739546775818, 0.8282967209815979, 0.8305568695068359, 0.8542743921279907, 0.8122915625572205, 0.7793517112731934, 0.7658349275588989, 0.8198325037956238, 0.7665749788284302, 0.8043359518051147], 'recall': [0.8088719248771667, 0.7942933440208435, 0.8377187252044678, 0.8871634006500244, 0.9096582531929016, 0.8076332807540894, 0.8095787167549133, 0.8161372542381287, 0.8298817276954651, 0.9138559699058533, 0.7703749537467957, 0.7772212028503418, 0.8972641229629517, 0.7913601398468018, 0.8897913098335266, 0.8155361413955688, 0.8801006078720093, 0.8551012277603149, 0.8261265158653259, 0.9154289364814758, 0.8238471150398254, 0.8097862005233765, 0.9139710664749146, 0.7622494101524353, 0.8410605192184448, 0.9098070859909058, 0.8300595879554749, 0.8365241289138794, 0.840550422668457, 0.8028336763381958, 0.8425552248954773, 0.824122428894043, 0.8862310647964478, 0.8159012794494629, 0.9155155420303345, 0.7748435139656067, 0.9142742156982422, 0.8604922294616699, 0.7668043375015259, 0.8318928480148315, 0.8186882138252258, 0.8409652709960938, 0.74295973777771, 0.8166382312774658, 0.8350719213485718, 0.7470916509628296, 0.9008091688156128, 0.8018059730529785, 0.876889705657959, 0.8256245851516724, 0.7719436287879944, 0.8040585517883301, 0.7710011601448059, 0.8516553640365601, 0.7980104684829712, 0.9140214920043945, 0.7985676527023315, 0.801267683506012, 0.912480354309082, 0.83536696434021], 'f1': [0.8227346539497375, 0.81377112865448, 0.8359734416007996, 0.8075929284095764, 0.8323510885238647, 0.8268291354179382, 0.8213970065116882, 0.8097253441810608, 0.8361915349960327, 0.8284302949905396, 0.797080397605896, 0.8073280453681946, 0.8160831332206726, 0.7942712306976318, 0.8772221207618713, 0.801043689250946, 0.8677240014076233, 0.8450478315353394, 0.7963826060295105, 0.8294342160224915, 0.834762692451477, 0.8253114223480225, 0.8084256649017334, 0.7879209518432617, 0.7966551780700684, 0.8356310129165649, 0.8303954601287842, 0.8318910002708435, 0.8105025291442871, 0.8078017830848694, 0.8629946708679199, 0.8193522095680237, 0.7975358366966248, 0.805350661277771, 0.783424973487854, 0.7842395305633545, 0.811069667339325, 0.8592768907546997, 0.7856670618057251, 0.8066930770874023, 0.840854287147522, 0.8230729103088379, 0.7611905932426453, 0.8214707374572754, 0.8005675673484802, 0.780538022518158, 0.8574797511100769, 0.7762517929077148, 0.8476673364639282, 0.8047468662261963, 0.8024532794952393, 0.8159977197647095, 0.7996717095375061, 0.852962851524353, 0.805087685585022, 0.8413316607475281, 0.7818588614463806, 0.8104438185691833, 0.8331882357597351, 0.8195578455924988], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.57.1)'}\n",
      "Precision: 0.8033\n",
      "Recall:    0.8350\n",
      "F1:        0.8171\n"
     ]
    }
   ],
   "source": [
    "# peft + sft\n",
    "evalmodel(generator=generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
