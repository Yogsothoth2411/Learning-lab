{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01324e3",
   "metadata": {},
   "source": [
    "---\n",
    "**LangPDFDemo**\n",
    "- 使用 LangChain UnstructuredPDFLoader 解析 PDF\n",
    "- 分類、逐一打印元素\n",
    "- 適合作為個人練習範例\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4f0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = r\"D:\\Learning-lab\\Test\\sample.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc1c3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "pdf_loader = UnstructuredPDFLoader(\n",
    "    pdf_path,\n",
    "    mode=\"elements\"   # 或 \"single_page\"\n",
    ")\n",
    "\n",
    "pdf_docs = pdf_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d29d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共讀取 359 份 Document\n"
     ]
    }
   ],
   "source": [
    "print(f\"共讀取 {len(pdf_docs)} 份 Document\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6112bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Document 1 ---\n",
      "2510.12323v1 [cs.AI] 14 Oct 2025\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(51.0), np.float64(631.0)), (np.float64(51.0), np.float64(1429.0)), (np.float64(99.0), np.float64(1429.0)), (np.float64(99.0), np.float64(631.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 1, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'UncategorizedText', 'element_id': 'dc03f3ec8b6688c2ff97432265348f10'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 2 ---\n",
      "arXiv\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(50.0), np.float64(1447.0)), (np.float64(50.0), np.float64(1571.0)), (np.float64(88.0), np.float64(1571.0)), (np.float64(88.0), np.float64(1447.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 1, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '664b9e652a6cb385de964b1b4e2316f0'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 3 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 1, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'd4e5875a04d5bedb6583c4272501d52b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 4 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(302.0), np.float64(227.0)), (np.float64(302.0), np.float64(260.0)), (np.float64(1349.0), np.float64(260.0)), (np.float64(1349.0), np.float64(227.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 1, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '924a103911b5587bc30a644ba4c2d006'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 5 ---\n",
      "Zirui Guo, Xubin Ren, Lingrui Xu, Jiahao Zhang, Chao Huang* The University of Hong Kong zrguol01@hku.hk xubinrencs@gmail.com chaohuang75@gmail.com\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(317.0), np.float64(326.0)), (np.float64(317.0), np.float64(411.0)), (np.float64(1197.0), np.float64(411.0)), (np.float64(1197.0), np.float64(326.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 1, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '924a103911b5587bc30a644ba4c2d006', 'category': 'UncategorizedText', 'element_id': '4f73ce674e1583e4704a5027b6eabfbf'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 6 ---\n",
      "ABSTRACT\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(774.0), np.float64(499.0)), (np.float64(774.0), np.float64(521.0)), (np.float64(927.0), np.float64(521.0)), (np.float64(927.0), np.float64(499.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 1, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '5703c61e76adf8c653e5b0718deb84a8'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 7 ---\n",
      "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding Large Language Models beyond their static training limitations. However, a critical misalignment exists between current RAG capabilities and real-world information environments. Modern knowledge repositories are inher- ently multimodal, containing rich combinations of textual content, visual elements, structured tables, and mathematical expressions. Yet existing RAG frameworks are limited to textual content,\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(400.0), np.float64(567.0)), (np.float64(400.0), np.float64(1201.0)), (np.float64(1304.0), np.float64(1201.0)), (np.float64(1304.0), np.float64(567.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 1, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '5703c61e76adf8c653e5b0718deb84a8', 'category': 'NarrativeText', 'element_id': '17f87a681e99ef0b639ea65b18a9ecfc'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 8 ---\n",
      "1 INTRODUCTION\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(305.0), np.float64(1264.0)), (np.float64(305.0), np.float64(1286.0)), (np.float64(572.0), np.float64(1286.0)), (np.float64(572.0), np.float64(1264.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 1, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'e9e0e39686016b6b04bcb06f4647a03a'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 9 ---\n",
      "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding the knowledge boundaries of Large Language Models (LLM) beyond their static training limita- tions Zhang et al. (2025). By enabling dynamic retrieval and incorporation of external knowledge during inference, RAG systems transform static language models into adaptive, knowledge-aware systems. This capability has proven essential for applications requiring up-to-date information, domain-specific knowledge, or \n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1332.0)), (np.float64(301.0), np.float64(1509.0)), (np.float64(1403.0), np.float64(1509.0)), (np.float64(1403.0), np.float64(1332.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 1, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'e9e0e39686016b6b04bcb06f4647a03a', 'category': 'NarrativeText', 'element_id': '09c7f95ecdc4cbf1af1416b0a4ccfa95'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 10 ---\n",
      "However, existing RAG frameworks focus exclusively on text-only knowledge while neglecting the rich multimodal information present in real-world documents. This limitation fundamentally mis- aligns with how information exists in authentic environments. Real-world knowledge repositories are inherently heterogeneous and multimodal Abootorabi et al. (2025). They contain rich combinations of textual content, visual elements, structured tables, and mathematical expressions across diverse document for\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1531.0)), (np.float64(300.0), np.float64(1739.0)), (np.float64(1403.0), np.float64(1739.0)), (np.float64(1403.0), np.float64(1531.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 1, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'e9e0e39686016b6b04bcb06f4647a03a', 'category': 'NarrativeText', 'element_id': '320f9fba950255a84417a5f68b93a3e9'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 11 ---\n",
      "The consequences of this limitation become particularly severe in document-intensive domains where multimodal content carries essential meaning. Academic research, financial analysis, and technical documentation represent prime examples of knowledge-rich environments. These domains fundamentally depend on visual and structured information. Critical insights are often encoded exclusively in non-textual formats. Such formats resist meaningful conversion to plain text.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1761.0)), (np.float64(299.0), np.float64(1908.0)), (np.float64(1399.0), np.float64(1908.0)), (np.float64(1399.0), np.float64(1761.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 1, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'e9e0e39686016b6b04bcb06f4647a03a', 'category': 'NarrativeText', 'element_id': '9a62c789f7ca3f0dc45b4e4e0b4064c5'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 12 ---\n",
      "The consequences of this limitation become particularly severe in knowledge-intensive domains where multimodal content carries essential meaning. Three representative scenarios illustrate the critical\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1930.0)), (np.float64(299.0), np.float64(1985.0)), (np.float64(1399.0), np.float64(1985.0)), (np.float64(1399.0), np.float64(1930.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 1, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'e9e0e39686016b6b04bcb06f4647a03a', 'category': 'NarrativeText', 'element_id': '4c2bcf65a870272f8bb60058f69bd51f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 13 ---\n",
      "Corresponding Author: Chao Huang\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(335.0), np.float64(2009.0)), (np.float64(335.0), np.float64(2032.0)), (np.float64(709.0), np.float64(2032.0)), (np.float64(709.0), np.float64(2009.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 1, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'e9e0e39686016b6b04bcb06f4647a03a', 'category': 'ListItem', 'element_id': 'cb12892cee53253b856fc35ca4a965d3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 14 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 2, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '8e2659b4bf0cc8eef82502891786e3ee'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 15 ---\n",
      "need for multimodal RAG capabilities. In Scientific Research, experimental results are primarily communicated through plots, diagrams, and statistical visualizations. These contain core discoveries that remain invisible to text-only systems. Financial Analysis relies heavily on market charts, correlation matrices, and performance tables. Investment insights are encoded in visual patterns rather than textual descriptions. Additionally, Medical Literature Analysis depends on radiological images, d\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(236.0)), (np.float64(300.0), np.float64(565.0)), (np.float64(1403.0), np.float64(565.0)), (np.float64(1403.0), np.float64(236.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 2, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8e2659b4bf0cc8eef82502891786e3ee', 'category': 'NarrativeText', 'element_id': '69672483af51860dc885f0fa22f6e818'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 16 ---\n",
      "Addressing multimodal RAG presents three fundamental technical challenges that demand principled solutions. This makes it significantly more complex than traditional text-only approaches. The naive solution of converting all multimodal content to textual descriptions introduces severe information loss. Visual elements such as charts, diagrams, and spatial layouts contain semantic richness that cannot be adequately captured through text alone. These inherent limitations necessitate the design of \n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(587.0)), (np.float64(300.0), np.float64(795.0)), (np.float64(1401.0), np.float64(795.0)), (np.float64(1401.0), np.float64(587.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 2, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8e2659b4bf0cc8eef82502891786e3ee', 'category': 'NarrativeText', 'element_id': '9f49b1d1ca6f0a44959610b37ba10ddd'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 17 ---\n",
      "Technical Challenges. e First, the unified multimodal representation challenge requires seam- lessly integrating diverse information types. The system must preserve their unique characteristics and cross-modal relationships. This demands advanced multimodal encoders that can capture both intra-modal and inter-modal dependencies without losing essential visual semantics. e Second, the structure-aware decomposition challenge demands intelligent parsing of complex layouts. The system must maintain \n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(817.0)), (np.float64(300.0), np.float64(1207.0)), (np.float64(1403.0), np.float64(1207.0)), (np.float64(1403.0), np.float64(817.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 2, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8e2659b4bf0cc8eef82502891786e3ee', 'category': 'NarrativeText', 'element_id': '550a9c26520b7499a98ad317a7385831'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 18 ---\n",
      "Our Contributions. To address these challenges, we introduce RAG-Anything, a unified framework that fundamentally reimagines multimodal knowledge representation and retrieval. Our approach employs a dual-graph construction strategy that elegantly bridges the gap between cross-modal understanding and fine-grained textual semantics. Rather than forcing diverse modalities into text- centric pipelines, RAG-Anything constructs complementary knowledge graphs that preserve both multimodal contextual re\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1229.0)), (np.float64(301.0), np.float64(1498.0)), (np.float64(1403.0), np.float64(1498.0)), (np.float64(1403.0), np.float64(1229.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 2, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8e2659b4bf0cc8eef82502891786e3ee', 'category': 'NarrativeText', 'element_id': '5d0150982d223b5e5aab9aa809672a09'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 19 ---\n",
      "Our cross-modal hybrid retrieval mechanism strategically combines structural knowledge nav- igation with semantic similarity matching. This architecture addresses the fundamental limita- tion of existing approaches that rely solely on embedding-based retrieval or keyword matching. RAG-Anything leverages explicit graph relationships to capture multi-hop reasoning patterns. It simultaneously employs dense vector representations to identify semantically relevant content that lacks direct structural\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1520.0)), (np.float64(301.0), np.float64(1819.0)), (np.float64(1403.0), np.float64(1819.0)), (np.float64(1403.0), np.float64(1520.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 2, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8e2659b4bf0cc8eef82502891786e3ee', 'category': 'NarrativeText', 'element_id': 'fe5270f272515b06a7dc8a4209258cad'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 20 ---\n",
      "Experimental Validation. To validate the effectiveness of our proposed approach, we conduct com- prehensive experiments on two challenging multimodal benchmarks: DocBench and MMLongBench. Our evaluation demonstrates that RAG-Anything achieves superior performance across diverse do- mains. The framework represents substantial improvements over state-of-the-art baselines. Notably, our performance gains become increasingly significant as content length increases. We observe particularly pronounced \n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1841.0)), (np.float64(300.0), np.float64(2018.0)), (np.float64(1403.0), np.float64(2018.0)), (np.float64(1403.0), np.float64(1841.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 2, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8e2659b4bf0cc8eef82502891786e3ee', 'category': 'NarrativeText', 'element_id': 'cf88fee477a179db6998f88ffaf4396f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 21 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '823690bbae8a4948e499ee65f7f81c1e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 22 ---\n",
      "that dual-graph construction and cross-modal hybrid retrieval are essential for handling complex multimodal materials. Our ablation studies reveal that graph-based knowledge representation provides the primary performance gains. Traditional chunk-based approaches fail to capture the structural relationships critical for multimodal reasoning. Case studies further demonstrate that our framework excels at precise localization within complex layouts. The system effectively disambiguates similar term\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(236.0)), (np.float64(300.0), np.float64(413.0)), (np.float64(1400.0), np.float64(413.0)), (np.float64(1400.0), np.float64(236.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '823690bbae8a4948e499ee65f7f81c1e', 'category': 'NarrativeText', 'element_id': 'da48798e114bc2d10fb91aeae26018ac'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 23 ---\n",
      "2 THE RAG-ANYTHING FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(302.0), np.float64(465.0)), (np.float64(302.0), np.float64(489.0)), (np.float64(863.0), np.float64(489.0)), (np.float64(863.0), np.float64(465.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '3ad738ceeb20cb443698ff26866a2d82'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 24 ---\n",
      "2.1 PRELIMINARY\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(535.0)), (np.float64(301.0), np.float64(554.0)), (np.float64(532.0), np.float64(554.0)), (np.float64(532.0), np.float64(535.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '9213f609ab9f4ae4672c8da410024a3e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 25 ---\n",
      "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for dynamically expanding the knowledge boundaries of LLMs. While LLMs demonstrate exceptional reasoning capabilities, their knowledge remains static and bounded by training data cutoffs. This creates an ever-widening gap with the rapidly evolving information landscape. RAG systems address this critical limitation by enabling LLMs to retrieve and incorporate external knowledge sources during inference. This transforms the\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(593.0)), (np.float64(299.0), np.float64(770.0)), (np.float64(1403.0), np.float64(770.0)), (np.float64(1403.0), np.float64(593.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '9213f609ab9f4ae4672c8da410024a3e', 'category': 'NarrativeText', 'element_id': 'f53b4b7aba2705f66a1c169a2d0fa375'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 26 ---\n",
      "The Multimodal Reality: Beyond Text-Only RAG. Current RAG systems face a critical limitation that severely restricts their real-world deployment. Existing frameworks operate under the restrictive assumption that knowledge corpus consists exclusively of plain textual documents. This assump- tion fundamentally misaligns with how information exists in authentic environments. Real-world knowledge repositories are inherently heterogeneous and multimodal, containing rich combinations of textual conten\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(788.0)), (np.float64(300.0), np.float64(1030.0)), (np.float64(1403.0), np.float64(1030.0)), (np.float64(1403.0), np.float64(788.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '9213f609ab9f4ae4672c8da410024a3e', 'category': 'NarrativeText', 'element_id': 'd87626671f781ca25d474490cb373e04'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 27 ---\n",
      "2.1.1 MOTIVATING RAG-ANYTHING\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1072.0)), (np.float64(301.0), np.float64(1091.0)), (np.float64(753.0), np.float64(1091.0)), (np.float64(753.0), np.float64(1072.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '593762819387ede01d88271c42ac0598'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 28 ---\n",
      "This multimodal reality introduces fundamental technical challenges that expose the inadequacy of current text-only RAG approaches. Effective multimodal RAG requires unified indexing strategies that can handle disparate data types, cross-modal retrieval mechanisms that preserve semantic relationships across modalities, and sophisticated synthesis techniques that can coherently integrate diverse information sources. These challenges demand a fundamentally different architectural approach rather t\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1125.0)), (np.float64(299.0), np.float64(1303.0)), (np.float64(1401.0), np.float64(1303.0)), (np.float64(1401.0), np.float64(1125.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '593762819387ede01d88271c42ac0598', 'category': 'NarrativeText', 'element_id': '4066aec75c589d5b4dd2184a7abbae36'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 29 ---\n",
      "The RAG-Anything framework introduces a unified approach for retrieving and processing knowl- edge from heterogeneous multimodal information sources. Our system addresses the fundamental challenge of handling diverse data modalities and document formats within a retrieval pipeline. The framework comprises three core components: universal indexing for multimodal knowledge, cross-modal adaptive retrieval, and knowledge-enhanced response generation. This integrated design enables effective knowledg\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1325.0)), (np.float64(299.0), np.float64(1502.0)), (np.float64(1403.0), np.float64(1502.0)), (np.float64(1403.0), np.float64(1325.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '593762819387ede01d88271c42ac0598', 'category': 'NarrativeText', 'element_id': '716c6c4d95ac470f907c299c703e02a9'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 30 ---\n",
      "2.2 UNIVERSAL REPRESENTATION FOR HETEROGENEOUS KNOWLEDGE\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1547.0)), (np.float64(301.0), np.float64(1566.0)), (np.float64(1162.0), np.float64(1566.0)), (np.float64(1162.0), np.float64(1547.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '68480ee3c1c520acd1da3ca6a144e614'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 31 ---\n",
      "A key requirement for universal knowledge access is the ability to represent heterogeneous multimodal content in a unified, retrieval-oriented abstraction. Unlike existing pipelines that simply parse documents into text segments, RAG-Anything introduces Multimodal Knowledge Unification. This process decomposes raw inputs into atomic knowledge units while preserving their structural context and semantic alignment. For instance, RAG-Anything ensures that figures remain grounded in their captions, \n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1605.0)), (np.float64(300.0), np.float64(1812.0)), (np.float64(1400.0), np.float64(1812.0)), (np.float64(1400.0), np.float64(1605.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '68480ee3c1c520acd1da3ca6a144e614', 'category': 'NarrativeText', 'element_id': '062a75330597da2122a1adc114671d67'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 32 ---\n",
      "Formally, each knowledge source k; € K (e.g., a web page) is decomposed into atomic content units:\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1834.0)), (np.float64(301.0), np.float64(1860.0)), (np.float64(1401.0), np.float64(1860.0)), (np.float64(1401.0), np.float64(1834.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '68480ee3c1c520acd1da3ca6a144e614', 'category': 'NarrativeText', 'element_id': 'bec18fd0ff11432f7ba2b6809d2dcb26'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 33 ---\n",
      "{ej = (tj, ey) FE ()\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(831.0), np.float64(1890.0)), (np.float64(831.0), np.float64(1933.0)), (np.float64(1400.0), np.float64(1933.0)), (np.float64(1400.0), np.float64(1890.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '04c2419781c73b1b3ded936b75238365'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 34 ---\n",
      "where each unit c; consists of a modality type t; € text, image, table, equation, ... and its corre- sponding raw content x;. The content «x; represents the extracted information from the original knowledge source, processed in a modality-aware manner to preserve semantic integrity.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1947.0)), (np.float64(300.0), np.float64(2033.0)), (np.float64(1403.0), np.float64(2033.0)), (np.float64(1403.0), np.float64(1947.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '04c2419781c73b1b3ded936b75238365', 'category': 'NarrativeText', 'element_id': 'ca89f57425bd56a521d5c1a1c92f1fc5'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 35 ---\n",
      "Decompose\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(705.0), np.float64(1885.0)), (np.float64(705.0), np.float64(1902.0)), (np.float64(796.0), np.float64(1902.0)), (np.float64(796.0), np.float64(1885.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'a2da8bcca5a8d7979040b0f3e0e3dae1'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 36 ---\n",
      "ky\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(654.0), np.float64(1896.0)), (np.float64(654.0), np.float64(1919.0)), (np.float64(674.0), np.float64(1919.0)), (np.float64(674.0), np.float64(1896.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 3, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '81f042922f0699fefb2522c657d755d6'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 37 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '8423e78b336b47fed6129db5bf96a23b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 38 ---\n",
      ": { Multimodal Knowledge Unification ' i Dual-Graph Construction for Multimodal Knowledge ' bQuery ------ 2 \\ ' ' (each document) ' Could you share insights ti === == Knowledge Graph »=--- =~ \\ '\\ onthe experimental |\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(256.0)), (np.float64(383.0), np.float64(313.0)), (np.float64(1303.0), np.float64(313.0)), (np.float64(1303.0), np.float64(256.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8423e78b336b47fed6129db5bf96a23b', 'category': 'NarrativeText', 'element_id': '93228707c96f82d84c2f287677895632'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 39 ---\n",
      "G8) Structured Content List ( ‘ . ! Knowledge Graph g MATT 77 Baron TT Dace Hierarchical Text 5 Be Extraction Fy\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(390.0), np.float64(310.0)), (np.float64(390.0), np.float64(395.0)), (np.float64(979.0), np.float64(395.0)), (np.float64(979.0), np.float64(310.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8423e78b336b47fed6129db5bf96a23b', 'category': 'NarrativeText', 'element_id': '853ec361f69f300865997a58897a2524'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 40 ---\n",
      "Gras oege\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(853.0), np.float64(428.0)), (np.float64(853.0), np.float64(438.0)), (np.float64(953.0), np.float64(438.0)), (np.float64(953.0), np.float64(428.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '03580073ce676c7720eb4732f19ed743'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 41 ---\n",
      "mage Caption & Graph\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(443.0), np.float64(426.0)), (np.float64(443.0), np.float64(449.0)), (np.float64(878.0), np.float64(449.0)), (np.float64(878.0), np.float64(426.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '5cfe33dfafa40d2e4cc160651e9ed802'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 42 ---\n",
      "‘Metadata Extraction\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(443.0), np.float64(440.0)), (np.float64(443.0), np.float64(450.0)), (np.float64(553.0), np.float64(450.0)), (np.float64(553.0), np.float64(440.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'bc37a22ffc3f64a5e27fa3f5ce514b7b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 43 ---\n",
      "e Structural Knowledge Negation\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(1184.0), np.float64(418.0)), (np.float64(1184.0), np.float64(455.0)), (np.float64(1275.0), np.float64(455.0)), (np.float64(1275.0), np.float64(418.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '25ffc2200b4806bc270ca4d2c909d568'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 44 ---\n",
      "Multi-modal Processors\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(660.0), np.float64(453.0)), (np.float64(660.0), np.float64(476.0)), (np.float64(722.0), np.float64(476.0)), (np.float64(722.0), np.float64(453.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '9583e9b25f2ba60e5a295db20ed0893d'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 45 ---\n",
      ": vu\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(662.0), np.float64(468.0)), (np.float64(662.0), np.float64(510.0)), (np.float64(950.0), np.float64(510.0)), (np.float64(950.0), np.float64(468.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '06009ee29710ede59733130ca6718435'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 46 ---\n",
      "tet vDB\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(909.0), np.float64(562.0)), (np.float64(909.0), np.float64(578.0)), (np.float64(973.0), np.float64(578.0)), (np.float64(973.0), np.float64(562.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '138442fa4bdacb7a79140742ac07d559'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 47 ---\n",
      "ogur panaiyay plug,\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(1284.0), np.float64(431.0)), (np.float64(1284.0), np.float64(558.0)), (np.float64(1297.0), np.float64(558.0)), (np.float64(1297.0), np.float64(431.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '138442fa4bdacb7a79140742ac07d559', 'category': 'UncategorizedText', 'element_id': '3a557b2579bf13c0842fb30d52f0441f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 48 ---\n",
      ".\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(753.0), np.float64(413.0)), (np.float64(753.0), np.float64(473.0)), (np.float64(783.0), np.float64(473.0)), (np.float64(783.0), np.float64(413.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '138442fa4bdacb7a79140742ac07d559', 'category': 'UncategorizedText', 'element_id': '2cec23ca5625896d9c9e1a7635a84bf3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 49 ---\n",
      "OQ\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(754.0), np.float64(556.0)), (np.float64(754.0), np.float64(586.0)), (np.float64(784.0), np.float64(586.0)), (np.float64(784.0), np.float64(556.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'a2f58691e83ea6728ff012b5b6b5a876'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 50 ---\n",
      "i ' Documents. Text Encoder | = = § 1 Maitimedal voa t ' '\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(803.0), np.float64(573.0)), (np.float64(803.0), np.float64(678.0)), (np.float64(1102.0), np.float64(678.0)), (np.float64(1102.0), np.float64(573.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a2f58691e83ea6728ff012b5b6b5a876', 'category': 'UncategorizedText', 'element_id': '85c023930b3aef2d3d56703259f1d567'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 51 ---\n",
      "Figure 1: Overview of our proposed universal RAG framework RAG-Anything.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(409.0), np.float64(722.0)), (np.float64(409.0), np.float64(747.0)), (np.float64(1289.0), np.float64(747.0)), (np.float64(1289.0), np.float64(722.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a2f58691e83ea6728ff012b5b6b5a876', 'category': 'NarrativeText', 'element_id': '691c0ebb382a6c1813852eb1dda815e9'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 52 ---\n",
      "LaTeX Equation Recognition\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(441.0), np.float64(539.0)), (np.float64(441.0), np.float64(565.0)), (np.float64(523.0), np.float64(565.0)), (np.float64(523.0), np.float64(539.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'cca3fa5814e1d281321824c80fbd59b7'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 53 ---\n",
      "ofuz vouonb3 —oyuy a6ouz\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(557.0), np.float64(411.0)), (np.float64(557.0), np.float64(570.0)), (np.float64(572.0), np.float64(570.0)), (np.float64(572.0), np.float64(411.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'a4422b9903f06e861cd5870a6ad87dac'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 54 ---\n",
      "Semantic Similarity Matching\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(1187.0), np.float64(563.0)), (np.float64(1187.0), np.float64(586.0)), (np.float64(1271.0), np.float64(586.0)), (np.float64(1271.0), np.float64(563.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '6a21ad587e27c5812c0a034d6ec40649'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 55 ---\n",
      "V8 over All\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(1056.0), np.float64(577.0)), (np.float64(1056.0), np.float64(586.0)), (np.float64(1109.0), np.float64(586.0)), (np.float64(1109.0), np.float64(577.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'fc6c732babae07b0dc1853dcc08feea2'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 56 ---\n",
      "o4uz jopow 14m Jonyxay,\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(735.0), np.float64(460.0)), (np.float64(735.0), np.float64(604.0)), (np.float64(745.0), np.float64(604.0)), (np.float64(745.0), np.float64(460.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'fc6c732babae07b0dc1853dcc08feea2', 'category': 'UncategorizedText', 'element_id': '63be181f251171aceb8444566f6ea379'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 57 ---\n",
      "Table Structure & Content Parsing\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(437.0), np.float64(637.0)), (np.float64(437.0), np.float64(663.0)), (np.float64(534.0), np.float64(663.0)), (np.float64(534.0), np.float64(637.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'f1cd185e07ca5935a2303c500170735b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 58 ---\n",
      "our aIqmL\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(560.0), np.float64(607.0)), (np.float64(560.0), np.float64(667.0)), (np.float64(571.0), np.float64(667.0)), (np.float64(571.0), np.float64(607.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'd15bed39f1e029258abc6a9f6861c067'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 59 ---\n",
      "| Based on the experimental | | data, the results revealed...\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(1222.0), np.float64(652.0)), (np.float64(1222.0), np.float64(690.0)), (np.float64(1391.0), np.float64(690.0)), (np.float64(1391.0), np.float64(652.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd15bed39f1e029258abc6a9f6861c067', 'category': 'NarrativeText', 'element_id': 'b5b8649db8527b57aaae2ef8beb0b4fb'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 60 ---\n",
      "To ensure high-fidelity extraction, RAG-Anything leverages specialized parsers for different content types. Text is segmented into coherent paragraphs or list items. Figures are extracted with associated metadata such as captions and cross-references. Tables are parsed into structured cells with headers and values. Mathematical expressions are converted into symbolic representations. The resulting x; preserves both content and structural context within the source. This provides a faithful, modal\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(809.0)), (np.float64(299.0), np.float64(1047.0)), (np.float64(1403.0), np.float64(1047.0)), (np.float64(1403.0), np.float64(809.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd15bed39f1e029258abc6a9f6861c067', 'category': 'NarrativeText', 'element_id': 'a5e6095473206f924d8866f982da0ce5'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 61 ---\n",
      "2.2.1 DUAL-GRAPH CONSTRUCTION FOR MULTIMODAL KNOWLEDGE\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1104.0)), (np.float64(301.0), np.float64(1123.0)), (np.float64(1144.0), np.float64(1123.0)), (np.float64(1144.0), np.float64(1104.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '33a7ac58815be07c3a022214ba4fb99b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 62 ---\n",
      "While multimodal knowledge unification provides a uniform abstraction across modalities, directly constructing a single unified graph often risks overlooking modality-specific structural signals. The proposed RAG-Anything addresses this challenge through a dual-graph construction strategy. The system first builds a cross-modal knowledge graph that faithfully grounds non-textual modalities within their contextual environment. It then constructs a text-based knowledge graph using es- tablished tex\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1164.0)), (np.float64(299.0), np.float64(1402.0)), (np.float64(1403.0), np.float64(1402.0)), (np.float64(1403.0), np.float64(1164.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '33a7ac58815be07c3a022214ba4fb99b', 'category': 'NarrativeText', 'element_id': '0e3f2278b87b8e70cf511bbffcb4524c'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 63 ---\n",
      "Cross-Modal Knowledge Graph: Non-textual content like images, tables, and equations contains rich semantic information that traditional text-only approaches often overlook. To preserve this knowledge, RAG-Anything constructs a multimodal knowledge graph where non-text atomic units are transformed into structured graph entities. RAG-Anything leverages multimodal large language models to derive two complementary textual representations from each atomic content unit. The first is a detailed descrip\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1449.0)), (np.float64(301.0), np.float64(1631.0)), (np.float64(1400.0), np.float64(1631.0)), (np.float64(1400.0), np.float64(1449.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '33a7ac58815be07c3a022214ba4fb99b', 'category': 'ListItem', 'element_id': '90df603c649aef5d27bc145a7bc54123'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 64 ---\n",
      "an entity summary ee containing key attributes such as entity name, type, and description for graph construction. The generation process is context-aware, processing each unit with its local neighborhood C; = {cx | |k — j| < 6}, where 6 controls the contextual window size. This ensures representations accurately reflect each unit’s role within the broader document structure.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(324.0), np.float64(1636.0)), (np.float64(324.0), np.float64(1758.0)), (np.float64(1400.0), np.float64(1758.0)), (np.float64(1400.0), np.float64(1636.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '33a7ac58815be07c3a022214ba4fb99b', 'category': 'NarrativeText', 'element_id': '96a5052628f13567c96a888391ccc93a'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 65 ---\n",
      "Building on these textual representations, RAG-Anything constructs the graph structure using non- text units as anchor points. For each non-text unit c;, the graph extraction routine R(-) processes\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(324.0), np.float64(1776.0)), (np.float64(324.0), np.float64(1834.0)), (np.float64(1403.0), np.float64(1834.0)), (np.float64(1403.0), np.float64(1776.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '33a7ac58815be07c3a022214ba4fb99b', 'category': 'NarrativeText', 'element_id': '98ce12937910bc43f6eba74698a31478'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 66 ---\n",
      "its description dchnk to identify fine-grained entities and relations: (V;,€)) = Rd’), (2)\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(324.0), np.float64(1836.0)), (np.float64(324.0), np.float64(1943.0)), (np.float64(1400.0), np.float64(1943.0)), (np.float64(1400.0), np.float64(1836.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '33a7ac58815be07c3a022214ba4fb99b', 'category': 'NarrativeText', 'element_id': '30f0e1ca8d758f46e9bb6027f0f34926'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 67 ---\n",
      "where V; and €; denote the sets of intra-chunk entities and their relations, respectively. Each atomic non-text unit is associated with a multimodal entity node vj\"™ that serves as an anchor for\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(324.0), np.float64(1976.0)), (np.float64(324.0), np.float64(2038.0)), (np.float64(1400.0), np.float64(2038.0)), (np.float64(1400.0), np.float64(1976.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 4, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '33a7ac58815be07c3a022214ba4fb99b', 'category': 'NarrativeText', 'element_id': '7491558dfb211cbc3957fb08ea17f4ca'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 68 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 5, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '3ff3963731e1ae0cb8492b4c80067a3b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 69 ---\n",
      "its intra-chunk entities through explicit be longs_to edges:\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(324.0), np.float64(236.0)), (np.float64(324.0), np.float64(261.0)), (np.float64(1001.0), np.float64(261.0)), (np.float64(1001.0), np.float64(236.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 5, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '3ff3963731e1ae0cb8492b4c80067a3b', 'category': 'NarrativeText', 'element_id': 'b7f10c10a5d851740c46925068db4634'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 70 ---\n",
      "V= {opm}; U U V;, (3) j E= Ue U Uttu Betongs—f0, up) swe Vj}. (4) j j\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(576.0), np.float64(287.0)), (np.float64(576.0), np.float64(431.0)), (np.float64(1400.0), np.float64(431.0)), (np.float64(1400.0), np.float64(287.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 5, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '3ff3963731e1ae0cb8492b4c80067a3b', 'category': 'NarrativeText', 'element_id': 'eb7be159c7f9d8245c6213bac01c1808'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 71 ---\n",
      "This construction preserves modality-specific grounding while ensuring non-textual content is con- textualized by its textual neighborhood. This enables reliable cross-modal retrieval and reasoning.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(323.0), np.float64(457.0)), (np.float64(323.0), np.float64(513.0)), (np.float64(1403.0), np.float64(513.0)), (np.float64(1403.0), np.float64(457.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 5, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '3ff3963731e1ae0cb8492b4c80067a3b', 'category': 'NarrativeText', 'element_id': '5d16e65b84a8c6758df48e95ba51a03b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 72 ---\n",
      "Text-based Knowledge Graph: For text modality chunks, we construct a traditional text-based knowledge graph following established methodologies similar to LightRAG (Guo et al., 2024) and GraphRAG (Edge et al., 2024). The extraction process operates directly on textual content x; where t; = text, leveraging named entity recognition and relation extraction techniques to identify entities and their semantic relationships. Given the rich semantic information inherent in textual content, multimodal c\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(531.0)), (np.float64(301.0), np.float64(769.0)), (np.float64(1400.0), np.float64(769.0)), (np.float64(1400.0), np.float64(531.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 5, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '3ff3963731e1ae0cb8492b4c80067a3b', 'category': 'ListItem', 'element_id': '10f399cba4de37b93240fe5c5db5959f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 73 ---\n",
      "2.2.2 GRAPH FUSION AND INDEX CREATION\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(811.0)), (np.float64(301.0), np.float64(830.0)), (np.float64(847.0), np.float64(830.0)), (np.float64(847.0), np.float64(811.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 5, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '7c3877fb35c5d4d0f3176c0ee764de45'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 74 ---\n",
      "The separate cross-modal and text-based knowledge graphs capture complementary aspects of document semantics. Integrating them creates a unified representation leveraging visual-textual associations and fine-grained textual relationships for enhanced retrieval.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(865.0)), (np.float64(299.0), np.float64(951.0)), (np.float64(1401.0), np.float64(951.0)), (np.float64(1401.0), np.float64(865.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 5, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '7c3877fb35c5d4d0f3176c0ee764de45', 'category': 'NarrativeText', 'element_id': '2920702ba605472b3addad1b497f4122'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 75 ---\n",
      "e (i) Entity Alignment and Graph Fusion. To create a unified knowledge representation, we merge the multimodal knowledge graph (V, EB ) and text-based knowledge graph through entity align- ment. This process uses entity names as primary matching keys to identify semantically equivalent entities across both graph structures. The integration consolidates their representations, creating a comprehensive knowledge graph G = (V,€). This graph captures both multimodal contextual relationships and text-\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(973.0)), (np.float64(300.0), np.float64(1215.0)), (np.float64(1403.0), np.float64(1215.0)), (np.float64(1403.0), np.float64(973.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 5, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '7c3877fb35c5d4d0f3176c0ee764de45', 'category': 'NarrativeText', 'element_id': '3c93e9d64549cc3b69d9416d91c454bb'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 76 ---\n",
      "e (ii) Dense Representation Generation. To enable efficient similarity-based retrieval, we construct a comprehensive embedding table 7 that encompasses all components generated during the indexing process. We encode dense representations for all graph entities, relationships, and atomic content chunks across modalities using an appropriate encoder. This creates a unified embedding space where each component s € entities, relations, chunks is mapped to its corresponding dense representation:\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1237.0)), (np.float64(300.0), np.float64(1388.0)), (np.float64(1400.0), np.float64(1388.0)), (np.float64(1400.0), np.float64(1237.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 5, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '7c3877fb35c5d4d0f3176c0ee764de45', 'category': 'NarrativeText', 'element_id': 'c7a6d08c749f26f8d6b1d176a9c44777'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 77 ---\n",
      "T =emb(s): 5 € VUEUG,, (5)\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(675.0), np.float64(1404.0)), (np.float64(675.0), np.float64(1446.0)), (np.float64(1400.0), np.float64(1446.0)), (np.float64(1400.0), np.float64(1404.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 5, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '7c3877fb35c5d4d0f3176c0ee764de45', 'category': 'UncategorizedText', 'element_id': '51cd9696227bef48644259486ab75e69'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 78 ---\n",
      "where emb(-) denotes the embedding function tailored for each component type. Together, the unified knowledge graph G and the embedding table 7 constitute the complete retrieval index I = (G,T). This provides both structural knowledge representation and dense vector space for efficient cross-modal similarity search during the subsequent retrieval stage.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1465.0)), (np.float64(300.0), np.float64(1583.0)), (np.float64(1400.0), np.float64(1583.0)), (np.float64(1400.0), np.float64(1465.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 5, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '7c3877fb35c5d4d0f3176c0ee764de45', 'category': 'NarrativeText', 'element_id': 'aabe0ff2cab1607f4b18b9683b147738'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 79 ---\n",
      "2.3 CROSS-MODAL HYBRID RETRIEVAL\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1629.0)), (np.float64(301.0), np.float64(1648.0)), (np.float64(795.0), np.float64(1648.0)), (np.float64(795.0), np.float64(1629.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 5, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'fe8adc2a634f3a6f64d6f66244544a1a'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 80 ---\n",
      "The retrieval stage operates on the index I = (G, T) to identify relevant knowledge components for a given user query. Traditional RAG methods face significant limitations when dealing with multimodal documents. They typically rely on semantic similarity within single modalities and fail to capture the rich interconnections between visual, mathematical, tabular, and textual elements. To address these challenges, our framework introduces a cross-modal hybrid retrieval mechanism. This mechanism le\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1685.0)), (np.float64(299.0), np.float64(1864.0)), (np.float64(1400.0), np.float64(1864.0)), (np.float64(1400.0), np.float64(1685.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 5, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'fe8adc2a634f3a6f64d6f66244544a1a', 'category': 'NarrativeText', 'element_id': 'e1b8af25ddfd0c277c90ccc5600561a4'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 81 ---\n",
      "Modality-Aware Query Encoding. Given a user query q, we first perform modality-aware query analysis to extract lexical cues and potential modality preferences embedded within the query. For instance, queries containing terms such as \"figure,\" \"chart,\" \"table,\" or \"equation\" provide explicit signals about the expected modality of relevant information. We then compute a unified text embedding e, using the same encoder employed during indexing, ensuring consistency between\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1886.0)), (np.float64(300.0), np.float64(2035.0)), (np.float64(1403.0), np.float64(2035.0)), (np.float64(1403.0), np.float64(1886.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 5, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'fe8adc2a634f3a6f64d6f66244544a1a', 'category': 'NarrativeText', 'element_id': 'b56707f4b2f9f2284662a6a70ccf39a3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 82 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 6, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'ab67c53856cfeb1d8959fcf7443571f1'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 83 ---\n",
      "query and knowledge representations. This embedding-based approach enables cross-modal retrieval capabilities where textual queries can effectively access multimodal content through their shared representations, maintaining retrieval consistency while preserving cross-modal accessibility.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(236.0)), (np.float64(300.0), np.float64(321.0)), (np.float64(1399.0), np.float64(321.0)), (np.float64(1399.0), np.float64(236.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 6, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ab67c53856cfeb1d8959fcf7443571f1', 'category': 'NarrativeText', 'element_id': 'f381b45b3898139a99ea36995f5690a4'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 84 ---\n",
      "Hybrid Knowledge Retrieval Architecture. Recognizing that knowledge relevance manifests through both explicit structural connections and implicit semantic relationships, we design a hybrid retrieval architecture that strategically combines two complementary mechanisms.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(343.0)), (np.float64(300.0), np.float64(429.0)), (np.float64(1399.0), np.float64(429.0)), (np.float64(1399.0), np.float64(343.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 6, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ab67c53856cfeb1d8959fcf7443571f1', 'category': 'NarrativeText', 'element_id': '2ee68f142774ac42eb03eed2962a60a2'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 85 ---\n",
      "e (i) Structural Knowledge Navigation. This mechanism addresses the challenge of capturing explicit relationships and multi-hop reasoning patterns. Traditional keyword-based retrieval often fails to identify knowledge connected through intermediate entities or cross-modal relationships. To overcome this limitation, we exploit the structural properties encoded within our unified knowledge graph G. We employ keyword matching and entity recognition to locate relevant graph components. The retrieval\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(451.0)), (np.float64(299.0), np.float64(629.0)), (np.float64(1403.0), np.float64(629.0)), (np.float64(1403.0), np.float64(451.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 6, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ab67c53856cfeb1d8959fcf7443571f1', 'category': 'NarrativeText', 'element_id': '0003d62d25dcba2236c500906ca1600f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 86 ---\n",
      "We then perform strategic neighborhood expansion to include related entities and relationships within a specified hop distance. This structural approach proves particularly effective at uncovering high- level semantic connections and entity-relation patterns that span multiple modalities. It capitalizes on the rich cross-modal linkages established in our multimodal knowledge graph. The structural navigation yields candidate set C.,,,(q) containing relevant entities, relationships, and their asso\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(651.0)), (np.float64(299.0), np.float64(828.0)), (np.float64(1403.0), np.float64(828.0)), (np.float64(1403.0), np.float64(651.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 6, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ab67c53856cfeb1d8959fcf7443571f1', 'category': 'NarrativeText', 'element_id': '3e07b4e1cb9a597df6487afac07629e4'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 87 ---\n",
      "e (ii) Semantic Similarity Matching. This mechanism addresses the challenge of identifying semantically relevant knowledge that lacks explicit structural connections. While structural navigation excels at following explicit relationships, it may miss relevant content that is semantically related but not directly connected in the graph topology. To bridge this gap, we conduct dense vector similarity search between the query embedding e, and all components stored in embedding table T.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(850.0)), (np.float64(301.0), np.float64(999.0)), (np.float64(1400.0), np.float64(999.0)), (np.float64(1400.0), np.float64(850.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 6, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ab67c53856cfeb1d8959fcf7443571f1', 'category': 'NarrativeText', 'element_id': 'ef5fe50a9417caa4e41c3471771fd979'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 88 ---\n",
      "This approach encompasses atomic content chunks across all modalities, graph entities, and relation- ship representations, enabling fine-grained semantic matching that can surface relevant knowledge even when traditional lexical or structural signals are absent. The learned embedding space captures nuanced semantic relationships and contextual similarities that complement the explicit structural signals from the navigation mechanism. This retrieval pathway returns the top-k most semantically sim\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1019.0)), (np.float64(299.0), np.float64(1226.0)), (np.float64(1403.0), np.float64(1226.0)), (np.float64(1403.0), np.float64(1019.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 6, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ab67c53856cfeb1d8959fcf7443571f1', 'category': 'NarrativeText', 'element_id': 'f32642a6b2866acce7192340fd1f5fce'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 89 ---\n",
      "Candidate Pool Unification. Both retrieval pathways may return overlapping candidates with differing relevance signals. This necessitates a principled approach to unify and rank results. Retrieval candidates from both pathways are unified into a comprehensive candidate pool: C(q) = Cstru(q) U Cseman(q). Simply merging candidates would ignore distinct evidence each pathway provides. It would fail to account for redundancy between retrieved content.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1248.0)), (np.float64(300.0), np.float64(1395.0)), (np.float64(1400.0), np.float64(1395.0)), (np.float64(1400.0), np.float64(1248.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 6, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ab67c53856cfeb1d8959fcf7443571f1', 'category': 'NarrativeText', 'element_id': 'f793828ed951db56f86a5572d7ceea1e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 90 ---\n",
      "e (i) Multi-Signal Fusion Scoring. To address these challenges, we apply a sophisticated fusion scoring mechanism integrating multiple complementary relevance signals. These include structural importance derived from graph topology, semantic similarity scores from embedding space, and query- inferred modality preferences obtained through lexical analysis. This multi-faceted scoring approach ensures that final ranked candidates C*(q) effectively balance structural knowledge relationships with sem\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1417.0)), (np.float64(301.0), np.float64(1594.0)), (np.float64(1403.0), np.float64(1594.0)), (np.float64(1403.0), np.float64(1417.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 6, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ab67c53856cfeb1d8959fcf7443571f1', 'category': 'NarrativeText', 'element_id': 'b06db12ee9e7ae405ca6c2091b3578c3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 91 ---\n",
      "e (ii) Hybrid Retrieval Integration. The resulting hybrid retrieval mechanism enables our framework to leverage the complementary strengths of both knowledge graphs and dense representations. This provides comprehensive coverage of relevant multimodal knowledge for response generation.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1617.0)), (np.float64(300.0), np.float64(1702.0)), (np.float64(1400.0), np.float64(1702.0)), (np.float64(1400.0), np.float64(1617.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 6, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ab67c53856cfeb1d8959fcf7443571f1', 'category': 'NarrativeText', 'element_id': '9a0b750d448134501193fbc565d2aae5'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 92 ---\n",
      "2.4 FROM RETRIEVAL TO SYNTHESIS\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1750.0)), (np.float64(301.0), np.float64(1769.0)), (np.float64(758.0), np.float64(1769.0)), (np.float64(758.0), np.float64(1750.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 6, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '4bb8bffa023b698eaea38a035161544f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 93 ---\n",
      "Effective multimodal question answering requires preserving rich visual semantics while maintaining coherent grounding across heterogeneous knowledge sources. Simple text-only approaches lose crucial visual information, while naive multimodal methods struggle with coherent cross-modal integration. Our synthesis stage addresses these challenges by systematically combining retrieved multimodal knowledge into comprehensive, evidence-grounded responses.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1809.0)), (np.float64(301.0), np.float64(1955.0)), (np.float64(1399.0), np.float64(1955.0)), (np.float64(1399.0), np.float64(1809.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 6, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '4bb8bffa023b698eaea38a035161544f', 'category': 'NarrativeText', 'element_id': '08027348abbe357529472c4d6c4f4490'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 94 ---\n",
      "e (i) Building Textual Context. Given the top-ranked retrieval candidates C*(q), we construct a structured textual context. We concatenate textual representations of all retrieved components, includ-\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1975.0)), (np.float64(301.0), np.float64(2033.0)), (np.float64(1403.0), np.float64(2033.0)), (np.float64(1403.0), np.float64(1975.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 6, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '4bb8bffa023b698eaea38a035161544f', 'category': 'NarrativeText', 'element_id': 'c1691797bed134f572213332350e51a5'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 95 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '11ef8fad681191c07f97d0318681c21b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 96 ---\n",
      "Table 1: Statistics of Experimental Datasets.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(604.0), np.float64(229.0)), (np.float64(604.0), np.float64(254.0)), (np.float64(1093.0), np.float64(254.0)), (np.float64(1093.0), np.float64(229.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '6e37947b47cfcd03de5fa7b2a851a92f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 97 ---\n",
      "Dataset #Documents #Avg. Pages #Avg. Tokens #Doc Types # Questions DocBench 229 66 46377 5 1102 MMLongBench 135 47.5 21214 7 1082\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(317.0), np.float64(272.0)), (np.float64(317.0), np.float64(375.0)), (np.float64(1384.0), np.float64(375.0)), (np.float64(1384.0), np.float64(272.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6e37947b47cfcd03de5fa7b2a851a92f', 'category': 'NarrativeText', 'element_id': '1a50f0e968babe944ad5a19967c5b61a'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 98 ---\n",
      "ing entity summaries, relationship descriptions, and chunk contents. The concatenation incorporates appropriate delimiters to indicate modality types and hierarchical origins. This approach ensures the language model can effectively parse and reason over heterogeneous knowledge components.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(419.0)), (np.float64(301.0), np.float64(505.0)), (np.float64(1399.0), np.float64(505.0)), (np.float64(1399.0), np.float64(419.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6e37947b47cfcd03de5fa7b2a851a92f', 'category': 'NarrativeText', 'element_id': 'f4fe1a6d6471016bb79d02edf292f52b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 99 ---\n",
      "e (ii) Recovering Visual Content. For multimodal chunks corresponding to visual artifacts, we perform dereferencing to recover original visual content, creating V*(q). This design maintains con- sistency with our unified embedding strategy. Textual proxies enable efficient retrieval while authentic visual content provides rich semantics necessary for sophisticated reasoning during synthesis.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(527.0)), (np.float64(300.0), np.float64(643.0)), (np.float64(1403.0), np.float64(643.0)), (np.float64(1403.0), np.float64(527.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6e37947b47cfcd03de5fa7b2a851a92f', 'category': 'NarrativeText', 'element_id': '9abe0fd699c68b75053ca0c7d0b943e9'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 100 ---\n",
      "The synthesis process jointly conditions on both the assembled comprehensive textual context and dereferenced visual artifacts using a vision-language model:\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(665.0)), (np.float64(299.0), np.float64(721.0)), (np.float64(1399.0), np.float64(721.0)), (np.float64(1399.0), np.float64(665.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6e37947b47cfcd03de5fa7b2a851a92f', 'category': 'NarrativeText', 'element_id': '3beacc13abf759dba191fb6e34ec47f8'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 101 ---\n",
      "Response = VLM(q,P(q), ¥*(2)): 6)\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(653.0), np.float64(745.0)), (np.float64(653.0), np.float64(773.0)), (np.float64(1400.0), np.float64(773.0)), (np.float64(1400.0), np.float64(745.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6e37947b47cfcd03de5fa7b2a851a92f', 'category': 'UncategorizedText', 'element_id': '8124ad95446b59f52c66c56495e2a0e3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 102 ---\n",
      "where the VLM integrates information from query, textual context, and visual content. This unified conditioning enables sophisticated visual interpretation while maintaining grounding in retrieved evidence. The resulting responses are both visually informed and factually grounded.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(798.0)), (np.float64(300.0), np.float64(884.0)), (np.float64(1399.0), np.float64(884.0)), (np.float64(1399.0), np.float64(798.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6e37947b47cfcd03de5fa7b2a851a92f', 'category': 'NarrativeText', 'element_id': '82b4e8af0663f242e3ddc16b33307a80'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 103 ---\n",
      "3 EVALUATION\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(302.0), np.float64(938.0)), (np.float64(302.0), np.float64(961.0)), (np.float64(535.0), np.float64(961.0)), (np.float64(535.0), np.float64(938.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'e7e53024452b056f0e2a1d7aaa29e753'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 104 ---\n",
      "3.1 EXPERIMENTAL SETTINGS\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(302.0), np.float64(1007.0)), (np.float64(302.0), np.float64(1026.0)), (np.float64(677.0), np.float64(1026.0)), (np.float64(677.0), np.float64(1007.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'f7d636275776b73fe248c168ea02922a'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 105 ---\n",
      "Evaluation Datasets. We conduct comprehensive evaluations on two challenging multimodal Document Question Answering (DQA) benchmarks that reflect real-world complexity and diversity. DocBench (Zou et al., 2024) provides a rigorous testbed with 229 multimodal documents spanning five critical domains: Academia, Finance, Government, Laws, and News. The dataset includes 1,102 expert-crafted question-answer pairs. These documents are notably extensive, averaging 66 pages and approximately 46,377 toke\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1065.0)), (np.float64(301.0), np.float64(1242.0)), (np.float64(1403.0), np.float64(1242.0)), (np.float64(1403.0), np.float64(1065.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'f7d636275776b73fe248c168ea02922a', 'category': 'NarrativeText', 'element_id': '976a34454ddf38d1349712bb78842698'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 106 ---\n",
      "MMLongBench (Ma et al., 2024) complements this evaluation by focusing specifically on long- context multimodal document comprehension. It features 135 documents across 7 diverse document types with 1,082 expert-annotated questions. Together, these benchmarks provide comprehensive coverage of the multimodal document understanding challenges that RAG-Anything aims to address. They ensure our evaluation captures both breadth across domains and depth in document complexity. Detailed dataset statisti\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1264.0)), (np.float64(299.0), np.float64(1442.0)), (np.float64(1403.0), np.float64(1442.0)), (np.float64(1403.0), np.float64(1264.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'f7d636275776b73fe248c168ea02922a', 'category': 'NarrativeText', 'element_id': 'b19434c99334ac4783b002b84598ae5d'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 107 ---\n",
      "Baselines. We compare RAG-Anything against the following methods for performance evaluation:\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1464.0)), (np.float64(301.0), np.float64(1489.0)), (np.float64(1393.0), np.float64(1489.0)), (np.float64(1393.0), np.float64(1464.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'f7d636275776b73fe248c168ea02922a', 'category': 'NarrativeText', 'element_id': '1f38b862522f12a0d7a795fff4680257'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 108 ---\n",
      "¢ GPT-40-mini: A powerful multimodal language model with native text and image understanding capabilities. Its 128K token context window enables direct processing of entire documents. We evaluate this model as a strong baseline for long-context multimodal understanding.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1525.0)), (np.float64(301.0), np.float64(1611.0)), (np.float64(1399.0), np.float64(1611.0)), (np.float64(1399.0), np.float64(1525.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'f7d636275776b73fe248c168ea02922a', 'category': 'NarrativeText', 'element_id': 'eff0541e58bf8a3afe71b6c16754fc57'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 109 ---\n",
      "¢ LightRAG (Guo et al., 2024): A graph-enhanced RAG system that integrates structured knowledge representation with dual-level retrieval mechanisms. It captures both fine-grained entity-relation information and broader semantic context, improving retrieval precision and response coherence.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1629.0)), (np.float64(301.0), np.float64(1715.0)), (np.float64(1399.0), np.float64(1715.0)), (np.float64(1399.0), np.float64(1629.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'f7d636275776b73fe248c168ea02922a', 'category': 'NarrativeText', 'element_id': '663c5137e321681a4ce417279393d2df'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 110 ---\n",
      "MMGraphRAG (Wan & Yu, 2025): A multimodal retrieval framework that constructs unified knowledge graphs spanning textual and visual content. This method employs spectral clustering for multimodal entity analysis and retrieves context along reasoning paths to guide generation.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1733.0)), (np.float64(301.0), np.float64(1819.0)), (np.float64(1399.0), np.float64(1819.0)), (np.float64(1399.0), np.float64(1733.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'f7d636275776b73fe248c168ea02922a', 'category': 'ListItem', 'element_id': 'd141f1c24c2dd5bc92a16323c6d62299'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 111 ---\n",
      "Experimental Settings. In our experiments, we implement all baselines using GPT-40-mini as the backbone LLM. Documents are parsed using MinerU (Wang et al., 2024) to extract text, im- ages, tables, and equations for downstream RAG processing. For the retrieval pipeline, we em- ploy the text-embedding-3-large model with 3072-dimensional embeddings. We use the bge-reranker-—v2-m3 model for reranking. For graph-based RAG methods, we enforce a com- bined entity-and-relation token limit of 20,000 tok\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1856.0)), (np.float64(300.0), np.float64(2033.0)), (np.float64(1403.0), np.float64(2033.0)), (np.float64(1403.0), np.float64(1856.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 7, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'f7d636275776b73fe248c168ea02922a', 'category': 'NarrativeText', 'element_id': '3b731d16381810ba8fab633f34a64a55'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 112 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 8, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '002e5af3f5de67f7f987f849ef0cd542'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 113 ---\n",
      "Table 2: Accuracy (%) on DocBench Dataset. Performance results with best scores highlighted in dark blue and second-best in light blue. Domain categories include Academia (Aca.), Finance (Fin.), Government (Gov.), Legal Documents (Law), and News Articles (News). Document types are categorized as Text-only (Txt.), Multimodal (Mm.), and Unanswerable queries (Una.).\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(229.0)), (np.float64(299.0), np.float64(346.0)), (np.float64(1399.0), np.float64(346.0)), (np.float64(1399.0), np.float64(229.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 8, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '002e5af3f5de67f7f987f849ef0cd542', 'category': 'NarrativeText', 'element_id': 'ffc6862408633748b2179bbb27e76c2c'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 114 ---\n",
      "Domains Types Aca. Fin. Gov. Law. News Txt. Mm. Una.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(548.0), np.float64(376.0)), (np.float64(548.0), np.float64(446.0)), (np.float64(1243.0), np.float64(446.0)), (np.float64(1243.0), np.float64(376.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 8, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '6c66646166b2b81dfdbf73ac14256411'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 115 ---\n",
      "GPT-40-mini 40.3. 46.9 60.3 59.2 61.0 61.0 43.8 49.6 51.2 LightRAG 53.8 56.2 59.5 61.8 65.7 85.0 59.7 46.8 58.4 MMGraphRAG 64.3 52.8 64.9 40.0 61.5 67.6 66.0 60.5 61.0 RAGAnything 61.4 67.0 61.5 60.2 663 85.0 76.3 46.0 63.4\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(326.0), np.float64(473.0)), (np.float64(326.0), np.float64(597.0)), (np.float64(1353.0), np.float64(597.0)), (np.float64(1353.0), np.float64(473.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 8, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6c66646166b2b81dfdbf73ac14256411', 'category': 'UncategorizedText', 'element_id': 'e644ad20c8b832af2d3e97985eecf2c7'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 116 ---\n",
      "Method Overall\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(370.0), np.float64(392.0)), (np.float64(370.0), np.float64(426.0)), (np.float64(1374.0), np.float64(426.0)), (np.float64(1374.0), np.float64(392.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 8, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '410caa0f4c209f1d28e7a44e49e13eac'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 117 ---\n",
      "Table 3: Accuracy (%) on MMLongBench across different domains and overall performance. Best re- sults are highlighted in dark blue and second-best in light blue.. Domain categories include Research Reports/Introductions (Res.), Tutorials/Workshops (Tut.), Academic Papers (Acad.), Guidebooks (Guid.), Brochures (Broch.), Administration/Industry Files (Admin.), and Financial Reports (Fin.).\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(650.0)), (np.float64(299.0), np.float64(767.0)), (np.float64(1403.0), np.float64(767.0)), (np.float64(1403.0), np.float64(650.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 8, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '410caa0f4c209f1d28e7a44e49e13eac', 'category': 'NarrativeText', 'element_id': '5a347ff63a7ba4397c1473c18ff0a976'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 118 ---\n",
      "Method Domains Overall\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(371.0), np.float64(798.0)), (np.float64(371.0), np.float64(848.0)), (np.float64(1373.0), np.float64(848.0)), (np.float64(1373.0), np.float64(798.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 8, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '81fcc8486eabbf6537f28f3df267b533'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 119 ---\n",
      "Res. Tut. Acad. Guid. Broch. Admin. Fin.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(554.0), np.float64(848.0)), (np.float64(554.0), np.float64(869.0)), (np.float64(1236.0), np.float64(869.0)), (np.float64(1236.0), np.float64(848.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 8, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '23bd10814d2cce221de8d387415cb5b1'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 120 ---\n",
      "GPT-40-mini 35.5 44.0 246 33.1 29.5 46.8 31.1 33.5 LightRAG 40.8 341 36.2 39.4 41.0 44.4 38.3 38.9 MMGraphRAG 40.8 36.5 35.7 35.8 28.2 46.9 38.5 37.7 RAGAnything 46.6 43.5 38.7 43.9 34.0 45.7 43.6 42.8\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(326.0), np.float64(897.0)), (np.float64(326.0), np.float64(1024.0)), (np.float64(1351.0), np.float64(1024.0)), (np.float64(1351.0), np.float64(897.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 8, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '23bd10814d2cce221de8d387415cb5b1', 'category': 'UncategorizedText', 'element_id': '795c5bef18ccfdc9a9916fbc2053f3fa'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 121 ---\n",
      "Outputs are constrained to a one-sentence format. For the baseline GPT-40-mini in our QA scenario, documents are concatenated into image form with a maximum of 50 pages per document, rendered at 144 dpi. Finally, all query results are evaluated for accuracy by GPT—40-mini.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1106.0)), (np.float64(301.0), np.float64(1192.0)), (np.float64(1402.0), np.float64(1192.0)), (np.float64(1402.0), np.float64(1106.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 8, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '23bd10814d2cce221de8d387415cb5b1', 'category': 'NarrativeText', 'element_id': '52128923bcb6c8c84829914b9dd6076e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 122 ---\n",
      "3.2 PERFORMANCE COMPARISON\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(302.0), np.float64(1245.0)), (np.float64(302.0), np.float64(1264.0)), (np.float64(714.0), np.float64(1264.0)), (np.float64(714.0), np.float64(1245.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 8, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '7a7ee02316a8f7bf711a4d87c812110d'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 123 ---\n",
      "Superior Performance and Cross-Domain Generalization. RAG-Anything demonstrates superior overall performance over baselines through its unified multimodal framework. Unlike LightRAG, which is restricted to text-only content processing, RAG-Anything treats text, images, tables, and equations as first-class entities. MMGraphRAG only adds basic image processing while treating tables and equations as plain text, missing crucial structural information. RAG-Anything introduces a comprehensive dual-gra\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1305.0)), (np.float64(300.0), np.float64(1513.0)), (np.float64(1403.0), np.float64(1513.0)), (np.float64(1403.0), np.float64(1305.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 8, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '7a7ee02316a8f7bf711a4d87c812110d', 'category': 'NarrativeText', 'element_id': 'b893dc528c1e5507807ae5c0287e8498'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 124 ---\n",
      "Enhanced Long-Context Performance. RAG-Anything demonstrates superior performance on long-context documents. The framework excels where relevant evidence is dispersed across multiple modalities and sections. It achieves the best results in information-dense domains such as Research Reports and Financial Reports on MMLongBench. These improvements stem from the structured context injection mechanism. This mechanism integrates dual-graph construction for cross-page entity alignment. It combines sem\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1535.0)), (np.float64(300.0), np.float64(1864.0)), (np.float64(1400.0), np.float64(1864.0)), (np.float64(1400.0), np.float64(1535.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 8, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '7a7ee02316a8f7bf711a4d87c812110d', 'category': 'NarrativeText', 'element_id': '5749db9ce8e60510fd54ac28a8e8e2b1'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 125 ---\n",
      "To systematically evaluate model performance across varying document lengths, we conducted comprehensive experiments on both datasets. As illustrated in Figure 2, RAG-Anything and MM- GraphRAG exhibit comparable performance on shorter documents. However, RAG-Anything’s advantages become increasingly pronounced as document length grows. On DocBench, the perfor- mance gap expands dramatically to over 13 points for documents exceeding 100 pages (68.2% vs.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1886.0)), (np.float64(299.0), np.float64(2033.0)), (np.float64(1403.0), np.float64(2033.0)), (np.float64(1403.0), np.float64(1886.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 8, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '7a7ee02316a8f7bf711a4d87c812110d', 'category': 'NarrativeText', 'element_id': 'fbdfa8ab14d584c1b53080dbbc0e3615'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 126 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 9, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'd3b0b4f04327da05b926985341350ea1'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 127 ---\n",
      "DocBench Accuracy DocBench QA Counts MMLongBench Accuracy MMLongBench QA Counts 100. 100; = RAGAnything 350] = RAGAnything 700: <2 MMGraphrac —e MMGraphRAG s 3 s 3 pa] a > 3 id = ig = G 150; | i 3” & 3° § 300 8 100, g {200 20 20; o 50, i S00 1-10 11:50 51-100101-200 200+ © F10 11'5051-10001:200200+ 1-10 11-50 51-100101°200 200+ 0° Io 11-5051-10001-200200+ Page Range Page Range Page Range Page Range\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(302.0), np.float64(230.0)), (np.float64(302.0), np.float64(482.0)), (np.float64(1397.0), np.float64(482.0)), (np.float64(1397.0), np.float64(230.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 9, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd3b0b4f04327da05b926985341350ea1', 'category': 'NarrativeText', 'element_id': '61f4672c2b5ca6d802dc072badf2b8bd'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 128 ---\n",
      "Figure 2: Performance evaluation across documents of varying lengths.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(456.0), np.float64(524.0)), (np.float64(456.0), np.float64(549.0)), (np.float64(1242.0), np.float64(549.0)), (np.float64(1242.0), np.float64(524.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 9, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd3b0b4f04327da05b926985341350ea1', 'category': 'NarrativeText', 'element_id': 'b388cb84fed0ed8654f622adeb0e743e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 129 ---\n",
      "Table 4: Ablation study results on DocBench. The “Chunk-only” variant bypasses dual-graph construction and relies solely on traditional chunk-based retrieval, while “w/o Reranker” eliminates cross-modal reranking but preserves the core graph-based architecture.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(570.0)), (np.float64(299.0), np.float64(656.0)), (np.float64(1399.0), np.float64(656.0)), (np.float64(1399.0), np.float64(570.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 9, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd3b0b4f04327da05b926985341350ea1', 'category': 'NarrativeText', 'element_id': '52b66e0e051a1761d3afdce6b0d3cfb7'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 130 ---\n",
      "Domains Types Aca. Fin. Gov. Law. News Txt. Mm. Una.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(536.0), np.float64(685.0)), (np.float64(536.0), np.float64(757.0)), (np.float64(1241.0), np.float64(757.0)), (np.float64(1241.0), np.float64(685.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 9, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'f5d6e407238034308edd7880d2bb84e2'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 131 ---\n",
      "Chunk-only 55.8 61.5 60.1 60.7 640 816 66.2 43.5 60.0 w/o Reranker 60.9 63.5 58.8 60.2 68.6 81.7 74.7 45.4 62.4 RAGAnything 61.4 67.0 61.5 60.2 663 85.0 76.3 46.0 63.4\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(326.0), np.float64(784.0)), (np.float64(326.0), np.float64(878.0)), (np.float64(1352.0), np.float64(878.0)), (np.float64(1352.0), np.float64(784.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 9, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'f5d6e407238034308edd7880d2bb84e2', 'category': 'UncategorizedText', 'element_id': '9e0607cf8c361899f3688b3484db2a33'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 132 ---\n",
      "Method Overall\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(363.0), np.float64(703.0)), (np.float64(363.0), np.float64(737.0)), (np.float64(1374.0), np.float64(737.0)), (np.float64(1374.0), np.float64(703.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 9, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'e6527d298e934e4fe8195926cf184cc7'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 133 ---\n",
      "54.6% for 101-200 pages; 68.8% vs. 55.0% for 200+ pages). On MMLongBench, RAG-Anything demonstrates consistent improvements across all length categories, achieving accuracy gains of 3.4 points for 11-50 pages, 9.3 points for 51-100 pages, and 7.9 points for 101-200 pages. These findings confirm that our dual-graph construction and cross-modal hybrid retrieval mechanism is particularly effective for long-document reasoning tasks.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(954.0)), (np.float64(300.0), np.float64(1101.0)), (np.float64(1399.0), np.float64(1101.0)), (np.float64(1399.0), np.float64(954.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 9, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'e6527d298e934e4fe8195926cf184cc7', 'category': 'NarrativeText', 'element_id': 'e1c757003ce9ebf8f1249deb0810caec'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 134 ---\n",
      "3.3. ARCHITECTURAL VALIDATION WITH ABLATION STUDIES\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(302.0), np.float64(1148.0)), (np.float64(302.0), np.float64(1167.0)), (np.float64(1039.0), np.float64(1167.0)), (np.float64(1039.0), np.float64(1148.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 9, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'dbe118c2ead43304859f9652268a50c9'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 135 ---\n",
      "To isolate and quantify the contributions of key architectural components in RAG-Anything, we conducted systematic ablation studies examining two critical design choices. Given that our approach fundamentally differs from existing methods through dual-graph construction and hybrid retrieval, we specifically evaluated: i) Chunk-only, which bypasses graph construction entirely and relies solely on traditional chunk-based retrieval, and ii) w/o Reranker, which eliminates the cross-modal reranking c\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1206.0)), (np.float64(299.0), np.float64(1383.0)), (np.float64(1402.0), np.float64(1383.0)), (np.float64(1402.0), np.float64(1206.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 9, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'dbe118c2ead43304859f9652268a50c9', 'category': 'NarrativeText', 'element_id': '77c2133e5ba130d267aba5b6824def80'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 136 ---\n",
      "As demonstrated in Table 4, the results validate our architectural design through striking performance variations. e Graph Construction is Essential. The chunk-only variant achieves merely 60.0% accuracy with substantial cross-domain drops. This demonstrates that traditional chunking fails to capture structural and cross-modal relationships essential for multimodal documents. e Reranking Provides Marginal Gains. Removing the reranker yields only a modest decline to 62.4%, while the full model ac\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1405.0)), (np.float64(300.0), np.float64(1613.0)), (np.float64(1401.0), np.float64(1613.0)), (np.float64(1401.0), np.float64(1405.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 9, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'dbe118c2ead43304859f9652268a50c9', 'category': 'NarrativeText', 'element_id': 'e2f183fe7d2bf26d0b540ea773a99d1e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 137 ---\n",
      "3.4 CASE STUDIES\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(302.0), np.float64(1659.0)), (np.float64(302.0), np.float64(1678.0)), (np.float64(541.0), np.float64(1678.0)), (np.float64(541.0), np.float64(1659.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 9, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '8fbd17f7031af7392066d2d7cc34c67c'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 138 ---\n",
      "Multimodal documents contain rich structural information within each modality. Understanding these intra-modal structures is crucial for accurate reasoning. We analyze two representative cases from DocBench to demonstrate how RAG-Anything leverages these structures. These cases highlight a key limitation of existing methods. Baselines either rely on superficial textual cues or flatten complex visual elements into plain text. In contrast, RAG-Anything builds modality-aware graphs that preserve es\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1717.0)), (np.float64(301.0), np.float64(1925.0)), (np.float64(1400.0), np.float64(1925.0)), (np.float64(1400.0), np.float64(1717.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 9, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8fbd17f7031af7392066d2d7cc34c67c', 'category': 'NarrativeText', 'element_id': 'ba76e656e88fa03d40aaf7eb715aa202'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 139 ---\n",
      "e Case 1: Multi-panel Figure Interpretation. This case examines a common scenario in academic literature. Researchers often need to compare results across different experimental conditions. These results are typically presented in multi-panel visualizations. Figure 3 shows a challenging t-SNE\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1947.0)), (np.float64(300.0), np.float64(2037.0)), (np.float64(1399.0), np.float64(2037.0)), (np.float64(1399.0), np.float64(1947.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 9, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8fbd17f7031af7392066d2d7cc34c67c', 'category': 'NarrativeText', 'element_id': 'afcf92aaa2d79d58b561b3a3ab70b377'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 140 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'b78181982ecdd5931353624acee4b2cb'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 141 ---\n",
      "‘Question: Which model’ style space shows a separation\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(360.0), np.float64(236.0)), (np.float64(360.0), np.float64(246.0)), (np.float64(661.0), np.float64(246.0)), (np.float64(661.0), np.float64(236.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'b78181982ecdd5931353624acee4b2cb', 'category': 'NarrativeText', 'element_id': '2eb161847c66b5f11fcd06f1640621d3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 142 ---\n",
      "between different styles according to GPT-4o-mini®: LighRAG®: According to Figure 2, the VAE According to Figure 2, the Variational Autoencoder model's style space shows a clearer (VAE) shows a clearer separation between different separation between different styles. styles in its style space compared to the\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(400.0), np.float64(239.0)), (np.float64(400.0), np.float64(321.0)), (np.float64(1382.0), np.float64(321.0)), (np.float64(1382.0), np.float64(239.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'b78181982ecdd5931353624acee4b2cb', 'category': 'NarrativeText', 'element_id': 'ae8d1365480475314ffa799ecbedcedf'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 143 ---\n",
      "Deterministic Autoencoder (DAE).\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(1061.0), np.float64(326.0)), (np.float64(1061.0), np.float64(338.0)), (np.float64(1269.0), np.float64(338.0)), (np.float64(1269.0), np.float64(326.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'f70c8d967cb3d4f644b9e31b22820cf4'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 144 ---\n",
      "MMGraphRAG@:\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(742.0), np.float64(342.0)), (np.float64(742.0), np.float64(357.0)), (np.float64(856.0), np.float64(357.0)), (np.float64(856.0), np.float64(342.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '00dad57a65dc4b74c570877523c86c75'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 145 ---\n",
      "According to Figure 2, the model's style space RAG-Anything(Correct®):\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(742.0), np.float64(358.0)), (np.float64(742.0), np.float64(377.0)), (np.float64(1246.0), np.float64(377.0)), (np.float64(1246.0), np.float64(358.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '00dad57a65dc4b74c570877523c86c75', 'category': 'NarrativeText', 'element_id': 'cd6900ca8835c948802a7f5308c0ae9f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 146 ---\n",
      "shows a clearer separation between different styles The DAE model's style space shows a clearer\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(742.0), np.float64(383.0)), (np.float64(742.0), np.float64(399.0)), (np.float64(1368.0), np.float64(399.0)), (np.float64(1368.0), np.float64(383.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '00dad57a65dc4b74c570877523c86c75', 'category': 'NarrativeText', 'element_id': 'b05e40866fdb804362894003bb19d457'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 147 ---\n",
      "in the Variational Autoencoder (VAE) compared to separation between different styles according to\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(742.0), np.float64(404.0)), (np.float64(742.0), np.float64(421.0)), (np.float64(1389.0), np.float64(421.0)), (np.float64(1389.0), np.float64(404.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '00dad57a65dc4b74c570877523c86c75', 'category': 'NarrativeText', 'element_id': 'b5c3501dd38fb6971d1febe8c9c54612'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 148 ---\n",
      "the Deterministic Autoencoder (DAE).\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(742.0), np.float64(424.0)), (np.float64(742.0), np.float64(436.0)), (np.float64(973.0), np.float64(436.0)), (np.float64(973.0), np.float64(424.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'a0d7169dfd7a12fef297367adf174778'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 149 ---\n",
      "Multimode! Document inthe document Figure 2\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(326.0), np.float64(428.0)), (np.float64(326.0), np.float64(444.0)), (np.float64(1117.0), np.float64(444.0)), (np.float64(1117.0), np.float64(428.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'a0776b0818659fdaeed3f0caf3d54794'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 150 ---\n",
      "Figure 3: Multi-panel figure interpretation case. The query requires identifying cluster separation patterns from the style-space panel, while avoiding confusion from the adjacent content-space panel.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(476.0)), (np.float64(300.0), np.float64(531.0)), (np.float64(1399.0), np.float64(531.0)), (np.float64(1399.0), np.float64(476.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a0776b0818659fdaeed3f0caf3d54794', 'category': 'NarrativeText', 'element_id': '05042daeb59b4d9326d2f4dff7cf9869'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 151 ---\n",
      "visualization with multiple subpanels. The query requires distinguishing between two related but distinct panels. RAG-Anything constructs a visual-layout graph where panels, axis titles, legends, and captions become nodes. Key edges encode semantic relationships. Panels contain specific plots. Captions provide contextual information. Subfigures relate hierarchically. This structure guides the retriever to focus on the style-space panel for comparing cluster separation patterns. The system avoids\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(573.0)), (np.float64(300.0), np.float64(750.0)), (np.float64(1403.0), np.float64(750.0)), (np.float64(1403.0), np.float64(573.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a0776b0818659fdaeed3f0caf3d54794', 'category': 'NarrativeText', 'element_id': '0e975a4bdbfd95c6c25b87ee251503bd'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 152 ---\n",
      "juestic Vhat was Novo Nordisk's total amount sj id salaries in 2020? A a\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(319.0), np.float64(787.0)), (np.float64(319.0), np.float64(808.0)), (np.float64(1143.0), np.float64(808.0)), (np.float64(1143.0), np.float64(787.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a0776b0818659fdaeed3f0caf3d54794', 'category': 'NarrativeText', 'element_id': '2cc1c9b2e143248074730124656fd140'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 153 ---\n",
      "Q unt d salar LightRAG®: Novo Nordisk spent DKK 11,503 million ts spent on wages and salaries in on wages and salaries in 2020. a 2020 was DKK 32,928 million. : ern MMGraphRAG®: RAG-Anything(Correct®): : Novo Nordisk spent a total of 's total amount spent on wages : oa nanan 11,503 million DKK on wages and salaries in 2020 was DKK 26,778 million. Multimodel Document Evidence table in the document and salaries in 2020.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(311.0), np.float64(790.0)), (np.float64(311.0), np.float64(973.0)), (np.float64(1389.0), np.float64(973.0)), (np.float64(1389.0), np.float64(790.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a0776b0818659fdaeed3f0caf3d54794', 'category': 'NarrativeText', 'element_id': 'db03e70ee692c16491bae23957421bbe'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 154 ---\n",
      "Figure 4: Financial table navigation case. The query involves locating the specific intersection of “Wages and salaries” row and “2020” column amid similar terminological entries.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(297.0), np.float64(1009.0)), (np.float64(297.0), np.float64(1065.0)), (np.float64(1401.0), np.float64(1065.0)), (np.float64(1401.0), np.float64(1009.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a0776b0818659fdaeed3f0caf3d54794', 'category': 'NarrativeText', 'element_id': '0b84cddb33b11d1782f222b01b33ecf8'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 155 ---\n",
      "e Case 2: Financial Table Navigation. This case addresses a common challenge in financial document analysis. Analysts must extract specific metrics from tables with similar terminology and multiple time periods. Figure 4 shows this scenario. The query involves resolving ambiguous financial terms and selecting the correct column for a specified year.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1084.0)), (np.float64(301.0), np.float64(1200.0)), (np.float64(1400.0), np.float64(1200.0)), (np.float64(1400.0), np.float64(1084.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a0776b0818659fdaeed3f0caf3d54794', 'category': 'NarrativeText', 'element_id': '0da8484a35e33dc236fa4c9a8b412e13'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 156 ---\n",
      "RAG-Anything transforms the financial report table into a structured graph. Each row header, column header (year), data cell, and unit becomes a node. The edges capture key relationships: row-of, column-of, header-applies-to, and unit-of. This structure enables precise navigation. The retriever focuses on the row “Wages and salaries” and the column for “2020”. It directs attention to the target cell (26,778 million). The system successfully disambiguates nearby entries like “Share-based payments\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1222.0)), (np.float64(300.0), np.float64(1460.0)), (np.float64(1403.0), np.float64(1460.0)), (np.float64(1403.0), np.float64(1222.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a0776b0818659fdaeed3f0caf3d54794', 'category': 'NarrativeText', 'element_id': '5326469dd7c1819e594cdf8831e725d8'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 157 ---\n",
      "e Key Insights. Both cases demonstrate how RAG-Anything’s structure-aware design delivers targeted advantages. Our approach transforms documents into explicit graph representations. These graphs capture intra-modal relationships that traditional methods miss. In figures, connections between panels, captions, and axes enable panel-level comparisons. This goes beyond keyword matching. In tables, row—column-unit graphs ensure accurate identification through modeling.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1482.0)), (np.float64(300.0), np.float64(1629.0)), (np.float64(1399.0), np.float64(1629.0)), (np.float64(1399.0), np.float64(1482.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a0776b0818659fdaeed3f0caf3d54794', 'category': 'NarrativeText', 'element_id': 'e284e757264e281d6cd2a27c444fad23'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 158 ---\n",
      "This structure-aware retrieval design reduces confusion from repeated terminology and complex layouts. Traditional RAG systems struggle with these scenarios due to lack of structural understanding. Even MMGraphRAG fails here because it only considers image modality entities. It ignores other modality entities like table cells, row headers, and column headers. RAG-Anything’s comprehensive graph representation captures all modality-specific entities and their relationships. This enables precise, m\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1651.0)), (np.float64(299.0), np.float64(1859.0)), (np.float64(1403.0), np.float64(1859.0)), (np.float64(1403.0), np.float64(1651.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a0776b0818659fdaeed3f0caf3d54794', 'category': 'NarrativeText', 'element_id': '42483ef4de34c1398ee879829c113e58'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 159 ---\n",
      "4 RELATED WORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1909.0)), (np.float64(301.0), np.float64(1932.0)), (np.float64(586.0), np.float64(1932.0)), (np.float64(586.0), np.float64(1909.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '3a908cbd93489da843c0c2a621598d6f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 160 ---\n",
      "e Graph-Enhanced Retrieval-Augmented Generation. Large language models struggle with long-context inputs and multi-hop queries, failing to precisely locate dispersed evidence (Zhang et al.,\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1977.0)), (np.float64(301.0), np.float64(2033.0)), (np.float64(1402.0), np.float64(2033.0)), (np.float64(1402.0), np.float64(1977.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '3a908cbd93489da843c0c2a621598d6f', 'category': 'NarrativeText', 'element_id': '2a2ee79b53ae946ac41485b9378273f2'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 161 ---\n",
      "10\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(839.0), np.float64(2091.0)), (np.float64(839.0), np.float64(2110.0)), (np.float64(862.0), np.float64(2110.0)), (np.float64(862.0), np.float64(2091.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 10, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '3a908cbd93489da843c0c2a621598d6f', 'category': 'UncategorizedText', 'element_id': '0b257f657c6163b7d0f05d80c90a2331'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 162 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 11, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '6a13df6d641e0e80b4cf56a2917efd0d'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 163 ---\n",
      "2025). Graph structures address this limitation by introducing explicit relational modeling, improving both retrieval efficiency and reasoning accuracy (Bei et al., 2025).\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(236.0)), (np.float64(300.0), np.float64(291.0)), (np.float64(1399.0), np.float64(291.0)), (np.float64(1399.0), np.float64(236.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 11, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6a13df6d641e0e80b4cf56a2917efd0d', 'category': 'NarrativeText', 'element_id': '0910f7047b476e4556e2c07d442162d0'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 164 ---\n",
      "Since GraphRAG (Edge et al., 2024), research has evolved along two complementary directions. First, graph construction approaches optimize structures for retrieval efficiency, ranging from Ligh- tRAG’s (Guo et al., 2024) sparsified indices to neural models like GNN-RAG (Mavromatis & Karypis, 2024) and memory-augmented variants like HippoRAG (Jimenez Gutierrez et al., 2024). Second, knowledge aggregation approaches integrate information for multi-level reasoning through hier- archical methods lik\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(313.0)), (np.float64(301.0), np.float64(586.0)), (np.float64(1403.0), np.float64(586.0)), (np.float64(1403.0), np.float64(313.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 11, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6a13df6d641e0e80b4cf56a2917efd0d', 'category': 'NarrativeText', 'element_id': '6ad3fbc4a247ab531e24e4c10fb6682e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 165 ---\n",
      "e Multimodal Retrieval-Augmented Generation. Multimodal RAG represents a natural evolution from text-based RAG systems, addressing the need to integrate external knowledge from diverse data modalities for comprehensive response generation (Abootorabi et al., 2025). However, current approaches are fundamentally constrained by their reliance on modality-specific architectures. Exist- ing methods demonstrate these constraints across domains: VideoRAG (Ren et al., 2025) employs dual-channel architec\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(604.0)), (np.float64(300.0), np.float64(903.0)), (np.float64(1403.0), np.float64(903.0)), (np.float64(1403.0), np.float64(604.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 11, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6a13df6d641e0e80b4cf56a2917efd0d', 'category': 'NarrativeText', 'element_id': '4e517cb031e851e26128e806b61a8e5f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 166 ---\n",
      "The fundamental problem underlying these limitations is architectural fragmentation. Current systems require specialized processing pipelines for each modality. This creates poor generalizability as new modalities demand custom architectures and fusion mechanisms. Such fragmentation introduces cross-modal alignment difficulties, modality biases, and information bottlenecks. These issues systematically compromise system performance and scalability. RAG-Anything addresses this fragmentation throug\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(925.0)), (np.float64(299.0), np.float64(1163.0)), (np.float64(1400.0), np.float64(1163.0)), (np.float64(1400.0), np.float64(925.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 11, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6a13df6d641e0e80b4cf56a2917efd0d', 'category': 'NarrativeText', 'element_id': '6694f0a72266952eb173eadf65834d45'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 167 ---\n",
      "5 CONCLUSION\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(302.0), np.float64(1214.0)), (np.float64(302.0), np.float64(1238.0)), (np.float64(542.0), np.float64(1238.0)), (np.float64(542.0), np.float64(1214.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 11, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'c183abb55279a40a236e81b9bda7e37a'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 168 ---\n",
      "RAG-Anything introduces a paradigm shift in multimodal retrieval through its unified graph-based framework. Our core technical innovation is the dual-graph construction strategy that seamlessly integrates cross-modal and text-based knowledge graphs. Rather than forcing diverse modalities into text-centric pipelines that lose critical structural information, our approach fundamentally reconcep- tualizes multimodal content as interconnected knowledge entities with rich semantic relationships. The \n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1284.0)), (np.float64(299.0), np.float64(1583.0)), (np.float64(1403.0), np.float64(1583.0)), (np.float64(1403.0), np.float64(1284.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 11, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'c183abb55279a40a236e81b9bda7e37a', 'category': 'NarrativeText', 'element_id': 'c1c5a1fd8a78b41c1b656fb930cd2b8e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 169 ---\n",
      "Our analysis in Appendix A.5 reveals critical challenges facing current multimodal RAG systems. Two fundamental issues emerge through systematic failure case examination. First, systems exhibit text-centric retrieval bias, preferentially accessing textual sources even when queries explicitly require visual information. Second, rigid spatial processing patterns fail to adapt to non-standard document layouts. These limitations manifest in cross-modal misalignment scenarios and structurally ambiguo\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1605.0)), (np.float64(299.0), np.float64(1812.0)), (np.float64(1403.0), np.float64(1812.0)), (np.float64(1403.0), np.float64(1605.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 11, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'c183abb55279a40a236e81b9bda7e37a', 'category': 'NarrativeText', 'element_id': 'b8e131162bcc91189fd1408f5db03a4d'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 170 ---\n",
      "11\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(839.0), np.float64(2091.0)), (np.float64(839.0), np.float64(2110.0)), (np.float64(859.0), np.float64(2110.0)), (np.float64(859.0), np.float64(2091.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 11, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'c183abb55279a40a236e81b9bda7e37a', 'category': 'UncategorizedText', 'element_id': 'b3845a489298b3bf41f9748088888f94'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 171 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '157a2bcc33a9f6740b70e5556f42e3de'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 172 ---\n",
      "REFERENCES\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(233.0)), (np.float64(301.0), np.float64(255.0)), (np.float64(485.0), np.float64(255.0)), (np.float64(485.0), np.float64(233.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 173 ---\n",
      "Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammad- khani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin Asgari. Ask in any modality: A comprehensive survey on multimodal retrieval-augmented genera- tion. arXiv preprint arXiv:2502.08826, 2025.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(285.0)), (np.float64(301.0), np.float64(401.0)), (np.float64(1403.0), np.float64(401.0)), (np.float64(1403.0), np.float64(285.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': '31e3a626cd83e7dccd08b097fa985846'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 174 ---\n",
      "Yuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi Chen, Sheng Zhou, Hao Chen, Yong Li, Jiajun Bu, Shirui Pan, Yizhou Yu, et al. Graphs meet ai agents: Taxonomy, progress, and future opportunities. arXiv preprint arXiv:2506.18019, 2025.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(429.0)), (np.float64(301.0), np.float64(514.0)), (np.float64(1403.0), np.float64(514.0)), (np.float64(1403.0), np.float64(429.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': '1cc454e0e0d61e82d7119d7e972c28c0'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 175 ---\n",
      "Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(542.0)), (np.float64(301.0), np.float64(628.0)), (np.float64(1402.0), np.float64(628.0)), (np.float64(1402.0), np.float64(542.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': '1a3a45f49b9b47f1a9ef0d00e64d4f32'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 176 ---\n",
      "Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval- augmented generation. arXiv preprint arXiv:2410.05779, 2024.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(656.0)), (np.float64(300.0), np.float64(711.0)), (np.float64(1403.0), np.float64(711.0)), (np.float64(1403.0), np.float64(656.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': '56f5463dc16467cb52b9643d00aece33'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 177 ---\n",
      "Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neuro- biologically inspired long-term memory for large language models. NeurIPS, 37:59532-59569, 2024.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(739.0)), (np.float64(301.0), np.float64(818.0)), (np.float64(1403.0), np.float64(818.0)), (np.float64(1403.0), np.float64(739.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': 'fe05951ec369c79186eb2839f4586a5f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 178 ---\n",
      "Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, and Lijuan Wang. Mm-vid: Advancing video understanding with gpt-4v(ision). arXiv preprint arXiv:2310.19773, 2023.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(852.0)), (np.float64(301.0), np.float64(938.0)), (np.float64(1399.0), np.float64(938.0)), (np.float64(1399.0), np.float64(852.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': '0d9d376e3c431088bf79feeaa373d195'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 179 ---\n",
      "Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. Advances in Neural Information Processing Systems, 37:95963-96010, 2024.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(965.0)), (np.float64(301.0), np.float64(1051.0)), (np.float64(1402.0), np.float64(1051.0)), (np.float64(1402.0), np.float64(965.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': '26345b62767ac0a75a59f115c9975bda'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 180 ---\n",
      "Costas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model reasoning. arXiv preprint arXiv:2405.20139, 2024.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1079.0)), (np.float64(301.0), np.float64(1134.0)), (np.float64(1399.0), np.float64(1134.0)), (np.float64(1399.0), np.float64(1079.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': '8f4c7ac92d68208be64e133ceb72e9e4'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 181 ---\n",
      "Xubin Ren, Lingrui Xu, Long Xia, Shuaigiang Wang, Dawei Yin, and Chao Huang. Vide- orag: Retrieval-augmented generation with extreme long-context videos. arXiv preprint arXiv:2502.01549, 2025.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1162.0)), (np.float64(301.0), np.float64(1246.0)), (np.float64(1403.0), np.float64(1246.0)), (np.float64(1403.0), np.float64(1162.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': '4868d8bd54929556c301f6f03b3cb1d0'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 182 ---\n",
      "Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning. Raptor: Recursive abstractive processing for tree-organized retrieval. In The Twelfth International Conference on Learning Representations, 2024.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1275.0)), (np.float64(301.0), np.float64(1361.0)), (np.float64(1403.0), np.float64(1361.0)), (np.float64(1403.0), np.float64(1275.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': 'e08d42a795677d750f0fc5ac2016823b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 183 ---\n",
      "Xueyao Wan and Hang Yu. Mmgraphrag: Bridging vision and language with interpretable multimodal knowledge graphs. arXiv preprint arXiv:2507.20804, 2025.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1389.0)), (np.float64(300.0), np.float64(1444.0)), (np.float64(1399.0), np.float64(1444.0)), (np.float64(1399.0), np.float64(1389.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': 'f58fd257e6f67278e6062b69283c50b4'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 184 ---\n",
      "Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document content extraction. arXiv preprint arXiv:2409. 18839, 2024.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1472.0)), (np.float64(301.0), np.float64(1558.0)), (np.float64(1402.0), np.float64(1558.0)), (np.float64(1402.0), np.float64(1472.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': '8a502798c116d92953985524e4f279d2'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 185 ---\n",
      "Shu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, and Yuchi Ma. Archrag: Attributed community- based hierarchical retrieval-augmented generation. arXiv preprint arXiv:2502.09891, 2025.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1585.0)), (np.float64(301.0), np.float64(1641.0)), (np.float64(1403.0), np.float64(1641.0)), (np.float64(1403.0), np.float64(1585.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': '5aedafc92a3da4ec0e7754caee1ae595'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 186 ---\n",
      "Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, and Maosong Sun. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. arXiv preprint arXiv:2410.10594, 2025.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1668.0)), (np.float64(301.0), np.float64(1754.0)), (np.float64(1402.0), np.float64(1754.0)), (np.float64(1402.0), np.float64(1668.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': 'a944fce209bda028a2491c38564438c3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 187 ---\n",
      "Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen, Yilin Xiao, Chuang Zhou, Yi Chang, and Xiao Huang. A survey of graph retrieval-augmented generation for customized large language models. arXiv preprint arXiv:2501.13958, 2025.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1782.0)), (np.float64(301.0), np.float64(1868.0)), (np.float64(1402.0), np.float64(1868.0)), (np.float64(1402.0), np.float64(1782.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': '7aa34c05df817d33cd9f2d28dfe77c1a'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 188 ---\n",
      "Anni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, and Dong Yu. Docbench: A benchmark for evaluating lIm-based document reading systems. arXiv preprint arXiv:2407.10701, 2024.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1895.0)), (np.float64(301.0), np.float64(1981.0)), (np.float64(1399.0), np.float64(1981.0)), (np.float64(1399.0), np.float64(1895.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'NarrativeText', 'element_id': '4610201c258758378d8a1cffe8a7a759'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 189 ---\n",
      "12\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(839.0), np.float64(2091.0)), (np.float64(839.0), np.float64(2110.0)), (np.float64(862.0), np.float64(2110.0)), (np.float64(862.0), np.float64(2091.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 12, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'ae2c28d20c5ce8206b6cc3cee003a9b9', 'category': 'UncategorizedText', 'element_id': 'e895d0b8b4bace00ba9c89037a676f1c'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 190 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'caf7446c6173bd3cdf2cd124f41675e4'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 191 ---\n",
      "A APPENDIX\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(233.0)), (np.float64(301.0), np.float64(255.0)), (np.float64(506.0), np.float64(255.0)), (np.float64(506.0), np.float64(233.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'd12a34df64531de2c5c004f5e8bf0b62'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 192 ---\n",
      "This appendix provides comprehensive supporting materials for our experimental evaluation and implementation details. Section A.1 presents detailed dataset statistics for the DocBench and MMLongBench multi-modal benchmarks, including document type distributions and complexity metrics. Section A.2 showcases additional case studies that demonstrate RAG-Anything’s structure- aware capabilities across diverse multimodal content understanding tasks. Section A.3 documents the complete set of multimoda\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(299.0)), (np.float64(299.0), np.float64(537.0)), (np.float64(1403.0), np.float64(537.0)), (np.float64(1403.0), np.float64(299.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd12a34df64531de2c5c004f5e8bf0b62', 'category': 'NarrativeText', 'element_id': '214298b5334954b6ee75dfb83c94145d'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 193 ---\n",
      "A.1 DATASET CHARACTERISTICS AND STATISTICS\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(580.0)), (np.float64(301.0), np.float64(599.0)), (np.float64(906.0), np.float64(599.0)), (np.float64(906.0), np.float64(580.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'd2c392d9165168bccf36f410b3e0beac'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 194 ---\n",
      "Table 5: Document type distribution and statistics for the DocBench benchmark.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(404.0), np.float64(640.0)), (np.float64(404.0), np.float64(665.0)), (np.float64(1293.0), np.float64(665.0)), (np.float64(1293.0), np.float64(640.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'b4a250936d1dcac5ecc88037fd2ec436'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 195 ---\n",
      "Type Acad. Fin. Gov. Law. News # Docs 49 40 44 46 50 # Questions 303 288 = 148 191 172 Avg. Pages 11 192 69 58 1\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(559.0), np.float64(704.0)), (np.float64(559.0), np.float64(838.0)), (np.float64(1136.0), np.float64(838.0)), (np.float64(1136.0), np.float64(704.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'b4a250936d1dcac5ecc88037fd2ec436', 'category': 'UncategorizedText', 'element_id': '6a27d66e50646f4e12322da59d6a10b9'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 196 ---\n",
      "Table 6: Document type distribution and statistics for the MMLongBench benchmark.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(373.0), np.float64(891.0)), (np.float64(373.0), np.float64(916.0)), (np.float64(1324.0), np.float64(916.0)), (np.float64(1324.0), np.float64(891.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '8a5775361ace653f184cc7f9bc4b5cd5'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 197 ---\n",
      "Type Res. Tut. Acad. Guid. Broch. Admin. Fin. # Docs 34 17 26 22 15 10 11 # Questions 292 138 199 155 100 81 117 Avg. Pages 39 58 35 78 30 17 87\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(454.0), np.float64(956.0)), (np.float64(454.0), np.float64(1090.0)), (np.float64(1241.0), np.float64(1090.0)), (np.float64(1241.0), np.float64(956.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8a5775361ace653f184cc7f9bc4b5cd5', 'category': 'UncategorizedText', 'element_id': '7e268818e9774e3ac3c7ee0567704da0'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 198 ---\n",
      "Tables 5 and 6 present the distribution of document types across the DocBench and MMLong- Bench benchmarks. e DocBench encompasses medium- to long-length documents spanning various domains, including legal, governmental, and financial files. Financial reports represent the most extensive category, averaging 192 pages per document, while the News category consists of concise single-page newspapers. e MMLongBench demonstrates a broader spectrum of document types and lengths. Research reports, tuto\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1136.0)), (np.float64(299.0), np.float64(1378.0)), (np.float64(1403.0), np.float64(1378.0)), (np.float64(1403.0), np.float64(1136.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8a5775361ace653f184cc7f9bc4b5cd5', 'category': 'NarrativeText', 'element_id': '8c8474a88e5ceaeddfd6e39ece83767d'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 199 ---\n",
      "Collectively, these two benchmarks provide comprehensive coverage ranging from brief news arti- cles to extensive technical and financial documentation. This establishes diverse and challenging evaluation contexts for multimodal document understanding tasks.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1400.0)), (np.float64(301.0), np.float64(1486.0)), (np.float64(1403.0), np.float64(1486.0)), (np.float64(1403.0), np.float64(1400.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8a5775361ace653f184cc7f9bc4b5cd5', 'category': 'NarrativeText', 'element_id': 'a0742540dd186e9d69707162c7f5780c'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 200 ---\n",
      "A.2 ADDITIONAL CASE STUDIES\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1528.0)), (np.float64(301.0), np.float64(1547.0)), (np.float64(708.0), np.float64(1547.0)), (np.float64(708.0), np.float64(1528.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '853e6d70aee9d90fc1a0d64198618fb8'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 201 ---\n",
      "Cueto: Which GCAN submodel component moval LightRAG®: resulted in the lowest agcuracy for Twitter15 Removing the source tweet from the GCAN model attention mechanism from the resulted in the Iowest accuracy for Twitter15, as\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(388.0), np.float64(1587.0)), (np.float64(388.0), np.float64(1654.0)), (np.float64(1382.0), np.float64(1654.0)), (np.float64(1382.0), np.float64(1587.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '853e6d70aee9d90fc1a0d64198618fb8', 'category': 'NarrativeText', 'element_id': '63a1ad609fefcd89690c0795e2d45825'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 202 ---\n",
      "SA WA wR OG WC WAL GCAN sub-model resulted in the lowest accuracy for Twitter!5. MMGraphRAG@: = os ‘model resulted in the lowest accuracy for\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(362.0), np.float64(1652.0)), (np.float64(362.0), np.float64(1784.0)), (np.float64(1024.0), np.float64(1784.0)), (np.float64(1024.0), np.float64(1652.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '853e6d70aee9d90fc1a0d64198618fb8', 'category': 'NarrativeText', 'element_id': '0b62528b4ec8752a842c605f31b41744'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 203 ---\n",
      "‘Twitterl5, as indicated by signi\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(798.0), np.float64(1790.0)), (np.float64(798.0), np.float64(1802.0)), (np.float64(966.0), np.float64(1802.0)), (np.float64(966.0), np.float64(1790.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '853e6d70aee9d90fc1a0d64198618fb8', 'category': 'NarrativeText', 'element_id': 'a74316e549e94397523dbb3f26930b1b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 204 ---\n",
      "Figure 4: GCA drops in the ablation analysis in Figure 4.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(527.0), np.float64(1799.0)), (np.float64(527.0), np.float64(1828.0)), (np.float64(1026.0), np.float64(1828.0)), (np.float64(1026.0), np.float64(1799.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '853e6d70aee9d90fc1a0d64198618fb8', 'category': 'NarrativeText', 'element_id': 'b0352e80a60d777cdbe8d458e29e05b7'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 205 ---\n",
      "ablation analysis in Accuracy\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(608.0), np.float64(1805.0)), (np.float64(608.0), np.float64(1815.0)), (np.float64(736.0), np.float64(1815.0)), (np.float64(736.0), np.float64(1805.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '3237f0b4ac205bb7d97109a4a3665597'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 206 ---\n",
      "Multimodel Document Evidence figure in the document\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(328.0), np.float64(1824.0)), (np.float64(328.0), np.float64(1837.0)), (np.float64(734.0), np.float64(1837.0)), (np.float64(734.0), np.float64(1824.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '68d1476f595ad41f07a34436cff88b30'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 207 ---\n",
      "indicated by a significant drop in performance when using the sub-model that excluded both source tweet embeddings and dual co-attention,\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(1087.0), np.float64(1660.0)), (np.float64(1087.0), np.float64(1709.0)), (np.float64(1382.0), np.float64(1709.0)), (np.float64(1382.0), np.float64(1660.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '68d1476f595ad41f07a34436cff88b30', 'category': 'NarrativeText', 'element_id': '65313fd09a5b6a29b724ad85b44433c5'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 208 ---\n",
      "RAG-Anything(Correct’®):\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(1088.0), np.float64(1731.0)), (np.float64(1088.0), np.float64(1746.0)), (np.float64(1257.0), np.float64(1746.0)), (np.float64(1257.0), np.float64(1731.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'bd6f9a9de6060f045926717c16a06d92'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 209 ---\n",
      "The removal of the source tweet embeddings and dua configuration \"-S-A,\" resulted in the lowest accuracy for Twitter15.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(1087.0), np.float64(1753.0)), (np.float64(1087.0), np.float64(1829.0)), (np.float64(1369.0), np.float64(1829.0)), (np.float64(1369.0), np.float64(1753.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'bd6f9a9de6060f045926717c16a06d92', 'category': 'NarrativeText', 'element_id': '81e9aff3ec01969ac531413ce29c583d'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 210 ---\n",
      "‘o-attention, indicated as model\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(1153.0), np.float64(1774.0)), (np.float64(1153.0), np.float64(1785.0)), (np.float64(1349.0), np.float64(1785.0)), (np.float64(1349.0), np.float64(1774.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'bd6f9a9de6060f045926717c16a06d92', 'category': 'NarrativeText', 'element_id': 'e78e8114ca2321242735f3e9cca3e1c1'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 211 ---\n",
      "Figure 5: Visual reasoning case. RAG-Anything correctly identifies \"-S-A\" as the lowest accuracy configuration, while baselines misinterpret spatial relationships.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1869.0)), (np.float64(301.0), np.float64(1924.0)), (np.float64(1400.0), np.float64(1924.0)), (np.float64(1400.0), np.float64(1869.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'bd6f9a9de6060f045926717c16a06d92', 'category': 'NarrativeText', 'element_id': '7de706cdb1f2a89339fcb269c304360e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 212 ---\n",
      "e Demonstrating Visual Reasoning Capabilities. Figure 5 illustrates how RAG-Anything handles complex visual reasoning tasks involving chart interpretation. The query asks which GCAN sub- model component removal yields the lowest accuracy on Twitter15. Traditional approaches struggle\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1947.0)), (np.float64(301.0), np.float64(2033.0)), (np.float64(1403.0), np.float64(2033.0)), (np.float64(1403.0), np.float64(1947.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'bd6f9a9de6060f045926717c16a06d92', 'category': 'NarrativeText', 'element_id': 'd54ef846908651370e95da2a7fc64e9b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 213 ---\n",
      "13\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(839.0), np.float64(2091.0)), (np.float64(839.0), np.float64(2110.0)), (np.float64(861.0), np.float64(2110.0)), (np.float64(861.0), np.float64(2091.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 13, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'bd6f9a9de6060f045926717c16a06d92', 'category': 'UncategorizedText', 'element_id': '14f1e2472aa4e06d89bf1fcbfc4c8d1d'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 214 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '7050c521e2c2cee52a22b8afc475e277'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 215 ---\n",
      "with spatial relationships between visual elements. RAG-Anything addresses this challenge by constructing a structured graph representation of the bar plot. Bars, axis labels, and legends become interconnected nodes. These are linked by semantic relations such as bar-of and label-applies-to.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(236.0)), (np.float64(300.0), np.float64(321.0)), (np.float64(1400.0), np.float64(321.0)), (np.float64(1400.0), np.float64(236.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '7050c521e2c2cee52a22b8afc475e277', 'category': 'NarrativeText', 'element_id': '9f85e1d0bde991171f8cf78e379ca359'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 216 ---\n",
      "This graph-based approach enables precise alignment between visual and textual elements. The system correctly identifies the bar labeled \"-S-A\" (removing source tweet embeddings and dual co-attention) and its corresponding accuracy value as the lowest performer. Baseline methods that flatten visual information often misinterpret spatial relationships. They frequently conflate nearby components. RAG-Anything’s structured representation preserves critical visual-textual associations. This leads to\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(343.0)), (np.float64(299.0), np.float64(521.0)), (np.float64(1403.0), np.float64(521.0)), (np.float64(1403.0), np.float64(343.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '7050c521e2c2cee52a22b8afc475e277', 'category': 'NarrativeText', 'element_id': '49b39d9740494785133711fe74eaaaa5'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 217 ---\n",
      "‘Question: Which model combination for the Evidence Inference\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(360.0), np.float64(592.0)), (np.float64(360.0), np.float64(602.0)), (np.float64(672.0), np.float64(602.0)), (np.float64(672.0), np.float64(592.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '52c8723320c445854ff5cc553629da4e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 218 ---\n",
      "dataset has the highest AUPRC value? GPT -4o-mini: LightRAG®:\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(423.0), np.float64(600.0)), (np.float64(423.0), np.float64(619.0)), (np.float64(1151.0), np.float64(619.0)), (np.float64(1151.0), np.float64(600.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '52c8723320c445854ff5cc553629da4e', 'category': 'NarrativeText', 'element_id': '0aea29e5a352d3c9c670154f5444ef34'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 219 ---\n",
      "The model combination for the Evidence Inference The model combination \"BERT + LSTM - dataset with the highest AUPRC value is the BERT- Attention\" has the highest AUPRC value for the to-BERT model, achieving an AUPRC of 0.455. Evidence Inference dataset at 0.429, MMGraphRAG®: RAG-Anything(Correct®): The model combination for the Evidence Inference The model combination for the Evidence dataset with the highest AUPRC values the \"Bett-To- Jy erence dataset with the highest AUPRC\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(742.0), np.float64(620.0)), (np.float64(742.0), np.float64(764.0)), (np.float64(1379.0), np.float64(764.0)), (np.float64(1379.0), np.float64(620.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '52c8723320c445854ff5cc553629da4e', 'category': 'NarrativeText', 'element_id': 'be7d3058267d38a9a841fafe2350240c'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 220 ---\n",
      "= Bert\" model, which achieved an AUPRC score of value is GloVe + LSTM - Attention, achieving\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(374.0), np.float64(760.0)), (np.float64(374.0), np.float64(781.0)), (np.float64(1378.0), np.float64(781.0)), (np.float64(1378.0), np.float64(760.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '52c8723320c445854ff5cc553629da4e', 'category': 'NarrativeText', 'element_id': '2109df3f10dcd7322fff5774ad30abcc'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 221 ---\n",
      "Multimode! Document. Evidence table in the document 0.708. a value of 0.506.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(326.0), np.float64(779.0)), (np.float64(326.0), np.float64(800.0)), (np.float64(1180.0), np.float64(800.0)), (np.float64(1180.0), np.float64(779.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '51059c90972684008a3369e11bf74bce'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 222 ---\n",
      "Figure 6: Tabular navigation case. RAG-Anything locates the highest AUPRC value (0.506), while the compared approaches struggle with structural ambiguity.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(845.0)), (np.float64(301.0), np.float64(901.0)), (np.float64(1399.0), np.float64(901.0)), (np.float64(1399.0), np.float64(845.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '51059c90972684008a3369e11bf74bce', 'category': 'NarrativeText', 'element_id': '290a0b486d4788973e473aa60c950dc0'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 223 ---\n",
      "e Handling Complex Tabular Structures. Figure 6 showcases RAG-Anything’s ability to navigate intricate tabular data where structural disambiguation is crucial. The query seeks the model combi- nation achieving the highest AUPRC value for the Evidence Inference dataset—a task complicated by repeated row labels across multiple datasets within the same table. This scenario highlights a fundamental limitation of conventional approaches that struggle with structural ambiguity in data.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(950.0)), (np.float64(300.0), np.float64(1097.0)), (np.float64(1403.0), np.float64(1097.0)), (np.float64(1403.0), np.float64(950.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '51059c90972684008a3369e11bf74bce', 'category': 'NarrativeText', 'element_id': 'd75072801cb44e2b2a450bb74c7d2a09'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 224 ---\n",
      "RAG-Anything overcomes this by parsing the table into a comprehensive relational graph where headers and data cells become nodes connected through explicit row-of and column-of relationships. This structured representation enables the system to correctly isolate the Evidence Inference dataset context and identify \"GloVe + LSTM — Attention\" with a score of 0.506 as the optimal configuration. By explicitly preserving hierarchical table constraints that other methods often collapse or misinterpret,\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1119.0)), (np.float64(299.0), np.float64(1296.0)), (np.float64(1403.0), np.float64(1296.0)), (np.float64(1403.0), np.float64(1119.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '51059c90972684008a3369e11bf74bce', 'category': 'NarrativeText', 'element_id': '3489b1ffaf32d45dee337d59f6a04b09'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 225 ---\n",
      "A.3> CONTEXT-AWARE MULTIMODAL PROMPTING\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1371.0)), (np.float64(301.0), np.float64(1390.0)), (np.float64(906.0), np.float64(1390.0)), (np.float64(906.0), np.float64(1371.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'be53a36d9f92c06608a0b817f950a85a'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 226 ---\n",
      "These three prompts orchestrate structured, context-aware multimodal analysis with JSON-formatted outputs. They systematically guide the model to extract comprehensive descriptions of visual, tabular, and mathematical content while maintaining explicit alignment with surrounding information.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1441.0)), (np.float64(299.0), np.float64(1526.0)), (np.float64(1402.0), np.float64(1526.0)), (np.float64(1402.0), np.float64(1441.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'be53a36d9f92c06608a0b817f950a85a', 'category': 'NarrativeText', 'element_id': '5c2f86ee65a7acafd88adb50e48d3221'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 227 ---\n",
      "Vision Analysis Prompt. Figure 7 orchestrates comprehensive image-context integration. The prompt directs the model to systematically capture compositional elements, object relationships, visual attributes, stylistic features, dynamic actions, and technical components (e.g., charts), while es- tablishing explicit connections to accompanying text. This approach transcends superficial description, enabling contextually-grounded interpretations that enhance knowledge retrieval and substantiation.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1549.0)), (np.float64(300.0), np.float64(1695.0)), (np.float64(1403.0), np.float64(1695.0)), (np.float64(1403.0), np.float64(1549.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'be53a36d9f92c06608a0b817f950a85a', 'category': 'NarrativeText', 'element_id': '9f6d3c7f63a9de0862652c574a2089d9'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 228 ---\n",
      "Table Analysis Prompt. Figure 8 structures systematic tabular content decomposition across multiple analytical dimensions: structural organization, column semantics, critical values, statistical patterns, and contextual relevance. Through precise terminology and numerical accuracy requirements, the prompt eliminates ambiguous generalizations and ensures faithful preservation of key indicators while maintaining coherent alignment with surrounding discourse.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1717.0)), (np.float64(300.0), np.float64(1864.0)), (np.float64(1402.0), np.float64(1864.0)), (np.float64(1402.0), np.float64(1717.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'be53a36d9f92c06608a0b817f950a85a', 'category': 'NarrativeText', 'element_id': '083096c1161e85eefd27292e219e1302'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 229 ---\n",
      "Equation Analysis Prompt. Figure 9 prioritizes semantic interpretation over syntactic restatement of mathematical expressions. The prompt instructs comprehensive analysis of variable definitions, operational logic, theoretical foundations, inter-formula relationships, and practical applications. This methodology ensures mathematical content becomes integral to broader argumentative frameworks, supporting enhanced retrieval accuracy, analytical traceability, and reasoning coherence.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1886.0)), (np.float64(301.0), np.float64(2033.0)), (np.float64(1402.0), np.float64(2033.0)), (np.float64(1402.0), np.float64(1886.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'be53a36d9f92c06608a0b817f950a85a', 'category': 'NarrativeText', 'element_id': 'f368b5d468952e5f3a0bf1d9b97b0522'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 230 ---\n",
      "14\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(839.0), np.float64(2091.0)), (np.float64(839.0), np.float64(2110.0)), (np.float64(862.0), np.float64(2110.0)), (np.float64(862.0), np.float64(2091.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 14, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'be53a36d9f92c06608a0b817f950a85a', 'category': 'UncategorizedText', 'element_id': 'a0b4cb1861901a6cd1b40af47e7ddcdb'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 231 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(99.0)), (np.float64(936.0), np.float64(99.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '04d4420eb279480b2b3e3043fd4011fd'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 232 ---\n",
      "Vision Analysis Prompt\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(328.0), np.float64(259.0)), (np.float64(328.0), np.float64(281.0)), (np.float64(614.0), np.float64(281.0)), (np.float64(614.0), np.float64(259.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '5ce691f389711eee5de01bb9d02d8ee3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 233 ---\n",
      "eee vision_analysis_prompt.png\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(337.0), np.float64(300.0)), (np.float64(337.0), np.float64(312.0)), (np.float64(579.0), np.float64(312.0)), (np.float64(579.0), np.float64(300.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '4ea7eaadd13c617d97ea49bca9539d33'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 234 ---\n",
      "1 Please analyze this image in detail, considering the surrounding context. Provide a JSON response with the following structure\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(356.0), np.float64(324.0)), (np.float64(356.0), np.float64(360.0)), (np.float64(1328.0), np.float64(360.0)), (np.float64(1328.0), np.float64(324.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '4ea7eaadd13c617d97ea49bca9539d33', 'category': 'NarrativeText', 'element_id': 'c444ccce1a7ccc4da58457f90a92cb08'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 235 ---\n",
      "\"detailed_description\": “A comprehensive and detailed visual description of the image following these guidelines:\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(404.0)), (np.float64(381.0), np.float64(440.0)), (np.float64(1283.0), np.float64(440.0)), (np.float64(1283.0), np.float64(404.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '4ea7eaadd13c617d97ea49bca9539d33', 'category': 'NarrativeText', 'element_id': '21aad107449f31a4e90424a5e0da3d88'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 236 ---\n",
      "= Describe the overall composition and layout\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(444.0)), (np.float64(383.0), np.float64(460.0)), (np.float64(781.0), np.float64(460.0)), (np.float64(781.0), np.float64(444.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'ae8c89c5ed99ff4a55734da214ebcb5e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 237 ---\n",
      "Identify all objects, people, text, and visual elements\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(464.0)), (np.float64(399.0), np.float64(480.0)), (np.float64(889.0), np.float64(480.0)), (np.float64(889.0), np.float64(464.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '6a78b8f75ba618d13d768dca44fa1601'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 238 ---\n",
      "Explain relationships between elements and how they relate to the surrounding context\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(484.0)), (np.float64(399.0), np.float64(500.0)), (np.float64(1157.0), np.float64(500.0)), (np.float64(1157.0), np.float64(484.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6a78b8f75ba618d13d768dca44fa1601', 'category': 'NarrativeText', 'element_id': '952e3bf058bbd46f2a6637b7acc36af5'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 239 ---\n",
      "Note colors, lighting, and visual style\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(504.0)), (np.float64(399.0), np.float64(519.0)), (np.float64(746.0), np.float64(519.0)), (np.float64(746.0), np.float64(504.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6a78b8f75ba618d13d768dca44fa1601', 'category': 'NarrativeText', 'element_id': 'f86b113782e0f52917fc5a432efc7206'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 240 ---\n",
      "Describe any actions or activities shown\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(524.0)), (np.float64(399.0), np.float64(539.0)), (np.float64(755.0), np.float64(539.0)), (np.float64(755.0), np.float64(524.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6a78b8f75ba618d13d768dca44fa1601', 'category': 'NarrativeText', 'element_id': '6f7b90e1a6400ebcaa719c7a2099b18f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 241 ---\n",
      "Include technical details if relevant (charts, diagrams, etc.)\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(544.0)), (np.float64(399.0), np.float64(559.0)), (np.float64(950.0), np.float64(559.0)), (np.float64(950.0), np.float64(544.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'c3d638f4a750045d0e09815f75fe79a8'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 242 ---\n",
      "Reference connections to the surrounding content when relevant\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(564.0)), (np.float64(399.0), np.float64(579.0)), (np.float64(951.0), np.float64(579.0)), (np.float64(951.0), np.float64(564.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'c3d638f4a750045d0e09815f75fe79a8', 'category': 'NarrativeText', 'element_id': '289ae0d95aaee4bb52fa9491a55b7e44'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 243 ---\n",
      "14 - Always use specific names instead of pronouns\",\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(349.0), np.float64(584.0)), (np.float64(349.0), np.float64(599.0)), (np.float64(815.0), np.float64(599.0)), (np.float64(815.0), np.float64(584.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'c3d638f4a750045d0e09815f75fe79a8', 'category': 'NarrativeText', 'element_id': 'f5b8c044f6870a7572decac966cc25f2'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 244 ---\n",
      "“entity_info\": {\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(604.0)), (np.float64(383.0), np.float64(619.0)), (np.float64(522.0), np.float64(619.0)), (np.float64(522.0), np.float64(604.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '6fd69b08f44509b84283c140eac656eb'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 245 ---\n",
      "“entity_name\": \"{entity_name}\",\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(624.0)), (np.float64(383.0), np.float64(639.0)), (np.float64(654.0), np.float64(639.0)), (np.float64(654.0), np.float64(624.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6fd69b08f44509b84283c140eac656eb', 'category': 'UncategorizedText', 'element_id': 'fb2bcc7f3b3b29965f497405e9756d04'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 246 ---\n",
      "“entity_type\": \"image\"\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(644.0)), (np.float64(383.0), np.float64(659.0)), (np.float64(575.0), np.float64(659.0)), (np.float64(575.0), np.float64(644.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'd8e3ccbe02558f65fe32c6ccb7065d6d'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 247 ---\n",
      "\"summary\": “concise summary of the image content, its significance, and relationship to surrounding content (max 190 words)\"\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(664.0)), (np.float64(383.0), np.float64(698.0)), (np.float64(1336.0), np.float64(698.0)), (np.float64(1336.0), np.float64(664.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd8e3ccbe02558f65fe32c6ccb7065d6d', 'category': 'NarrativeText', 'element_id': '74634d890d3884c25b52b38615b19953'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 248 ---\n",
      "}\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(704.0)), (np.float64(382.0), np.float64(717.0)), (np.float64(388.0), np.float64(717.0)), (np.float64(388.0), np.float64(704.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd8e3ccbe02558f65fe32c6ccb7065d6d', 'category': 'UncategorizedText', 'element_id': 'df910fc88bb04c85104fdfcbd57e1f45'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 249 ---\n",
      "}\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(724.0)), (np.float64(382.0), np.float64(737.0)), (np.float64(388.0), np.float64(737.0)), (np.float64(388.0), np.float64(724.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd8e3ccbe02558f65fe32c6ccb7065d6d', 'category': 'UncategorizedText', 'element_id': 'd61fc0e1c52a53b5933833bc0fcac238'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 250 ---\n",
      "12\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(349.0), np.float64(544.0)), (np.float64(349.0), np.float64(553.0)), (np.float64(360.0), np.float64(553.0)), (np.float64(360.0), np.float64(544.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd8e3ccbe02558f65fe32c6ccb7065d6d', 'category': 'UncategorizedText', 'element_id': 'ab283a26df9dd578217bf58167bcdc6e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 251 ---\n",
      "Context from surrounding content: {context}\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(764.0)), (np.float64(381.0), np.float64(798.0)), (np.float64(672.0), np.float64(798.0)), (np.float64(672.0), np.float64(764.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd8e3ccbe02558f65fe32c6ccb7065d6d', 'category': 'NarrativeText', 'element_id': '7958e15595ac66ada77b78b24505f976'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 252 ---\n",
      "Image details: - Image Path: {image_path} + Captions: {captions}\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(824.0)), (np.float64(381.0), np.float64(879.0)), (np.float64(612.0), np.float64(879.0)), (np.float64(612.0), np.float64(824.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'e82c914533fda7e801d81887becc9219'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 253 ---\n",
      "Footnotes: {footnotes}\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(883.0)), (np.float64(383.0), np.float64(898.0)), (np.float64(594.0), np.float64(898.0)), (np.float64(594.0), np.float64(883.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'e82c914533fda7e801d81887becc9219', 'category': 'ListItem', 'element_id': '419cfc3f57365396bdbc7b6f8ae980e3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 254 ---\n",
      "Focus on providing accurate, detailed visual analysis that incorporates the context and would be useful for knowledge retrieval.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(923.0)), (np.float64(382.0), np.float64(959.0)), (np.float64(1337.0), np.float64(959.0)), (np.float64(1337.0), np.float64(923.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'e82c914533fda7e801d81887becc9219', 'category': 'NarrativeText', 'element_id': 'c625c53b4f20b753acf4ce2849a647d0'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 255 ---\n",
      "Figure 7: Vision analysis prompt for context-aware image interpretation and knowledge extraction.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(303.0), np.float64(1020.0)), (np.float64(303.0), np.float64(1045.0)), (np.float64(1396.0), np.float64(1045.0)), (np.float64(1396.0), np.float64(1020.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '7a390b43a467eb67a3f8ad80430a7621'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 256 ---\n",
      "Table Analysis Prompt\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(328.0), np.float64(1148.0)), (np.float64(328.0), np.float64(1170.0)), (np.float64(601.0), np.float64(1170.0)), (np.float64(601.0), np.float64(1148.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'd7ad2323d2f05c6a8864bc66b8566596'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 257 ---\n",
      "eee table_analysis_pronpt.png 1 Please analyze this table content considering the surrounding context, and provide a JSON response with the\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(337.0), np.float64(1189.0)), (np.float64(337.0), np.float64(1229.0)), (np.float64(1337.0), np.float64(1229.0)), (np.float64(1337.0), np.float64(1189.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd7ad2323d2f05c6a8864bc66b8566596', 'category': 'NarrativeText', 'element_id': 'b1f544d95642709682c369c640ba2fb6'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 258 ---\n",
      "following structure\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(1233.0)), (np.float64(381.0), np.float64(1249.0)), (np.float64(550.0), np.float64(1249.0)), (np.float64(550.0), np.float64(1233.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd7ad2323d2f05c6a8864bc66b8566596', 'category': 'NarrativeText', 'element_id': '12d5d66055ec091c1b8070ad449bb393'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 259 ---\n",
      "“detailed_description\": “A comprehensive analysis of the table including: Table structure and organization\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(1293.0)), (np.float64(383.0), np.float64(1329.0)), (np.float64(1030.0), np.float64(1329.0)), (np.float64(1030.0), np.float64(1293.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd7ad2323d2f05c6a8864bc66b8566596', 'category': 'NarrativeText', 'element_id': 'be79bc3486e92f61d31db5153be84b42'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 260 ---\n",
      "Column headers and their meanings\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(1333.0)), (np.float64(399.0), np.float64(1349.0)), (np.float64(692.0), np.float64(1349.0)), (np.float64(692.0), np.float64(1333.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'c7dfc7d3bc619b6312fac614cad6bafb'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 261 ---\n",
      "Key data points and patterns\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(1353.0)), (np.float64(399.0), np.float64(1369.0)), (np.float64(647.0), np.float64(1369.0)), (np.float64(647.0), np.float64(1353.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '0db505660aec14360a27526d9dea6ed2'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 262 ---\n",
      "Statistical insights and trends\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(1373.0)), (np.float64(399.0), np.float64(1389.0)), (np.float64(674.0), np.float64(1389.0)), (np.float64(674.0), np.float64(1373.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '0047a9cae11ef9933bb2d583bda575d4'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 263 ---\n",
      "Relationships between data elements\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(1393.0)), (np.float64(399.0), np.float64(1408.0)), (np.float64(710.0), np.float64(1408.0)), (np.float64(710.0), np.float64(1393.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '5a1d6ccf28450ffd15d0ece19fa9fea5'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 264 ---\n",
      "Significance of the data presented in relation to surrounding context\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(1413.0)), (np.float64(399.0), np.float64(1428.0)), (np.float64(1014.0), np.float64(1428.0)), (np.float64(1014.0), np.float64(1413.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '5a1d6ccf28450ffd15d0ece19fa9fea5', 'category': 'NarrativeText', 'element_id': '8c4d09be1fd57f7ef262b5e20e199dc5'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 265 ---\n",
      "12 - How the table supports or illustrates concepts from the surrounding content\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(349.0), np.float64(1433.0)), (np.float64(349.0), np.float64(1448.0)), (np.float64(1068.0), np.float64(1448.0)), (np.float64(1068.0), np.float64(1433.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '5a1d6ccf28450ffd15d0ece19fa9fea5', 'category': 'NarrativeText', 'element_id': 'df3f371a1ae6e67752296ab054407eee'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 266 ---\n",
      "13. Always use specific names and values instead of general references.\",\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(349.0), np.float64(1453.0)), (np.float64(349.0), np.float64(1468.0)), (np.float64(994.0), np.float64(1468.0)), (np.float64(994.0), np.float64(1453.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '5a1d6ccf28450ffd15d0ece19fa9fea5', 'category': 'ListItem', 'element_id': '4c410394754b3c2d55eae33e6ac53bce'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 267 ---\n",
      "14 “entity_info\": {\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(349.0), np.float64(1473.0)), (np.float64(349.0), np.float64(1488.0)), (np.float64(522.0), np.float64(1488.0)), (np.float64(522.0), np.float64(1473.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'f664343b5ac2cba4463870a0c683be26'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 268 ---\n",
      "“entity_name\": \"{entity_name}\",\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(1493.0)), (np.float64(383.0), np.float64(1508.0)), (np.float64(654.0), np.float64(1508.0)), (np.float64(654.0), np.float64(1493.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'f664343b5ac2cba4463870a0c683be26', 'category': 'UncategorizedText', 'element_id': '52ddbf15b03949009c285bb7a466857e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 269 ---\n",
      "“entity_type\": \"table\",\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(1513.0)), (np.float64(383.0), np.float64(1528.0)), (np.float64(582.0), np.float64(1528.0)), (np.float64(582.0), np.float64(1513.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'f664343b5ac2cba4463870a0c683be26', 'category': 'UncategorizedText', 'element_id': 'af47b3a9d2b94052bf6a3c3ff4cdd790'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 270 ---\n",
      "\"summary\": “concise summary of the table's purpose, key findings, and relationship to surrounding content (max 100 words)\"\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(1533.0)), (np.float64(382.0), np.float64(1567.0)), (np.float64(1364.0), np.float64(1567.0)), (np.float64(1364.0), np.float64(1533.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'f664343b5ac2cba4463870a0c683be26', 'category': 'NarrativeText', 'element_id': '62b7270b7aa8901659b0379b543db81f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 271 ---\n",
      "+\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(1573.0)), (np.float64(382.0), np.float64(1587.0)), (np.float64(388.0), np.float64(1587.0)), (np.float64(388.0), np.float64(1573.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'f664343b5ac2cba4463870a0c683be26', 'category': 'UncategorizedText', 'element_id': 'dc74ce19ac59443f7623628a53e6673f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 272 ---\n",
      "}\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(1593.0)), (np.float64(382.0), np.float64(1606.0)), (np.float64(388.0), np.float64(1606.0)), (np.float64(388.0), np.float64(1593.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'f664343b5ac2cba4463870a0c683be26', 'category': 'UncategorizedText', 'element_id': 'dd442489af0ff5760dd2f6ca3e8ebf38'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 273 ---\n",
      "Context from surrounding content: {context}\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(1633.0)), (np.float64(381.0), np.float64(1667.0)), (np.float64(672.0), np.float64(1667.0)), (np.float64(672.0), np.float64(1633.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'f664343b5ac2cba4463870a0c683be26', 'category': 'NarrativeText', 'element_id': 'b0b2a797370a564b426b3f3d4fa97b6b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 274 ---\n",
      "Table Information:\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(380.0), np.float64(1693.0)), (np.float64(380.0), np.float64(1705.0)), (np.float64(538.0), np.float64(1705.0)), (np.float64(538.0), np.float64(1693.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'ef4ca4b18ebb678e8360856b7bb56cbe'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 275 ---\n",
      "Image Path: {table_img_path} Caption; {table_caption} Body: {table_body} Footnotes: {table_footnote}\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(1713.0)), (np.float64(381.0), np.float64(1788.0)), (np.float64(629.0), np.float64(1788.0)), (np.float64(629.0), np.float64(1713.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'e93cc3c81779ec792b246cb9572ff86f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 276 ---\n",
      "Focus on extracting meaningful insights and relationships from the tabular data in the context of the surrounding content.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(1812.0)), (np.float64(382.0), np.float64(1848.0)), (np.float64(1283.0), np.float64(1848.0)), (np.float64(1283.0), np.float64(1812.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'e93cc3c81779ec792b246cb9572ff86f', 'category': 'NarrativeText', 'element_id': '53bd4797e507c2b53d9e904977360cb7'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 277 ---\n",
      "Figure 8: Table analysis prompt for structured content decomposition and semantic understanding.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(305.0), np.float64(1909.0)), (np.float64(305.0), np.float64(1934.0)), (np.float64(1394.0), np.float64(1934.0)), (np.float64(1394.0), np.float64(1909.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '62e7023130c5a9bdef7b4e4d4e364bd2'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 278 ---\n",
      "15\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(838.0), np.float64(2091.0)), (np.float64(838.0), np.float64(2111.0)), (np.float64(861.0), np.float64(2111.0)), (np.float64(861.0), np.float64(2091.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 15, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '62e7023130c5a9bdef7b4e4d4e364bd2', 'category': 'UncategorizedText', 'element_id': 'f5ad430be405999a9a3649be99ea6976'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 279 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(99.0)), (np.float64(936.0), np.float64(99.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '020fc8aedcf70ec16a5827257beac0b3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 280 ---\n",
      "Equation Analysis Prompt\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(330.0), np.float64(259.0)), (np.float64(330.0), np.float64(281.0)), (np.float64(640.0), np.float64(281.0)), (np.float64(640.0), np.float64(259.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'b12749e6b9430d13ca61632b5f99539f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 281 ---\n",
      "equation_analysis_prompt.png Please analyze this mathematical equation considering the surrounding context, and provide a JSON response\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(300.0)), (np.float64(381.0), np.float64(340.0)), (np.float64(1328.0), np.float64(340.0)), (np.float64(1328.0), np.float64(300.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'b12749e6b9430d13ca61632b5f99539f', 'category': 'NarrativeText', 'element_id': '9ba756ffde292b70f9494683a0cb85e0'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 282 ---\n",
      "with the following structure:\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(380.0), np.float64(344.0)), (np.float64(380.0), np.float64(360.0)), (np.float64(636.0), np.float64(360.0)), (np.float64(636.0), np.float64(344.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '491cfc2ed235555ccdbd93ebc52c69a3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 283 ---\n",
      "“detailed_description\": “A comprehensive analysis of the equation including: Mathematical meaning and interpretation\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(404.0)), (np.float64(383.0), np.float64(440.0)), (np.float64(1057.0), np.float64(440.0)), (np.float64(1057.0), np.float64(404.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '491cfc2ed235555ccdbd93ebc52c69a3', 'category': 'NarrativeText', 'element_id': '89a85105fb2d614ee8f8123539ceed80'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 284 ---\n",
      "Variables and their definitions in the context of surrounding content\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(444.0)), (np.float64(399.0), np.float64(460.0)), (np.float64(1014.0), np.float64(460.0)), (np.float64(1014.0), np.float64(444.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '491cfc2ed235555ccdbd93ebc52c69a3', 'category': 'NarrativeText', 'element_id': 'babff403efb38d7ade1ffce0e14884e6'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 285 ---\n",
      "Mathematical operations and functions used\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(464.0)), (np.float64(399.0), np.float64(480.0)), (np.float64(773.0), np.float64(480.0)), (np.float64(773.0), np.float64(464.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '491cfc2ed235555ccdbd93ebc52c69a3', 'category': 'NarrativeText', 'element_id': '0127c602a9527c0b9be5c8346499eabf'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 286 ---\n",
      "Application domain and context based on surrounding material\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(398.0), np.float64(484.0)), (np.float64(398.0), np.float64(499.0)), (np.float64(934.0), np.float64(499.0)), (np.float64(934.0), np.float64(484.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '491cfc2ed235555ccdbd93ebc52c69a3', 'category': 'NarrativeText', 'element_id': '25b95c707790fd82777e4cacbf8522e3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 287 ---\n",
      "Physical or theoretical significance\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(504.0)), (np.float64(399.0), np.float64(519.0)), (np.float64(720.0), np.float64(519.0)), (np.float64(720.0), np.float64(504.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '42d4202754054df4f5350275c0315960'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 288 ---\n",
      "Relationship to other mathematical concepts mentioned in the context\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(524.0)), (np.float64(399.0), np.float64(539.0)), (np.float64(1005.0), np.float64(539.0)), (np.float64(1005.0), np.float64(524.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '42d4202754054df4f5350275c0315960', 'category': 'NarrativeText', 'element_id': 'a0a86fe2bb9c7680114a8c367a956709'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 289 ---\n",
      "Practical applications or use cases\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(399.0), np.float64(544.0)), (np.float64(399.0), np.float64(559.0)), (np.float64(710.0), np.float64(559.0)), (np.float64(710.0), np.float64(544.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'd4b316b9c4d92cf51256541e6bb05cc5'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 290 ---\n",
      "How the equation relates to the broader discussion or framework\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(564.0)), (np.float64(383.0), np.float64(579.0)), (np.float64(961.0), np.float64(579.0)), (np.float64(961.0), np.float64(564.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd4b316b9c4d92cf51256541e6bb05cc5', 'category': 'ListItem', 'element_id': 'ccb93dc497eaac55a92e3f9d568142f3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 291 ---\n",
      "Always use specific mathematical terminology\",\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(380.0), np.float64(584.0)), (np.float64(380.0), np.float64(599.0)), (np.float64(797.0), np.float64(599.0)), (np.float64(797.0), np.float64(584.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'd4b316b9c4d92cf51256541e6bb05cc5', 'category': 'NarrativeText', 'element_id': 'f9bcba30b151afaebbea5fadc5bf735e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 292 ---\n",
      "“entity_info\": {\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(604.0)), (np.float64(383.0), np.float64(619.0)), (np.float64(522.0), np.float64(619.0)), (np.float64(522.0), np.float64(604.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '864d607d90875e743fe75fb586bd3cfd'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 293 ---\n",
      "“entity_name\": \"{entity_name}\"\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(624.0)), (np.float64(383.0), np.float64(639.0)), (np.float64(646.0), np.float64(639.0)), (np.float64(646.0), np.float64(624.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'd0002a65f49f4c1c435b3050dd90243b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 294 ---\n",
      "“entity_type\": \"equation\"\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(644.0)), (np.float64(383.0), np.float64(659.0)), (np.float64(602.0), np.float64(659.0)), (np.float64(602.0), np.float64(644.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'a6212f31d4c5598ecad9dfb5decb3cf3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 295 ---\n",
      "\"summary\": “concise summary of the equation's purpose, significance, and role in the surrounding context (max 100 words)\"\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(664.0)), (np.float64(382.0), np.float64(697.0)), (np.float64(1354.0), np.float64(697.0)), (np.float64(1354.0), np.float64(664.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a6212f31d4c5598ecad9dfb5decb3cf3', 'category': 'NarrativeText', 'element_id': 'a187e5acb8a5a89899e9ba2a02e018d4'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 296 ---\n",
      "}\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(704.0)), (np.float64(382.0), np.float64(717.0)), (np.float64(388.0), np.float64(717.0)), (np.float64(388.0), np.float64(704.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a6212f31d4c5598ecad9dfb5decb3cf3', 'category': 'UncategorizedText', 'element_id': 'ad4f4eaca7c76c5d169bb94a16613185'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 297 ---\n",
      "}\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(724.0)), (np.float64(382.0), np.float64(737.0)), (np.float64(388.0), np.float64(737.0)), (np.float64(388.0), np.float64(724.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a6212f31d4c5598ecad9dfb5decb3cf3', 'category': 'UncategorizedText', 'element_id': '3646faafdc52f1b16b3ff9603ebd9a43'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 298 ---\n",
      "Context from surrounding content: {context}\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(764.0)), (np.float64(381.0), np.float64(798.0)), (np.float64(672.0), np.float64(798.0)), (np.float64(672.0), np.float64(764.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a6212f31d4c5598ecad9dfb5decb3cf3', 'category': 'NarrativeText', 'element_id': '71de8e76fc2238c963676b5d9da2375c'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 299 ---\n",
      "Equation Information Equation: {equation_text} Format: {equation_format}\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(824.0)), (np.float64(381.0), np.float64(879.0)), (np.float64(603.0), np.float64(879.0)), (np.float64(603.0), np.float64(824.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'b630af4cd9c6771e8153e1a327e3ae2a'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 300 ---\n",
      "Focus on providing mathematical insights and explaining the equation's significance within the broader context.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(903.0)), (np.float64(381.0), np.float64(935.0)), (np.float64(1292.0), np.float64(935.0)), (np.float64(1292.0), np.float64(903.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'b630af4cd9c6771e8153e1a327e3ae2a', 'category': 'NarrativeText', 'element_id': 'e6dd73d2b3a84cebaf3ebe21b59c8354'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 301 ---\n",
      "Figure 9: Equation analysis prompt for mathematical expression interpretation and integration.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(326.0), np.float64(990.0)), (np.float64(326.0), np.float64(1015.0)), (np.float64(1372.0), np.float64(1015.0)), (np.float64(1372.0), np.float64(990.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '478ec2ef702a7520b090a2ec3797bd36'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 302 ---\n",
      "Accuracy Evaluation Prompt\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(328.0), np.float64(1049.0)), (np.float64(328.0), np.float64(1071.0)), (np.float64(666.0), np.float64(1071.0)), (np.float64(666.0), np.float64(1049.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '557f79d0c3034b49ff20fd0eff1b912f'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 303 ---\n",
      "1\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(356.0), np.float64(1115.0)), (np.float64(356.0), np.float64(1123.0)), (np.float64(360.0), np.float64(1123.0)), (np.float64(360.0), np.float64(1115.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '557f79d0c3034b49ff20fd0eff1b912f', 'category': 'UncategorizedText', 'element_id': 'aeb3a6152933f7517b7b8c7255a35528'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 304 ---\n",
      "accuracy_evaluation_prompt.png\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(384.0), np.float64(1090.0)), (np.float64(384.0), np.float64(1102.0)), (np.float64(608.0), np.float64(1102.0)), (np.float64(608.0), np.float64(1090.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '5fa02fe3e2634d1b65f7d6596a8fcd92'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 305 ---\n",
      "You are an expert evaluator tasked with assessing the accuracy of answers generated by a RAG (Retrieval-Augmented Generation) system.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(1114.0)), (np.float64(381.0), np.float64(1150.0)), (np.float64(1203.0), np.float64(1150.0)), (np.float64(1203.0), np.float64(1114.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '5fa02fe3e2634d1b65f7d6596a8fcd92', 'category': 'NarrativeText', 'element_id': '4425f6deb550e0f3a6e00f21e3eb6de6'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 306 ---\n",
      "**Task**: Evaluate whether the generated answer correctly responds to the given question based on the expected answer.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(1174.0)), (np.float64(381.0), np.float64(1206.0)), (np.float64(1363.0), np.float64(1206.0)), (np.float64(1363.0), np.float64(1174.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '5fa02fe3e2634d1b65f7d6596a8fcd92', 'category': 'NarrativeText', 'element_id': '20461cf72f985860ccf7e26634265270'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 307 ---\n",
      "**Question**: {question} **Expected Answer**: {expected_answer}\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(1234.0)), (np.float64(382.0), np.float64(1290.0)), (np.float64(719.0), np.float64(1290.0)), (np.float64(719.0), np.float64(1234.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '39ed3aeced842e320cbb6e0b02f010e5'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 308 ---\n",
      "**Generated Answer**: {generated_answer}\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(1314.0)), (np.float64(382.0), np.float64(1329.0)), (np.float64(737.0), np.float64(1329.0)), (np.float64(737.0), np.float64(1314.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '71403203b1b02180c0ba69d4c6267a0e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 309 ---\n",
      "**Evaluation Criteriat*\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(1374.0)), (np.float64(382.0), np.float64(1386.0)), (np.float64(585.0), np.float64(1386.0)), (np.float64(585.0), np.float64(1374.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'a6309cde48af67f868e06c7789116ffd'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 310 ---\n",
      "1. **Accuracy (0 or 1)**: Does the generated answer match the factual content of the expected answer? - 1: The generated answer is factually correct and aligns with the expected answer\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(1394.0)), (np.float64(382.0), np.float64(1429.0)), (np.float64(1282.0), np.float64(1429.0)), (np.float64(1282.0), np.float64(1394.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a6309cde48af67f868e06c7789116ffd', 'category': 'ListItem', 'element_id': 'e7651f283273de72f1bd02e7e1893dc9'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 311 ---\n",
      "0: The generated answer is factually incorrect or contradicts the expected answer\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(1434.0)), (np.float64(383.0), np.float64(1449.0)), (np.float64(1122.0), np.float64(1449.0)), (np.float64(1122.0), np.float64(1434.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a6309cde48af67f868e06c7789116ffd', 'category': 'ListItem', 'element_id': '929c9b45fc6f3908742d926148e467c1'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 312 ---\n",
      "tInstructions**: - Focus on factual correctness, not writing style or format\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(1474.0)), (np.float64(382.0), np.float64(1509.0)), (np.float64(907.0), np.float64(1509.0)), (np.float64(907.0), np.float64(1474.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a6309cde48af67f868e06c7789116ffd', 'category': 'ListItem', 'element_id': '0b054384e2928390cd99de15b5662dff'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 313 ---\n",
      "Consider partial matches: if the generated answer contains the correct information but includes additional context, it should still be considered accurate\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(1514.0)), (np.float64(381.0), np.float64(1547.0)), (np.float64(1345.0), np.float64(1547.0)), (np.float64(1345.0), np.float64(1514.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a6309cde48af67f868e06c7789116ffd', 'category': 'ListItem', 'element_id': '4bd6d56bfe64c811ee2d468e74930369'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 314 ---\n",
      "For numerical answers, check if the values match or are equivalent\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(1554.0)), (np.float64(383.0), np.float64(1569.0)), (np.float64(987.0), np.float64(1569.0)), (np.float64(987.0), np.float64(1554.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a6309cde48af67f868e06c7789116ffd', 'category': 'ListItem', 'element_id': '09f344e2cd9e2b545ce2b20656e37cf9'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 315 ---\n",
      "For list answers, check if all key elements are present\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(1574.0)), (np.float64(383.0), np.float64(1589.0)), (np.float64(889.0), np.float64(1589.0)), (np.float64(889.0), np.float64(1574.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a6309cde48af67f868e06c7789116ffd', 'category': 'ListItem', 'element_id': 'c323c129e9fb1fb9aefb119f80dc2f29'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 316 ---\n",
      "If the expected answer is \"Not answerable\" and the generated answer indicates inability to answer, consider it accurate\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(1594.0)), (np.float64(381.0), np.float64(1626.0)), (np.float64(1355.0), np.float64(1626.0)), (np.float64(1355.0), np.float64(1594.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a6309cde48af67f868e06c7789116ffd', 'category': 'NarrativeText', 'element_id': '4cd550db3a1c37ad3930d07508628faa'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 317 ---\n",
      "**Qutput Format**;\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(1654.0)), (np.float64(382.0), np.float64(1669.0)), (np.float64(538.0), np.float64(1669.0)), (np.float64(538.0), np.float64(1654.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'a96dc3be0da2e53cbd17f4db5e69597a'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 318 ---\n",
      "Please respond with a JSON object containing only {\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(381.0), np.float64(1673.0)), (np.float64(381.0), np.float64(1707.0)), (np.float64(818.0), np.float64(1707.0)), (np.float64(818.0), np.float64(1673.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a96dc3be0da2e53cbd17f4db5e69597a', 'category': 'NarrativeText', 'element_id': 'eb723c1d38f11f2113c15efad8039595'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 319 ---\n",
      "\"accuracy\": © or 1,\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(1714.0)), (np.float64(383.0), np.float64(1729.0)), (np.float64(547.0), np.float64(1729.0)), (np.float64(547.0), np.float64(1714.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': 'a96dc3be0da2e53cbd17f4db5e69597a', 'category': 'UncategorizedText', 'element_id': '5ca87f41391259e6cb49a01b73193156'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 320 ---\n",
      "\"reasoning\": \"Brief explanation of your evaluation\"\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(383.0), np.float64(1733.0)), (np.float64(383.0), np.float64(1749.0)), (np.float64(834.0), np.float64(1749.0)), (np.float64(834.0), np.float64(1733.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '6ae79eeec47745bf7cefa0f973f784b9'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 321 ---\n",
      "+\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(382.0), np.float64(1754.0)), (np.float64(382.0), np.float64(1767.0)), (np.float64(388.0), np.float64(1767.0)), (np.float64(388.0), np.float64(1754.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '6ae79eeec47745bf7cefa0f973f784b9', 'category': 'UncategorizedText', 'element_id': '14a31f51e376e80ece13e87d1db28a21'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 322 ---\n",
      "Figure 10: Accuracy evaluation prompt for consistent factual assessment across question types.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(323.0), np.float64(1830.0)), (np.float64(323.0), np.float64(1855.0)), (np.float64(1375.0), np.float64(1855.0)), (np.float64(1375.0), np.float64(1830.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '9ffc54edd5bc98dcde4feaa7561f7430'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 323 ---\n",
      "A.4. ACCURACY EVALUATION PROMPT DESIGN\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1888.0)), (np.float64(301.0), np.float64(1907.0)), (np.float64(875.0), np.float64(1907.0)), (np.float64(875.0), np.float64(1888.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '89fa78d16326221a988366d210f47d40'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 324 ---\n",
      "Figure 10 presents the standardized prompt specifically designed for systematic factual accuracy as- sessment of generated responses across multiple domains. The prompt establishes explicit evaluation criteria that prioritize content correctness over stylistic considerations, producing binary accuracy\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1947.0)), (np.float64(300.0), np.float64(2033.0)), (np.float64(1403.0), np.float64(2033.0)), (np.float64(1403.0), np.float64(1947.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '89fa78d16326221a988366d210f47d40', 'category': 'NarrativeText', 'element_id': '274f0d75999e9698ff1b3ce0b46e7dc6'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 325 ---\n",
      "16\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(839.0), np.float64(2091.0)), (np.float64(839.0), np.float64(2110.0)), (np.float64(862.0), np.float64(2110.0)), (np.float64(862.0), np.float64(2091.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 16, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '89fa78d16326221a988366d210f47d40', 'category': 'UncategorizedText', 'element_id': '492d8639d825820bf0f67e3dc7a06f81'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 326 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '8448aaab17ca9e801ad6891ebdac599a'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 327 ---\n",
      "classifications accompanied by concise analytical justifications. All accuracy evaluations throughout our comprehensive experimental framework were conducted using GPT-40-mini, ensuring consistent and reliable assessment standards across diverse question categories and specialized domains.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(236.0)), (np.float64(301.0), np.float64(321.0)), (np.float64(1400.0), np.float64(321.0)), (np.float64(1400.0), np.float64(236.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8448aaab17ca9e801ad6891ebdac599a', 'category': 'NarrativeText', 'element_id': '99b177ff1c91112c82f3d17c84d10d52'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 328 ---\n",
      "A.5 CHALLENGES AND FUTURE DIRECTIONS FOR MULTI-MODAL RAG\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(366.0)), (np.float64(301.0), np.float64(385.0)), (np.float64(1160.0), np.float64(385.0)), (np.float64(1160.0), np.float64(366.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '60cc6ad6dd1b3650c9352f5fc4167c19'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 329 ---\n",
      "While current multimodal RAG systems demonstrate promising capabilities, their limitations emerge most clearly through systematic analysis of failure cases. Understanding where and why these systems break down is crucial for advancing the field beyond current performance plateaus. Examining failure patterns helps identify fundamental architectural bottlenecks and design principles for more robust multimodal systems. Our investigation reveals two critical failure patterns exposing deeper systemic\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(424.0)), (np.float64(299.0), np.float64(631.0)), (np.float64(1400.0), np.float64(631.0)), (np.float64(1400.0), np.float64(424.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '60cc6ad6dd1b3650c9352f5fc4167c19', 'category': 'NarrativeText', 'element_id': '67eab20f65991b0d0381ed0c889d7b52'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 330 ---\n",
      "e Text-Centric Retrieval Bias: Systems exhibit strong preference for textual sources, even when queries explicitly demand visual information. This reveals inadequate cross-modal attention.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(664.0)), (np.float64(301.0), np.float64(719.0)), (np.float64(1399.0), np.float64(719.0)), (np.float64(1399.0), np.float64(664.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '60cc6ad6dd1b3650c9352f5fc4167c19', 'category': 'NarrativeText', 'element_id': '6392c7f70fe709dd3c6a5ce90fd3c2ee'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 331 ---\n",
      "e Document Structure Processing Challenges: Systems struggle with complex layouts and non- linear information flows. This exposes limitations in spatial reasoning and contextual understanding.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(741.0)), (np.float64(301.0), np.float64(797.0)), (np.float64(1403.0), np.float64(797.0)), (np.float64(1403.0), np.float64(741.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '60cc6ad6dd1b3650c9352f5fc4167c19', 'category': 'NarrativeText', 'element_id': 'e25521485ca0144027c314f336282daf'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 332 ---\n",
      "These failure modes illuminate key insights about current multimodal AI. They provide concrete directions for architectural innovations that could substantially improve system robustness.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(819.0)), (np.float64(299.0), np.float64(874.0)), (np.float64(1399.0), np.float64(874.0)), (np.float64(1399.0), np.float64(819.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '60cc6ad6dd1b3650c9352f5fc4167c19', 'category': 'NarrativeText', 'element_id': '1ab5d4139d9dbc3bb95a68fcc72dd489'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 333 ---\n",
      "‘Question: According to Figure 3, what is the sequence followed to generate the\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(312.0), np.float64(918.0)), (np.float64(312.0), np.float64(928.0)), (np.float64(670.0), np.float64(928.0)), (np.float64(670.0), np.float64(918.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '60cc6ad6dd1b3650c9352f5fc4167c19', 'category': 'NarrativeText', 'element_id': 'e7550eaede8758df3778b999c0164afa'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 334 ---\n",
      "final labels for Named Entity Revogt GPT-4o-mini®: LightRAG®: Expected ansmer The sequence is Characters > Char Embedding > Char involves inputting words and their representations a sequence where word representations are first HRCI 2 Nogl Eeatures > Word Representation > Word LSTMEE/B = into a Word CRF layer, followed by Word LSTM-B, _ processed through a convolution layer, followed by a Word LSTM-F, and character-level representations, __Bi-LSTM layer and then a softmax or CRF layer to\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(312.0), np.float64(922.0)), (np.float64(312.0), np.float64(1001.0)), (np.float64(1378.0), np.float64(1001.0)), (np.float64(1378.0), np.float64(922.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '60cc6ad6dd1b3650c9352f5fc4167c19', 'category': 'NarrativeText', 'element_id': '59392bb94f90552324c263dbfec086d8'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 335 ---\n",
      "—__ der in the figure is from bottom to top) culminating in the final prediction of entity labels. produce the final labels. 3 oa = MMGraphRAG @: RAG-Anything ®:\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(319.0), np.float64(1001.0)), (np.float64(319.0), np.float64(1062.0)), (np.float64(1200.0), np.float64(1062.0)), (np.float64(1200.0), np.float64(1001.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '60cc6ad6dd1b3650c9352f5fc4167c19', 'category': 'NarrativeText', 'element_id': '794979b938106ccea05cc0a037595710'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 336 ---\n",
      "involves using a combined representation of word _...involves processing input characters through\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(730.0), np.float64(1057.0)), (np.float64(730.0), np.float64(1087.0)), (np.float64(1378.0), np.float64(1087.0)), (np.float64(1378.0), np.float64(1057.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '60cc6ad6dd1b3650c9352f5fc4167c19', 'category': 'NarrativeText', 'element_id': '3b5a1229c4c1bda773514a1255e87bd4'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 337 ---\n",
      "= = embeddings and character-level convolution, which is pj-directional LSTM layers (Char LSTM-B a * | then processed through a Bi-LSTM layer, followed by and Char LSTM-F) followed by a softmax or 7 a softmax or CRF layer to produce the final labels. CRF layer that produces the final labels.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(538.0), np.float64(1086.0)), (np.float64(538.0), np.float64(1148.0)), (np.float64(1372.0), np.float64(1148.0)), (np.float64(1372.0), np.float64(1086.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '60cc6ad6dd1b3650c9352f5fc4167c19', 'category': 'NarrativeText', 'element_id': '99fd6b4dcfa85ed8fecca701b2359b5c'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 338 ---\n",
      "Multimodel Document [Evidence figure in the document\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(323.0), np.float64(1142.0)), (np.float64(323.0), np.float64(1152.0)), (np.float64(648.0), np.float64(1152.0)), (np.float64(648.0), np.float64(1142.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '50fe3c3ff9fda9c11e105865f598e246'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 339 ---\n",
      "Figure 11: Cross-modal noise case. All methods fail to retrieve the correct answer from the specified image, instead retrieving noisy textual evidence that misaligns with the structured visual content.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1193.0)), (np.float64(301.0), np.float64(1249.0)), (np.float64(1399.0), np.float64(1249.0)), (np.float64(1399.0), np.float64(1193.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '50fe3c3ff9fda9c11e105865f598e246', 'category': 'NarrativeText', 'element_id': '0d5b0d6fc38d3c466ce2ec54d0d5b4a7'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 340 ---\n",
      "‘Question: Which models style space shows a clearer separation\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(368.0), np.float64(1284.0)), (np.float64(368.0), np.float64(1312.0)), (np.float64(705.0), np.float64(1312.0)), (np.float64(705.0), np.float64(1284.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '50fe3c3ff9fda9c11e105865f598e246', 'category': 'NarrativeText', 'element_id': '985cab515dab6dd4fbfb308d7864104b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 341 ---\n",
      "GPT-4o-mini®: LighRAG®: between different styles according to Figure 2? i ht RAGS\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(412.0), np.float64(1300.0)), (np.float64(412.0), np.float64(1324.0)), (np.float64(1165.0), np.float64(1324.0)), (np.float64(1165.0), np.float64(1300.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '50fe3c3ff9fda9c11e105865f598e246', 'category': 'NarrativeText', 'element_id': '2d886ce73a4945ddf0c3df5bead4ea83'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 342 ---\n",
      "‘The Joint goal accuracy in the \"Train\" domain The Joint goa improved by 6.26% when using the GEM fine-tuning improved from 44,\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(776.0), np.float64(1310.0)), (np.float64(776.0), np.float64(1356.0)), (np.float64(1193.0), np.float64(1356.0)), (np.float64(1193.0), np.float64(1310.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '50fe3c3ff9fda9c11e105865f598e246', 'category': 'NarrativeText', 'element_id': 'a75417e1dd2136ebc1fc5a87caf40edc'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 343 ---\n",
      "e “Train” domain\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(1279.0), np.float64(1320.0)), (np.float64(1279.0), np.float64(1329.0)), (np.float64(1380.0), np.float64(1329.0)), (np.float64(1380.0), np.float64(1320.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': 'f6fb0b5aedaa803ac7bb3776043fe42e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 344 ---\n",
      "from scratch to\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(1299.0), np.float64(1338.0)), (np.float64(1299.0), np.float64(1347.0)), (np.float64(1380.0), np.float64(1347.0)), (np.float64(1380.0), np.float64(1338.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '4148f847adff40f07f7e55d0b8c925d9'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 345 ---\n",
      "— nw “oiat Sa [Hors] | strategy compared to training from scratch, 50.51% with GEM fine-tuning, indicating an Entutn on Domains Faeroe! |p Paan_| | 65.35% versus 59.09% improvement of 6.27% Regn Tuan 95755825 476 Le pe eunigot | Ye tom guie nar oo, | MMGraphRAG®®: RAG-Anything®: on I new domain | GEM | 5354 96.27 | 50.69 96.42 The Joint goal ac cy in the \"Train\" domain The Joint Goal Accuracy in the \"Train\" domain = z Totes Ne Fee | Hel oe improved from 44.24% when training from scratch to impro\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(330.0), np.float64(1342.0)), (np.float64(330.0), np.float64(1487.0)), (np.float64(1387.0), np.float64(1487.0)), (np.float64(1387.0), np.float64(1342.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '4148f847adff40f07f7e55d0b8c925d9', 'category': 'NarrativeText', 'element_id': '58775ed453bebd81d27d5f2a9aabe769'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 346 ---\n",
      "em | 1993792 resulting in an improvement of 15.597 strategy, resulting in an improvement of\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(605.0), np.float64(1475.0)), (np.float64(605.0), np.float64(1507.0)), (np.float64(1340.0), np.float64(1507.0)), (np.float64(1340.0), np.float64(1475.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '4148f847adff40f07f7e55d0b8c925d9', 'category': 'NarrativeText', 'element_id': 'bb29edbbafe4cfbbc5f04d43fa7c515e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 347 ---\n",
      "Multimodel Document Evidence table in the document 13.86%.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(328.0), np.float64(1503.0)), (np.float64(328.0), np.float64(1518.0)), (np.float64(1138.0), np.float64(1518.0)), (np.float64(1138.0), np.float64(1503.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '5850255265aea80b4f8418a200f31c70'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 348 ---\n",
      "Figure 12: Ambiguous table structure case. All methods fail to correctly parse the confusing table layout with merged cells and unclear column boundaries, leading to incorrect data extraction.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(1562.0)), (np.float64(301.0), np.float64(1618.0)), (np.float64(1399.0), np.float64(1618.0)), (np.float64(1399.0), np.float64(1562.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '5850255265aea80b4f8418a200f31c70', 'category': 'NarrativeText', 'element_id': '59c959c2691ad76d77bbddc0c8e5206e'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 349 ---\n",
      "Case 1: Cross-Modal Misalignment. Figure || presents a particularly revealing failure scenario where all evaluated methods consistently produce incorrect answers despite having access to the necessary information. This universal failure across different architectures suggests fundamental limitations in how current systems handle noisy, heterogeneous multimodal data—a critical challenge as real-world applications inevitably involve imperfect, inconsistent information sources. The failure exposes \n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1640.0)), (np.float64(300.0), np.float64(1817.0)), (np.float64(1399.0), np.float64(1817.0)), (np.float64(1399.0), np.float64(1640.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '5850255265aea80b4f8418a200f31c70', 'category': 'NarrativeText', 'element_id': 'c97e6b81a88c85b5faa6839007d560ba'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 350 ---\n",
      "Issue 1: Retrieval Bias Toward Text. Current RAG systems demonstrate pronounced bias toward textual passages. This occurs particularly when visual content lacks exact keyword matches. The bias persists even when queries contain explicit instructions to prioritize visual sources. This reveals a fundamental weakness in cross-modal attention mechanisms.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(1839.0)), (np.float64(300.0), np.float64(1949.0)), (np.float64(1399.0), np.float64(1949.0)), (np.float64(1399.0), np.float64(1839.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '5850255265aea80b4f8418a200f31c70', 'category': 'NarrativeText', 'element_id': '7e45b72c87bdecc868aff19a65471dac'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 351 ---\n",
      "The retrieved textual information, while topically related, often operates at a different granularity level than visual content. Images may contain precise, structured data such as specific numerical values,\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1977.0)), (np.float64(299.0), np.float64(2033.0)), (np.float64(1402.0), np.float64(2033.0)), (np.float64(1402.0), np.float64(1977.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '5850255265aea80b4f8418a200f31c70', 'category': 'NarrativeText', 'element_id': '3a6f51d7da94bd2155b22cfa6db8d55a'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 352 ---\n",
      "17\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(839.0), np.float64(2091.0)), (np.float64(839.0), np.float64(2110.0)), (np.float64(861.0), np.float64(2110.0)), (np.float64(861.0), np.float64(2091.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 17, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '5850255265aea80b4f8418a200f31c70', 'category': 'UncategorizedText', 'element_id': '844d8f844ada892247217dc49912e8a3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 353 ---\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(301.0), np.float64(79.0)), (np.float64(301.0), np.float64(98.0)), (np.float64(936.0), np.float64(98.0)), (np.float64(936.0), np.float64(79.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 18, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'category': 'Title', 'element_id': '8576be446478e4e86379b68275d8dd0b'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'category', 'element_id'])\n",
      "\n",
      "--- Document 354 ---\n",
      "detailed diagrams, or exact spatial relationships. Corresponding text typically provides general, conceptual descriptions. This semantic misalignment introduces noise that actively misleads the reasoning process. The system attempts to reconcile incompatible levels of detail and specificity.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(236.0)), (np.float64(300.0), np.float64(321.0)), (np.float64(1402.0), np.float64(321.0)), (np.float64(1402.0), np.float64(236.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 18, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8576be446478e4e86379b68275d8dd0b', 'category': 'NarrativeText', 'element_id': '7b8e2f86614f67c0bce4a6b5a38bbc12'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 355 ---\n",
      "Issue 2: Rigid Spatial Processing Patterns. Current visual processing models exhibit fundamental rigidity in spatial interpretation. Most systems default to sequential scanning patterns—top-to- bottom and left-to-right—that mirror natural reading conventions. While effective for simple text documents, this approach creates systematic failures with structurally complex real-world content. Many documents require non-conventional processing strategies. Tables demand column-wise interpretation, tech\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(343.0)), (np.float64(300.0), np.float64(582.0)), (np.float64(1403.0), np.float64(582.0)), (np.float64(1403.0), np.float64(343.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 18, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8576be446478e4e86379b68275d8dd0b', 'category': 'NarrativeText', 'element_id': '6b545de3a15bf268449357cff80035a3'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 356 ---\n",
      "In the observed failure case, the correct answer required integrating visual elements in reverse order from the model’s default processing sequence. The system’s inability to recognize and adapt to this structural requirement led to systematic misinterpretation. This represents a fundamental architectural limitation where spatial reasoning remains static regardless of document context or query intent. When spatial processing patterns are misaligned with document structure, the extracted informat\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(604.0)), (np.float64(299.0), np.float64(811.0)), (np.float64(1403.0), np.float64(811.0)), (np.float64(1403.0), np.float64(604.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 18, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8576be446478e4e86379b68275d8dd0b', 'category': 'NarrativeText', 'element_id': '01c3bd96375bf2d518908d2efede11cd'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 357 ---\n",
      "Case 2: Structural Noise in Ambiguous Table Layouts. As shown in Figure 12, all methods failed when confronted with a structurally ambiguous table. The primary failure stems from the table’s confusing design: the GEM row lacks dedicated cell boundaries, and the \"Joint\" and \"Slot\" columns merge without clear separation. These structural irregularities create parsing ambiguities that system- atically mislead extraction algorithms. This failure pattern reveals a critical vulnerability in current RA\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(300.0), np.float64(833.0)), (np.float64(300.0), np.float64(1100.0)), (np.float64(1403.0), np.float64(1100.0)), (np.float64(1403.0), np.float64(833.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 18, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8576be446478e4e86379b68275d8dd0b', 'category': 'NarrativeText', 'element_id': 'b62ae07b028c1c0f8eb04b2c9dbc97b6'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 358 ---\n",
      "The case highlights two essential directions for enhancing robustness. RAG systems require layout- aware parsing mechanisms that can recognize and adapt to structural irregularities rather than imposing rigid formatting assumptions. Additionally, integrating visual processing capabilities could significantly improve noise resilience, as visual models can leverage spatial relationships and contextual design cues that are lost in purely structural representations.\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(299.0), np.float64(1124.0)), (np.float64(299.0), np.float64(1271.0)), (np.float64(1403.0), np.float64(1271.0)), (np.float64(1403.0), np.float64(1124.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 18, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8576be446478e4e86379b68275d8dd0b', 'category': 'NarrativeText', 'element_id': '0b226c6c7c2df7e5f8f4834a2dcad698'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n",
      "--- Document 359 ---\n",
      "18\n",
      "Metadata: {'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf', 'coordinates': {'points': ((np.float64(839.0), np.float64(2091.0)), (np.float64(839.0), np.float64(2110.0)), (np.float64(861.0), np.float64(2110.0)), (np.float64(861.0), np.float64(2091.0))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-10-21T15:20:27', 'page_number': 18, 'file_directory': 'D:\\\\Learning-lab\\\\Test', 'filename': 'sample.pdf', 'parent_id': '8576be446478e4e86379b68275d8dd0b', 'category': 'UncategorizedText', 'element_id': '9d0d0871134ed382391104a569db8a18'}\n",
      "dict_keys(['source', 'coordinates', 'filetype', 'languages', 'last_modified', 'page_number', 'file_directory', 'filename', 'parent_id', 'category', 'element_id'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 逐一列印\n",
    "for i, doc in enumerate(pdf_docs, start=1):\n",
    "    print(f\"--- Document {i} ---\")\n",
    "    print(doc.page_content[:500])  # 前 500 字\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "    print(doc.metadata.keys())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d21f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Element 1 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 1\n",
      "Content snippet: 2510.12323v1 [cs.AI] 14 Oct 2025\n",
      "\n",
      "--- Element 2 ---\n",
      "Category: Title\n",
      "Page number: 1\n",
      "Content snippet: arXiv\n",
      "\n",
      "--- Element 3 ---\n",
      "Category: Title\n",
      "Page number: 1\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 4 ---\n",
      "Category: Title\n",
      "Page number: 1\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 5 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 1\n",
      "Content snippet: Zirui Guo, Xubin Ren, Lingrui Xu, Jiahao Zhang, Chao Huang* The University of Hong Kong zrguol01@hku.hk xubinrencs@gmail.com chaohuang75@gmail.com\n",
      "\n",
      "--- Element 6 ---\n",
      "Category: Title\n",
      "Page number: 1\n",
      "Content snippet: ABSTRACT\n",
      "\n",
      "--- Element 7 ---\n",
      "Category: NarrativeText\n",
      "Page number: 1\n",
      "Content snippet: Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding Large Language Models beyond their static training limitations. However, a critical misalignment exists between\n",
      "\n",
      "--- Element 8 ---\n",
      "Category: Title\n",
      "Page number: 1\n",
      "Content snippet: 1 INTRODUCTION\n",
      "\n",
      "--- Element 9 ---\n",
      "Category: NarrativeText\n",
      "Page number: 1\n",
      "Content snippet: Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding the knowledge boundaries of Large Language Models (LLM) beyond their static training limita- tions Zhang et al.\n",
      "\n",
      "--- Element 10 ---\n",
      "Category: NarrativeText\n",
      "Page number: 1\n",
      "Content snippet: However, existing RAG frameworks focus exclusively on text-only knowledge while neglecting the rich multimodal information present in real-world documents. This limitation fundamentally mis- aligns wi\n",
      "\n",
      "--- Element 11 ---\n",
      "Category: NarrativeText\n",
      "Page number: 1\n",
      "Content snippet: The consequences of this limitation become particularly severe in document-intensive domains where multimodal content carries essential meaning. Academic research, financial analysis, and technical do\n",
      "\n",
      "--- Element 12 ---\n",
      "Category: NarrativeText\n",
      "Page number: 1\n",
      "Content snippet: The consequences of this limitation become particularly severe in knowledge-intensive domains where multimodal content carries essential meaning. Three representative scenarios illustrate the critical\n",
      "\n",
      "--- Element 13 ---\n",
      "Category: ListItem\n",
      "Page number: 1\n",
      "Content snippet: Corresponding Author: Chao Huang\n",
      "\n",
      "--- Element 14 ---\n",
      "Category: Title\n",
      "Page number: 2\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 15 ---\n",
      "Category: NarrativeText\n",
      "Page number: 2\n",
      "Content snippet: need for multimodal RAG capabilities. In Scientific Research, experimental results are primarily communicated through plots, diagrams, and statistical visualizations. These contain core discoveries th\n",
      "\n",
      "--- Element 16 ---\n",
      "Category: NarrativeText\n",
      "Page number: 2\n",
      "Content snippet: Addressing multimodal RAG presents three fundamental technical challenges that demand principled solutions. This makes it significantly more complex than traditional text-only approaches. The naive so\n",
      "\n",
      "--- Element 17 ---\n",
      "Category: NarrativeText\n",
      "Page number: 2\n",
      "Content snippet: Technical Challenges. e First, the unified multimodal representation challenge requires seam- lessly integrating diverse information types. The system must preserve their unique characteristics and cr\n",
      "\n",
      "--- Element 18 ---\n",
      "Category: NarrativeText\n",
      "Page number: 2\n",
      "Content snippet: Our Contributions. To address these challenges, we introduce RAG-Anything, a unified framework that fundamentally reimagines multimodal knowledge representation and retrieval. Our approach employs a d\n",
      "\n",
      "--- Element 19 ---\n",
      "Category: NarrativeText\n",
      "Page number: 2\n",
      "Content snippet: Our cross-modal hybrid retrieval mechanism strategically combines structural knowledge nav- igation with semantic similarity matching. This architecture addresses the fundamental limita- tion of exist\n",
      "\n",
      "--- Element 20 ---\n",
      "Category: NarrativeText\n",
      "Page number: 2\n",
      "Content snippet: Experimental Validation. To validate the effectiveness of our proposed approach, we conduct com- prehensive experiments on two challenging multimodal benchmarks: DocBench and MMLongBench. Our evaluati\n",
      "\n",
      "--- Element 21 ---\n",
      "Category: Title\n",
      "Page number: 3\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 22 ---\n",
      "Category: NarrativeText\n",
      "Page number: 3\n",
      "Content snippet: that dual-graph construction and cross-modal hybrid retrieval are essential for handling complex multimodal materials. Our ablation studies reveal that graph-based knowledge representation provides th\n",
      "\n",
      "--- Element 23 ---\n",
      "Category: Title\n",
      "Page number: 3\n",
      "Content snippet: 2 THE RAG-ANYTHING FRAMEWORK\n",
      "\n",
      "--- Element 24 ---\n",
      "Category: Title\n",
      "Page number: 3\n",
      "Content snippet: 2.1 PRELIMINARY\n",
      "\n",
      "--- Element 25 ---\n",
      "Category: NarrativeText\n",
      "Page number: 3\n",
      "Content snippet: Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for dynamically expanding the knowledge boundaries of LLMs. While LLMs demonstrate exceptional reasoning capabilities, their \n",
      "\n",
      "--- Element 26 ---\n",
      "Category: NarrativeText\n",
      "Page number: 3\n",
      "Content snippet: The Multimodal Reality: Beyond Text-Only RAG. Current RAG systems face a critical limitation that severely restricts their real-world deployment. Existing frameworks operate under the restrictive assu\n",
      "\n",
      "--- Element 27 ---\n",
      "Category: Title\n",
      "Page number: 3\n",
      "Content snippet: 2.1.1 MOTIVATING RAG-ANYTHING\n",
      "\n",
      "--- Element 28 ---\n",
      "Category: NarrativeText\n",
      "Page number: 3\n",
      "Content snippet: This multimodal reality introduces fundamental technical challenges that expose the inadequacy of current text-only RAG approaches. Effective multimodal RAG requires unified indexing strategies that c\n",
      "\n",
      "--- Element 29 ---\n",
      "Category: NarrativeText\n",
      "Page number: 3\n",
      "Content snippet: The RAG-Anything framework introduces a unified approach for retrieving and processing knowl- edge from heterogeneous multimodal information sources. Our system addresses the fundamental challenge of \n",
      "\n",
      "--- Element 30 ---\n",
      "Category: Title\n",
      "Page number: 3\n",
      "Content snippet: 2.2 UNIVERSAL REPRESENTATION FOR HETEROGENEOUS KNOWLEDGE\n",
      "\n",
      "--- Element 31 ---\n",
      "Category: NarrativeText\n",
      "Page number: 3\n",
      "Content snippet: A key requirement for universal knowledge access is the ability to represent heterogeneous multimodal content in a unified, retrieval-oriented abstraction. Unlike existing pipelines that simply parse \n",
      "\n",
      "--- Element 32 ---\n",
      "Category: NarrativeText\n",
      "Page number: 3\n",
      "Content snippet: Formally, each knowledge source k; € K (e.g., a web page) is decomposed into atomic content units:\n",
      "\n",
      "--- Element 33 ---\n",
      "Category: Title\n",
      "Page number: 3\n",
      "Content snippet: {ej = (tj, ey) FE ()\n",
      "\n",
      "--- Element 34 ---\n",
      "Category: NarrativeText\n",
      "Page number: 3\n",
      "Content snippet: where each unit c; consists of a modality type t; € text, image, table, equation, ... and its corre- sponding raw content x;. The content «x; represents the extracted information from the original kno\n",
      "\n",
      "--- Element 35 ---\n",
      "Category: Title\n",
      "Page number: 3\n",
      "Content snippet: Decompose\n",
      "\n",
      "--- Element 36 ---\n",
      "Category: Title\n",
      "Page number: 3\n",
      "Content snippet: ky\n",
      "\n",
      "--- Element 37 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 38 ---\n",
      "Category: NarrativeText\n",
      "Page number: 4\n",
      "Content snippet: : { Multimodal Knowledge Unification ' i Dual-Graph Construction for Multimodal Knowledge ' bQuery ------ 2 \\ ' ' (each document) ' Could you share insights ti === == Knowledge Graph »=--- =~ \\ '\\ ont\n",
      "\n",
      "--- Element 39 ---\n",
      "Category: NarrativeText\n",
      "Page number: 4\n",
      "Content snippet: G8) Structured Content List ( ‘ . ! Knowledge Graph g MATT 77 Baron TT Dace Hierarchical Text 5 Be Extraction Fy\n",
      "\n",
      "--- Element 40 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: Gras oege\n",
      "\n",
      "--- Element 41 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: mage Caption & Graph\n",
      "\n",
      "--- Element 42 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: ‘Metadata Extraction\n",
      "\n",
      "--- Element 43 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: e Structural Knowledge Negation\n",
      "\n",
      "--- Element 44 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: Multi-modal Processors\n",
      "\n",
      "--- Element 45 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: : vu\n",
      "\n",
      "--- Element 46 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: tet vDB\n",
      "\n",
      "--- Element 47 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 4\n",
      "Content snippet: ogur panaiyay plug,\n",
      "\n",
      "--- Element 48 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 4\n",
      "Content snippet: .\n",
      "\n",
      "--- Element 49 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: OQ\n",
      "\n",
      "--- Element 50 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 4\n",
      "Content snippet: i ' Documents. Text Encoder | = = § 1 Maitimedal voa t ' '\n",
      "\n",
      "--- Element 51 ---\n",
      "Category: NarrativeText\n",
      "Page number: 4\n",
      "Content snippet: Figure 1: Overview of our proposed universal RAG framework RAG-Anything.\n",
      "\n",
      "--- Element 52 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: LaTeX Equation Recognition\n",
      "\n",
      "--- Element 53 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: ofuz vouonb3 —oyuy a6ouz\n",
      "\n",
      "--- Element 54 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: Semantic Similarity Matching\n",
      "\n",
      "--- Element 55 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: V8 over All\n",
      "\n",
      "--- Element 56 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 4\n",
      "Content snippet: o4uz jopow 14m Jonyxay,\n",
      "\n",
      "--- Element 57 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: Table Structure & Content Parsing\n",
      "\n",
      "--- Element 58 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: our aIqmL\n",
      "\n",
      "--- Element 59 ---\n",
      "Category: NarrativeText\n",
      "Page number: 4\n",
      "Content snippet: | Based on the experimental | | data, the results revealed...\n",
      "\n",
      "--- Element 60 ---\n",
      "Category: NarrativeText\n",
      "Page number: 4\n",
      "Content snippet: To ensure high-fidelity extraction, RAG-Anything leverages specialized parsers for different content types. Text is segmented into coherent paragraphs or list items. Figures are extracted with associa\n",
      "\n",
      "--- Element 61 ---\n",
      "Category: Title\n",
      "Page number: 4\n",
      "Content snippet: 2.2.1 DUAL-GRAPH CONSTRUCTION FOR MULTIMODAL KNOWLEDGE\n",
      "\n",
      "--- Element 62 ---\n",
      "Category: NarrativeText\n",
      "Page number: 4\n",
      "Content snippet: While multimodal knowledge unification provides a uniform abstraction across modalities, directly constructing a single unified graph often risks overlooking modality-specific structural signals. The \n",
      "\n",
      "--- Element 63 ---\n",
      "Category: ListItem\n",
      "Page number: 4\n",
      "Content snippet: Cross-Modal Knowledge Graph: Non-textual content like images, tables, and equations contains rich semantic information that traditional text-only approaches often overlook. To preserve this knowledge,\n",
      "\n",
      "--- Element 64 ---\n",
      "Category: NarrativeText\n",
      "Page number: 4\n",
      "Content snippet: an entity summary ee containing key attributes such as entity name, type, and description for graph construction. The generation process is context-aware, processing each unit with its local neighborh\n",
      "\n",
      "--- Element 65 ---\n",
      "Category: NarrativeText\n",
      "Page number: 4\n",
      "Content snippet: Building on these textual representations, RAG-Anything constructs the graph structure using non- text units as anchor points. For each non-text unit c;, the graph extraction routine R(-) processes\n",
      "\n",
      "--- Element 66 ---\n",
      "Category: NarrativeText\n",
      "Page number: 4\n",
      "Content snippet: its description dchnk to identify fine-grained entities and relations: (V;,€)) = Rd’), (2)\n",
      "\n",
      "--- Element 67 ---\n",
      "Category: NarrativeText\n",
      "Page number: 4\n",
      "Content snippet: where V; and €; denote the sets of intra-chunk entities and their relations, respectively. Each atomic non-text unit is associated with a multimodal entity node vj\"™ that serves as an anchor for\n",
      "\n",
      "--- Element 68 ---\n",
      "Category: Title\n",
      "Page number: 5\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 69 ---\n",
      "Category: NarrativeText\n",
      "Page number: 5\n",
      "Content snippet: its intra-chunk entities through explicit be longs_to edges:\n",
      "\n",
      "--- Element 70 ---\n",
      "Category: NarrativeText\n",
      "Page number: 5\n",
      "Content snippet: V= {opm}; U U V;, (3) j E= Ue U Uttu Betongs—f0, up) swe Vj}. (4) j j\n",
      "\n",
      "--- Element 71 ---\n",
      "Category: NarrativeText\n",
      "Page number: 5\n",
      "Content snippet: This construction preserves modality-specific grounding while ensuring non-textual content is con- textualized by its textual neighborhood. This enables reliable cross-modal retrieval and reasoning.\n",
      "\n",
      "--- Element 72 ---\n",
      "Category: ListItem\n",
      "Page number: 5\n",
      "Content snippet: Text-based Knowledge Graph: For text modality chunks, we construct a traditional text-based knowledge graph following established methodologies similar to LightRAG (Guo et al., 2024) and GraphRAG (Edg\n",
      "\n",
      "--- Element 73 ---\n",
      "Category: Title\n",
      "Page number: 5\n",
      "Content snippet: 2.2.2 GRAPH FUSION AND INDEX CREATION\n",
      "\n",
      "--- Element 74 ---\n",
      "Category: NarrativeText\n",
      "Page number: 5\n",
      "Content snippet: The separate cross-modal and text-based knowledge graphs capture complementary aspects of document semantics. Integrating them creates a unified representation leveraging visual-textual associations a\n",
      "\n",
      "--- Element 75 ---\n",
      "Category: NarrativeText\n",
      "Page number: 5\n",
      "Content snippet: e (i) Entity Alignment and Graph Fusion. To create a unified knowledge representation, we merge the multimodal knowledge graph (V, EB ) and text-based knowledge graph through entity align- ment. This \n",
      "\n",
      "--- Element 76 ---\n",
      "Category: NarrativeText\n",
      "Page number: 5\n",
      "Content snippet: e (ii) Dense Representation Generation. To enable efficient similarity-based retrieval, we construct a comprehensive embedding table 7 that encompasses all components generated during the indexing pro\n",
      "\n",
      "--- Element 77 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 5\n",
      "Content snippet: T =emb(s): 5 € VUEUG,, (5)\n",
      "\n",
      "--- Element 78 ---\n",
      "Category: NarrativeText\n",
      "Page number: 5\n",
      "Content snippet: where emb(-) denotes the embedding function tailored for each component type. Together, the unified knowledge graph G and the embedding table 7 constitute the complete retrieval index I = (G,T). This \n",
      "\n",
      "--- Element 79 ---\n",
      "Category: Title\n",
      "Page number: 5\n",
      "Content snippet: 2.3 CROSS-MODAL HYBRID RETRIEVAL\n",
      "\n",
      "--- Element 80 ---\n",
      "Category: NarrativeText\n",
      "Page number: 5\n",
      "Content snippet: The retrieval stage operates on the index I = (G, T) to identify relevant knowledge components for a given user query. Traditional RAG methods face significant limitations when dealing with multimodal\n",
      "\n",
      "--- Element 81 ---\n",
      "Category: NarrativeText\n",
      "Page number: 5\n",
      "Content snippet: Modality-Aware Query Encoding. Given a user query q, we first perform modality-aware query analysis to extract lexical cues and potential modality preferences embedded within the query. For instance, \n",
      "\n",
      "--- Element 82 ---\n",
      "Category: Title\n",
      "Page number: 6\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 83 ---\n",
      "Category: NarrativeText\n",
      "Page number: 6\n",
      "Content snippet: query and knowledge representations. This embedding-based approach enables cross-modal retrieval capabilities where textual queries can effectively access multimodal content through their shared repre\n",
      "\n",
      "--- Element 84 ---\n",
      "Category: NarrativeText\n",
      "Page number: 6\n",
      "Content snippet: Hybrid Knowledge Retrieval Architecture. Recognizing that knowledge relevance manifests through both explicit structural connections and implicit semantic relationships, we design a hybrid retrieval a\n",
      "\n",
      "--- Element 85 ---\n",
      "Category: NarrativeText\n",
      "Page number: 6\n",
      "Content snippet: e (i) Structural Knowledge Navigation. This mechanism addresses the challenge of capturing explicit relationships and multi-hop reasoning patterns. Traditional keyword-based retrieval often fails to i\n",
      "\n",
      "--- Element 86 ---\n",
      "Category: NarrativeText\n",
      "Page number: 6\n",
      "Content snippet: We then perform strategic neighborhood expansion to include related entities and relationships within a specified hop distance. This structural approach proves particularly effective at uncovering hig\n",
      "\n",
      "--- Element 87 ---\n",
      "Category: NarrativeText\n",
      "Page number: 6\n",
      "Content snippet: e (ii) Semantic Similarity Matching. This mechanism addresses the challenge of identifying semantically relevant knowledge that lacks explicit structural connections. While structural navigation excel\n",
      "\n",
      "--- Element 88 ---\n",
      "Category: NarrativeText\n",
      "Page number: 6\n",
      "Content snippet: This approach encompasses atomic content chunks across all modalities, graph entities, and relation- ship representations, enabling fine-grained semantic matching that can surface relevant knowledge e\n",
      "\n",
      "--- Element 89 ---\n",
      "Category: NarrativeText\n",
      "Page number: 6\n",
      "Content snippet: Candidate Pool Unification. Both retrieval pathways may return overlapping candidates with differing relevance signals. This necessitates a principled approach to unify and rank results. Retrieval can\n",
      "\n",
      "--- Element 90 ---\n",
      "Category: NarrativeText\n",
      "Page number: 6\n",
      "Content snippet: e (i) Multi-Signal Fusion Scoring. To address these challenges, we apply a sophisticated fusion scoring mechanism integrating multiple complementary relevance signals. These include structural importa\n",
      "\n",
      "--- Element 91 ---\n",
      "Category: NarrativeText\n",
      "Page number: 6\n",
      "Content snippet: e (ii) Hybrid Retrieval Integration. The resulting hybrid retrieval mechanism enables our framework to leverage the complementary strengths of both knowledge graphs and dense representations. This pro\n",
      "\n",
      "--- Element 92 ---\n",
      "Category: Title\n",
      "Page number: 6\n",
      "Content snippet: 2.4 FROM RETRIEVAL TO SYNTHESIS\n",
      "\n",
      "--- Element 93 ---\n",
      "Category: NarrativeText\n",
      "Page number: 6\n",
      "Content snippet: Effective multimodal question answering requires preserving rich visual semantics while maintaining coherent grounding across heterogeneous knowledge sources. Simple text-only approaches lose crucial \n",
      "\n",
      "--- Element 94 ---\n",
      "Category: NarrativeText\n",
      "Page number: 6\n",
      "Content snippet: e (i) Building Textual Context. Given the top-ranked retrieval candidates C*(q), we construct a structured textual context. We concatenate textual representations of all retrieved components, includ-\n",
      "\n",
      "--- Element 95 ---\n",
      "Category: Title\n",
      "Page number: 7\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 96 ---\n",
      "Category: Title\n",
      "Page number: 7\n",
      "Content snippet: Table 1: Statistics of Experimental Datasets.\n",
      "\n",
      "--- Element 97 ---\n",
      "Category: NarrativeText\n",
      "Page number: 7\n",
      "Content snippet: Dataset #Documents #Avg. Pages #Avg. Tokens #Doc Types # Questions DocBench 229 66 46377 5 1102 MMLongBench 135 47.5 21214 7 1082\n",
      "\n",
      "--- Element 98 ---\n",
      "Category: NarrativeText\n",
      "Page number: 7\n",
      "Content snippet: ing entity summaries, relationship descriptions, and chunk contents. The concatenation incorporates appropriate delimiters to indicate modality types and hierarchical origins. This approach ensures th\n",
      "\n",
      "--- Element 99 ---\n",
      "Category: NarrativeText\n",
      "Page number: 7\n",
      "Content snippet: e (ii) Recovering Visual Content. For multimodal chunks corresponding to visual artifacts, we perform dereferencing to recover original visual content, creating V*(q). This design maintains con- siste\n",
      "\n",
      "--- Element 100 ---\n",
      "Category: NarrativeText\n",
      "Page number: 7\n",
      "Content snippet: The synthesis process jointly conditions on both the assembled comprehensive textual context and dereferenced visual artifacts using a vision-language model:\n",
      "\n",
      "--- Element 101 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 7\n",
      "Content snippet: Response = VLM(q,P(q), ¥*(2)): 6)\n",
      "\n",
      "--- Element 102 ---\n",
      "Category: NarrativeText\n",
      "Page number: 7\n",
      "Content snippet: where the VLM integrates information from query, textual context, and visual content. This unified conditioning enables sophisticated visual interpretation while maintaining grounding in retrieved evi\n",
      "\n",
      "--- Element 103 ---\n",
      "Category: Title\n",
      "Page number: 7\n",
      "Content snippet: 3 EVALUATION\n",
      "\n",
      "--- Element 104 ---\n",
      "Category: Title\n",
      "Page number: 7\n",
      "Content snippet: 3.1 EXPERIMENTAL SETTINGS\n",
      "\n",
      "--- Element 105 ---\n",
      "Category: NarrativeText\n",
      "Page number: 7\n",
      "Content snippet: Evaluation Datasets. We conduct comprehensive evaluations on two challenging multimodal Document Question Answering (DQA) benchmarks that reflect real-world complexity and diversity. DocBench (Zou et \n",
      "\n",
      "--- Element 106 ---\n",
      "Category: NarrativeText\n",
      "Page number: 7\n",
      "Content snippet: MMLongBench (Ma et al., 2024) complements this evaluation by focusing specifically on long- context multimodal document comprehension. It features 135 documents across 7 diverse document types with 1,\n",
      "\n",
      "--- Element 107 ---\n",
      "Category: NarrativeText\n",
      "Page number: 7\n",
      "Content snippet: Baselines. We compare RAG-Anything against the following methods for performance evaluation:\n",
      "\n",
      "--- Element 108 ---\n",
      "Category: NarrativeText\n",
      "Page number: 7\n",
      "Content snippet: ¢ GPT-40-mini: A powerful multimodal language model with native text and image understanding capabilities. Its 128K token context window enables direct processing of entire documents. We evaluate this\n",
      "\n",
      "--- Element 109 ---\n",
      "Category: NarrativeText\n",
      "Page number: 7\n",
      "Content snippet: ¢ LightRAG (Guo et al., 2024): A graph-enhanced RAG system that integrates structured knowledge representation with dual-level retrieval mechanisms. It captures both fine-grained entity-relation infor\n",
      "\n",
      "--- Element 110 ---\n",
      "Category: ListItem\n",
      "Page number: 7\n",
      "Content snippet: MMGraphRAG (Wan & Yu, 2025): A multimodal retrieval framework that constructs unified knowledge graphs spanning textual and visual content. This method employs spectral clustering for multimodal entit\n",
      "\n",
      "--- Element 111 ---\n",
      "Category: NarrativeText\n",
      "Page number: 7\n",
      "Content snippet: Experimental Settings. In our experiments, we implement all baselines using GPT-40-mini as the backbone LLM. Documents are parsed using MinerU (Wang et al., 2024) to extract text, im- ages, tables, an\n",
      "\n",
      "--- Element 112 ---\n",
      "Category: Title\n",
      "Page number: 8\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 113 ---\n",
      "Category: NarrativeText\n",
      "Page number: 8\n",
      "Content snippet: Table 2: Accuracy (%) on DocBench Dataset. Performance results with best scores highlighted in dark blue and second-best in light blue. Domain categories include Academia (Aca.), Finance (Fin.), Gover\n",
      "\n",
      "--- Element 114 ---\n",
      "Category: Title\n",
      "Page number: 8\n",
      "Content snippet: Domains Types Aca. Fin. Gov. Law. News Txt. Mm. Una.\n",
      "\n",
      "--- Element 115 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 8\n",
      "Content snippet: GPT-40-mini 40.3. 46.9 60.3 59.2 61.0 61.0 43.8 49.6 51.2 LightRAG 53.8 56.2 59.5 61.8 65.7 85.0 59.7 46.8 58.4 MMGraphRAG 64.3 52.8 64.9 40.0 61.5 67.6 66.0 60.5 61.0 RAGAnything 61.4 67.0 61.5 60.2 \n",
      "\n",
      "--- Element 116 ---\n",
      "Category: Title\n",
      "Page number: 8\n",
      "Content snippet: Method Overall\n",
      "\n",
      "--- Element 117 ---\n",
      "Category: NarrativeText\n",
      "Page number: 8\n",
      "Content snippet: Table 3: Accuracy (%) on MMLongBench across different domains and overall performance. Best re- sults are highlighted in dark blue and second-best in light blue.. Domain categories include Research Re\n",
      "\n",
      "--- Element 118 ---\n",
      "Category: Title\n",
      "Page number: 8\n",
      "Content snippet: Method Domains Overall\n",
      "\n",
      "--- Element 119 ---\n",
      "Category: Title\n",
      "Page number: 8\n",
      "Content snippet: Res. Tut. Acad. Guid. Broch. Admin. Fin.\n",
      "\n",
      "--- Element 120 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 8\n",
      "Content snippet: GPT-40-mini 35.5 44.0 246 33.1 29.5 46.8 31.1 33.5 LightRAG 40.8 341 36.2 39.4 41.0 44.4 38.3 38.9 MMGraphRAG 40.8 36.5 35.7 35.8 28.2 46.9 38.5 37.7 RAGAnything 46.6 43.5 38.7 43.9 34.0 45.7 43.6 42.\n",
      "\n",
      "--- Element 121 ---\n",
      "Category: NarrativeText\n",
      "Page number: 8\n",
      "Content snippet: Outputs are constrained to a one-sentence format. For the baseline GPT-40-mini in our QA scenario, documents are concatenated into image form with a maximum of 50 pages per document, rendered at 144 d\n",
      "\n",
      "--- Element 122 ---\n",
      "Category: Title\n",
      "Page number: 8\n",
      "Content snippet: 3.2 PERFORMANCE COMPARISON\n",
      "\n",
      "--- Element 123 ---\n",
      "Category: NarrativeText\n",
      "Page number: 8\n",
      "Content snippet: Superior Performance and Cross-Domain Generalization. RAG-Anything demonstrates superior overall performance over baselines through its unified multimodal framework. Unlike LightRAG, which is restrict\n",
      "\n",
      "--- Element 124 ---\n",
      "Category: NarrativeText\n",
      "Page number: 8\n",
      "Content snippet: Enhanced Long-Context Performance. RAG-Anything demonstrates superior performance on long-context documents. The framework excels where relevant evidence is dispersed across multiple modalities and se\n",
      "\n",
      "--- Element 125 ---\n",
      "Category: NarrativeText\n",
      "Page number: 8\n",
      "Content snippet: To systematically evaluate model performance across varying document lengths, we conducted comprehensive experiments on both datasets. As illustrated in Figure 2, RAG-Anything and MM- GraphRAG exhibit\n",
      "\n",
      "--- Element 126 ---\n",
      "Category: Title\n",
      "Page number: 9\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 127 ---\n",
      "Category: NarrativeText\n",
      "Page number: 9\n",
      "Content snippet: DocBench Accuracy DocBench QA Counts MMLongBench Accuracy MMLongBench QA Counts 100. 100; = RAGAnything 350] = RAGAnything 700: <2 MMGraphrac —e MMGraphRAG s 3 s 3 pa] a > 3 id = ig = G 150; | i 3” & \n",
      "\n",
      "--- Element 128 ---\n",
      "Category: NarrativeText\n",
      "Page number: 9\n",
      "Content snippet: Figure 2: Performance evaluation across documents of varying lengths.\n",
      "\n",
      "--- Element 129 ---\n",
      "Category: NarrativeText\n",
      "Page number: 9\n",
      "Content snippet: Table 4: Ablation study results on DocBench. The “Chunk-only” variant bypasses dual-graph construction and relies solely on traditional chunk-based retrieval, while “w/o Reranker” eliminates cross-mod\n",
      "\n",
      "--- Element 130 ---\n",
      "Category: Title\n",
      "Page number: 9\n",
      "Content snippet: Domains Types Aca. Fin. Gov. Law. News Txt. Mm. Una.\n",
      "\n",
      "--- Element 131 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 9\n",
      "Content snippet: Chunk-only 55.8 61.5 60.1 60.7 640 816 66.2 43.5 60.0 w/o Reranker 60.9 63.5 58.8 60.2 68.6 81.7 74.7 45.4 62.4 RAGAnything 61.4 67.0 61.5 60.2 663 85.0 76.3 46.0 63.4\n",
      "\n",
      "--- Element 132 ---\n",
      "Category: Title\n",
      "Page number: 9\n",
      "Content snippet: Method Overall\n",
      "\n",
      "--- Element 133 ---\n",
      "Category: NarrativeText\n",
      "Page number: 9\n",
      "Content snippet: 54.6% for 101-200 pages; 68.8% vs. 55.0% for 200+ pages). On MMLongBench, RAG-Anything demonstrates consistent improvements across all length categories, achieving accuracy gains of 3.4 points for 11-\n",
      "\n",
      "--- Element 134 ---\n",
      "Category: Title\n",
      "Page number: 9\n",
      "Content snippet: 3.3. ARCHITECTURAL VALIDATION WITH ABLATION STUDIES\n",
      "\n",
      "--- Element 135 ---\n",
      "Category: NarrativeText\n",
      "Page number: 9\n",
      "Content snippet: To isolate and quantify the contributions of key architectural components in RAG-Anything, we conducted systematic ablation studies examining two critical design choices. Given that our approach funda\n",
      "\n",
      "--- Element 136 ---\n",
      "Category: NarrativeText\n",
      "Page number: 9\n",
      "Content snippet: As demonstrated in Table 4, the results validate our architectural design through striking performance variations. e Graph Construction is Essential. The chunk-only variant achieves merely 60.0% accur\n",
      "\n",
      "--- Element 137 ---\n",
      "Category: Title\n",
      "Page number: 9\n",
      "Content snippet: 3.4 CASE STUDIES\n",
      "\n",
      "--- Element 138 ---\n",
      "Category: NarrativeText\n",
      "Page number: 9\n",
      "Content snippet: Multimodal documents contain rich structural information within each modality. Understanding these intra-modal structures is crucial for accurate reasoning. We analyze two representative cases from Do\n",
      "\n",
      "--- Element 139 ---\n",
      "Category: NarrativeText\n",
      "Page number: 9\n",
      "Content snippet: e Case 1: Multi-panel Figure Interpretation. This case examines a common scenario in academic literature. Researchers often need to compare results across different experimental conditions. These resu\n",
      "\n",
      "--- Element 140 ---\n",
      "Category: Title\n",
      "Page number: 10\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 141 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: ‘Question: Which model’ style space shows a separation\n",
      "\n",
      "--- Element 142 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: between different styles according to GPT-4o-mini®: LighRAG®: According to Figure 2, the VAE According to Figure 2, the Variational Autoencoder model's style space shows a clearer (VAE) shows a cleare\n",
      "\n",
      "--- Element 143 ---\n",
      "Category: Title\n",
      "Page number: 10\n",
      "Content snippet: Deterministic Autoencoder (DAE).\n",
      "\n",
      "--- Element 144 ---\n",
      "Category: Title\n",
      "Page number: 10\n",
      "Content snippet: MMGraphRAG@:\n",
      "\n",
      "--- Element 145 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: According to Figure 2, the model's style space RAG-Anything(Correct®):\n",
      "\n",
      "--- Element 146 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: shows a clearer separation between different styles The DAE model's style space shows a clearer\n",
      "\n",
      "--- Element 147 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: in the Variational Autoencoder (VAE) compared to separation between different styles according to\n",
      "\n",
      "--- Element 148 ---\n",
      "Category: Title\n",
      "Page number: 10\n",
      "Content snippet: the Deterministic Autoencoder (DAE).\n",
      "\n",
      "--- Element 149 ---\n",
      "Category: Title\n",
      "Page number: 10\n",
      "Content snippet: Multimode! Document inthe document Figure 2\n",
      "\n",
      "--- Element 150 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: Figure 3: Multi-panel figure interpretation case. The query requires identifying cluster separation patterns from the style-space panel, while avoiding confusion from the adjacent content-space panel.\n",
      "\n",
      "--- Element 151 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: visualization with multiple subpanels. The query requires distinguishing between two related but distinct panels. RAG-Anything constructs a visual-layout graph where panels, axis titles, legends, and \n",
      "\n",
      "--- Element 152 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: juestic Vhat was Novo Nordisk's total amount sj id salaries in 2020? A a\n",
      "\n",
      "--- Element 153 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: Q unt d salar LightRAG®: Novo Nordisk spent DKK 11,503 million ts spent on wages and salaries in on wages and salaries in 2020. a 2020 was DKK 32,928 million. : ern MMGraphRAG®: RAG-Anything(Correct®)\n",
      "\n",
      "--- Element 154 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: Figure 4: Financial table navigation case. The query involves locating the specific intersection of “Wages and salaries” row and “2020” column amid similar terminological entries.\n",
      "\n",
      "--- Element 155 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: e Case 2: Financial Table Navigation. This case addresses a common challenge in financial document analysis. Analysts must extract specific metrics from tables with similar terminology and multiple ti\n",
      "\n",
      "--- Element 156 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: RAG-Anything transforms the financial report table into a structured graph. Each row header, column header (year), data cell, and unit becomes a node. The edges capture key relationships: row-of, colu\n",
      "\n",
      "--- Element 157 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: e Key Insights. Both cases demonstrate how RAG-Anything’s structure-aware design delivers targeted advantages. Our approach transforms documents into explicit graph representations. These graphs captu\n",
      "\n",
      "--- Element 158 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: This structure-aware retrieval design reduces confusion from repeated terminology and complex layouts. Traditional RAG systems struggle with these scenarios due to lack of structural understanding. Ev\n",
      "\n",
      "--- Element 159 ---\n",
      "Category: Title\n",
      "Page number: 10\n",
      "Content snippet: 4 RELATED WORK\n",
      "\n",
      "--- Element 160 ---\n",
      "Category: NarrativeText\n",
      "Page number: 10\n",
      "Content snippet: e Graph-Enhanced Retrieval-Augmented Generation. Large language models struggle with long-context inputs and multi-hop queries, failing to precisely locate dispersed evidence (Zhang et al.,\n",
      "\n",
      "--- Element 161 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 10\n",
      "Content snippet: 10\n",
      "\n",
      "--- Element 162 ---\n",
      "Category: Title\n",
      "Page number: 11\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 163 ---\n",
      "Category: NarrativeText\n",
      "Page number: 11\n",
      "Content snippet: 2025). Graph structures address this limitation by introducing explicit relational modeling, improving both retrieval efficiency and reasoning accuracy (Bei et al., 2025).\n",
      "\n",
      "--- Element 164 ---\n",
      "Category: NarrativeText\n",
      "Page number: 11\n",
      "Content snippet: Since GraphRAG (Edge et al., 2024), research has evolved along two complementary directions. First, graph construction approaches optimize structures for retrieval efficiency, ranging from Ligh- tRAG’\n",
      "\n",
      "--- Element 165 ---\n",
      "Category: NarrativeText\n",
      "Page number: 11\n",
      "Content snippet: e Multimodal Retrieval-Augmented Generation. Multimodal RAG represents a natural evolution from text-based RAG systems, addressing the need to integrate external knowledge from diverse data modalities\n",
      "\n",
      "--- Element 166 ---\n",
      "Category: NarrativeText\n",
      "Page number: 11\n",
      "Content snippet: The fundamental problem underlying these limitations is architectural fragmentation. Current systems require specialized processing pipelines for each modality. This creates poor generalizability as n\n",
      "\n",
      "--- Element 167 ---\n",
      "Category: Title\n",
      "Page number: 11\n",
      "Content snippet: 5 CONCLUSION\n",
      "\n",
      "--- Element 168 ---\n",
      "Category: NarrativeText\n",
      "Page number: 11\n",
      "Content snippet: RAG-Anything introduces a paradigm shift in multimodal retrieval through its unified graph-based framework. Our core technical innovation is the dual-graph construction strategy that seamlessly integr\n",
      "\n",
      "--- Element 169 ---\n",
      "Category: NarrativeText\n",
      "Page number: 11\n",
      "Content snippet: Our analysis in Appendix A.5 reveals critical challenges facing current multimodal RAG systems. Two fundamental issues emerge through systematic failure case examination. First, systems exhibit text-c\n",
      "\n",
      "--- Element 170 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 11\n",
      "Content snippet: 11\n",
      "\n",
      "--- Element 171 ---\n",
      "Category: Title\n",
      "Page number: 12\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 172 ---\n",
      "Category: Title\n",
      "Page number: 12\n",
      "Content snippet: REFERENCES\n",
      "\n",
      "--- Element 173 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammad- khani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin Asgari. Ask in any modality: A \n",
      "\n",
      "--- Element 174 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Yuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi Chen, Sheng Zhou, Hao Chen, Yong Li, Jiajun Bu, Shirui Pan, Yizhou Yu, et al. Graphs meet ai agents: Taxonomy, progress, and future opportunities. arXiv \n",
      "\n",
      "--- Element 175 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: A graph rag approach \n",
      "\n",
      "--- Element 176 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval- augmented generation. arXiv preprint arXiv:2410.05779, 2024.\n",
      "\n",
      "--- Element 177 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neuro- biologically inspired long-term memory for large language models. NeurIPS, 37:59532-59569, 2024.\n",
      "\n",
      "--- Element 178 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, and Lijuan Wang. Mm-vid: Advancing video understanding wi\n",
      "\n",
      "--- Element 179 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding with visualiza\n",
      "\n",
      "--- Element 180 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Costas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model reasoning. arXiv preprint arXiv:2405.20139, 2024.\n",
      "\n",
      "--- Element 181 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Xubin Ren, Lingrui Xu, Long Xia, Shuaigiang Wang, Dawei Yin, and Chao Huang. Vide- orag: Retrieval-augmented generation with extreme long-context videos. arXiv preprint arXiv:2502.01549, 2025.\n",
      "\n",
      "--- Element 182 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning. Raptor: Recursive abstractive processing for tree-organized retrieval. In The Twelfth International Con\n",
      "\n",
      "--- Element 183 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Xueyao Wan and Hang Yu. Mmgraphrag: Bridging vision and language with interpretable multimodal knowledge graphs. arXiv preprint arXiv:2507.20804, 2025.\n",
      "\n",
      "--- Element 184 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document content extraction. arXiv pr\n",
      "\n",
      "--- Element 185 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Shu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, and Yuchi Ma. Archrag: Attributed community- based hierarchical retrieval-augmented generation. arXiv preprint arXiv:2502.09891, 2025.\n",
      "\n",
      "--- Element 186 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, and Maosong Sun. Visrag: Vision-based retrieval-augmented generation on multi-modality d\n",
      "\n",
      "--- Element 187 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen, Yilin Xiao, Chuang Zhou, Yi Chang, and Xiao Huang. A survey of graph retrieval-augmented generation for cus\n",
      "\n",
      "--- Element 188 ---\n",
      "Category: NarrativeText\n",
      "Page number: 12\n",
      "Content snippet: Anni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, and Dong Yu. Docbench: A benchmark for evaluating lIm-based document reading systems. arXiv preprint arXiv:2407.107\n",
      "\n",
      "--- Element 189 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 12\n",
      "Content snippet: 12\n",
      "\n",
      "--- Element 190 ---\n",
      "Category: Title\n",
      "Page number: 13\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 191 ---\n",
      "Category: Title\n",
      "Page number: 13\n",
      "Content snippet: A APPENDIX\n",
      "\n",
      "--- Element 192 ---\n",
      "Category: NarrativeText\n",
      "Page number: 13\n",
      "Content snippet: This appendix provides comprehensive supporting materials for our experimental evaluation and implementation details. Section A.1 presents detailed dataset statistics for the DocBench and MMLongBench \n",
      "\n",
      "--- Element 193 ---\n",
      "Category: Title\n",
      "Page number: 13\n",
      "Content snippet: A.1 DATASET CHARACTERISTICS AND STATISTICS\n",
      "\n",
      "--- Element 194 ---\n",
      "Category: Title\n",
      "Page number: 13\n",
      "Content snippet: Table 5: Document type distribution and statistics for the DocBench benchmark.\n",
      "\n",
      "--- Element 195 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 13\n",
      "Content snippet: Type Acad. Fin. Gov. Law. News # Docs 49 40 44 46 50 # Questions 303 288 = 148 191 172 Avg. Pages 11 192 69 58 1\n",
      "\n",
      "--- Element 196 ---\n",
      "Category: Title\n",
      "Page number: 13\n",
      "Content snippet: Table 6: Document type distribution and statistics for the MMLongBench benchmark.\n",
      "\n",
      "--- Element 197 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 13\n",
      "Content snippet: Type Res. Tut. Acad. Guid. Broch. Admin. Fin. # Docs 34 17 26 22 15 10 11 # Questions 292 138 199 155 100 81 117 Avg. Pages 39 58 35 78 30 17 87\n",
      "\n",
      "--- Element 198 ---\n",
      "Category: NarrativeText\n",
      "Page number: 13\n",
      "Content snippet: Tables 5 and 6 present the distribution of document types across the DocBench and MMLong- Bench benchmarks. e DocBench encompasses medium- to long-length documents spanning various domains, including \n",
      "\n",
      "--- Element 199 ---\n",
      "Category: NarrativeText\n",
      "Page number: 13\n",
      "Content snippet: Collectively, these two benchmarks provide comprehensive coverage ranging from brief news arti- cles to extensive technical and financial documentation. This establishes diverse and challenging evalua\n",
      "\n",
      "--- Element 200 ---\n",
      "Category: Title\n",
      "Page number: 13\n",
      "Content snippet: A.2 ADDITIONAL CASE STUDIES\n",
      "\n",
      "--- Element 201 ---\n",
      "Category: NarrativeText\n",
      "Page number: 13\n",
      "Content snippet: Cueto: Which GCAN submodel component moval LightRAG®: resulted in the lowest agcuracy for Twitter15 Removing the source tweet from the GCAN model attention mechanism from the resulted in the Iowest ac\n",
      "\n",
      "--- Element 202 ---\n",
      "Category: NarrativeText\n",
      "Page number: 13\n",
      "Content snippet: SA WA wR OG WC WAL GCAN sub-model resulted in the lowest accuracy for Twitter!5. MMGraphRAG@: = os ‘model resulted in the lowest accuracy for\n",
      "\n",
      "--- Element 203 ---\n",
      "Category: NarrativeText\n",
      "Page number: 13\n",
      "Content snippet: ‘Twitterl5, as indicated by signi\n",
      "\n",
      "--- Element 204 ---\n",
      "Category: NarrativeText\n",
      "Page number: 13\n",
      "Content snippet: Figure 4: GCA drops in the ablation analysis in Figure 4.\n",
      "\n",
      "--- Element 205 ---\n",
      "Category: Title\n",
      "Page number: 13\n",
      "Content snippet: ablation analysis in Accuracy\n",
      "\n",
      "--- Element 206 ---\n",
      "Category: Title\n",
      "Page number: 13\n",
      "Content snippet: Multimodel Document Evidence figure in the document\n",
      "\n",
      "--- Element 207 ---\n",
      "Category: NarrativeText\n",
      "Page number: 13\n",
      "Content snippet: indicated by a significant drop in performance when using the sub-model that excluded both source tweet embeddings and dual co-attention,\n",
      "\n",
      "--- Element 208 ---\n",
      "Category: Title\n",
      "Page number: 13\n",
      "Content snippet: RAG-Anything(Correct’®):\n",
      "\n",
      "--- Element 209 ---\n",
      "Category: NarrativeText\n",
      "Page number: 13\n",
      "Content snippet: The removal of the source tweet embeddings and dua configuration \"-S-A,\" resulted in the lowest accuracy for Twitter15.\n",
      "\n",
      "--- Element 210 ---\n",
      "Category: NarrativeText\n",
      "Page number: 13\n",
      "Content snippet: ‘o-attention, indicated as model\n",
      "\n",
      "--- Element 211 ---\n",
      "Category: NarrativeText\n",
      "Page number: 13\n",
      "Content snippet: Figure 5: Visual reasoning case. RAG-Anything correctly identifies \"-S-A\" as the lowest accuracy configuration, while baselines misinterpret spatial relationships.\n",
      "\n",
      "--- Element 212 ---\n",
      "Category: NarrativeText\n",
      "Page number: 13\n",
      "Content snippet: e Demonstrating Visual Reasoning Capabilities. Figure 5 illustrates how RAG-Anything handles complex visual reasoning tasks involving chart interpretation. The query asks which GCAN sub- model compone\n",
      "\n",
      "--- Element 213 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 13\n",
      "Content snippet: 13\n",
      "\n",
      "--- Element 214 ---\n",
      "Category: Title\n",
      "Page number: 14\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 215 ---\n",
      "Category: NarrativeText\n",
      "Page number: 14\n",
      "Content snippet: with spatial relationships between visual elements. RAG-Anything addresses this challenge by constructing a structured graph representation of the bar plot. Bars, axis labels, and legends become inter\n",
      "\n",
      "--- Element 216 ---\n",
      "Category: NarrativeText\n",
      "Page number: 14\n",
      "Content snippet: This graph-based approach enables precise alignment between visual and textual elements. The system correctly identifies the bar labeled \"-S-A\" (removing source tweet embeddings and dual co-attention)\n",
      "\n",
      "--- Element 217 ---\n",
      "Category: Title\n",
      "Page number: 14\n",
      "Content snippet: ‘Question: Which model combination for the Evidence Inference\n",
      "\n",
      "--- Element 218 ---\n",
      "Category: NarrativeText\n",
      "Page number: 14\n",
      "Content snippet: dataset has the highest AUPRC value? GPT -4o-mini: LightRAG®:\n",
      "\n",
      "--- Element 219 ---\n",
      "Category: NarrativeText\n",
      "Page number: 14\n",
      "Content snippet: The model combination for the Evidence Inference The model combination \"BERT + LSTM - dataset with the highest AUPRC value is the BERT- Attention\" has the highest AUPRC value for the to-BERT model, ac\n",
      "\n",
      "--- Element 220 ---\n",
      "Category: NarrativeText\n",
      "Page number: 14\n",
      "Content snippet: = Bert\" model, which achieved an AUPRC score of value is GloVe + LSTM - Attention, achieving\n",
      "\n",
      "--- Element 221 ---\n",
      "Category: Title\n",
      "Page number: 14\n",
      "Content snippet: Multimode! Document. Evidence table in the document 0.708. a value of 0.506.\n",
      "\n",
      "--- Element 222 ---\n",
      "Category: NarrativeText\n",
      "Page number: 14\n",
      "Content snippet: Figure 6: Tabular navigation case. RAG-Anything locates the highest AUPRC value (0.506), while the compared approaches struggle with structural ambiguity.\n",
      "\n",
      "--- Element 223 ---\n",
      "Category: NarrativeText\n",
      "Page number: 14\n",
      "Content snippet: e Handling Complex Tabular Structures. Figure 6 showcases RAG-Anything’s ability to navigate intricate tabular data where structural disambiguation is crucial. The query seeks the model combi- nation \n",
      "\n",
      "--- Element 224 ---\n",
      "Category: NarrativeText\n",
      "Page number: 14\n",
      "Content snippet: RAG-Anything overcomes this by parsing the table into a comprehensive relational graph where headers and data cells become nodes connected through explicit row-of and column-of relationships. This str\n",
      "\n",
      "--- Element 225 ---\n",
      "Category: Title\n",
      "Page number: 14\n",
      "Content snippet: A.3> CONTEXT-AWARE MULTIMODAL PROMPTING\n",
      "\n",
      "--- Element 226 ---\n",
      "Category: NarrativeText\n",
      "Page number: 14\n",
      "Content snippet: These three prompts orchestrate structured, context-aware multimodal analysis with JSON-formatted outputs. They systematically guide the model to extract comprehensive descriptions of visual, tabular,\n",
      "\n",
      "--- Element 227 ---\n",
      "Category: NarrativeText\n",
      "Page number: 14\n",
      "Content snippet: Vision Analysis Prompt. Figure 7 orchestrates comprehensive image-context integration. The prompt directs the model to systematically capture compositional elements, object relationships, visual attri\n",
      "\n",
      "--- Element 228 ---\n",
      "Category: NarrativeText\n",
      "Page number: 14\n",
      "Content snippet: Table Analysis Prompt. Figure 8 structures systematic tabular content decomposition across multiple analytical dimensions: structural organization, column semantics, critical values, statistical patte\n",
      "\n",
      "--- Element 229 ---\n",
      "Category: NarrativeText\n",
      "Page number: 14\n",
      "Content snippet: Equation Analysis Prompt. Figure 9 prioritizes semantic interpretation over syntactic restatement of mathematical expressions. The prompt instructs comprehensive analysis of variable definitions, oper\n",
      "\n",
      "--- Element 230 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 14\n",
      "Content snippet: 14\n",
      "\n",
      "--- Element 231 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 232 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: Vision Analysis Prompt\n",
      "\n",
      "--- Element 233 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: eee vision_analysis_prompt.png\n",
      "\n",
      "--- Element 234 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: 1 Please analyze this image in detail, considering the surrounding context. Provide a JSON response with the following structure\n",
      "\n",
      "--- Element 235 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: \"detailed_description\": “A comprehensive and detailed visual description of the image following these guidelines:\n",
      "\n",
      "--- Element 236 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: = Describe the overall composition and layout\n",
      "\n",
      "--- Element 237 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: Identify all objects, people, text, and visual elements\n",
      "\n",
      "--- Element 238 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: Explain relationships between elements and how they relate to the surrounding context\n",
      "\n",
      "--- Element 239 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: Note colors, lighting, and visual style\n",
      "\n",
      "--- Element 240 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: Describe any actions or activities shown\n",
      "\n",
      "--- Element 241 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: Include technical details if relevant (charts, diagrams, etc.)\n",
      "\n",
      "--- Element 242 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: Reference connections to the surrounding content when relevant\n",
      "\n",
      "--- Element 243 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: 14 - Always use specific names instead of pronouns\",\n",
      "\n",
      "--- Element 244 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: “entity_info\": {\n",
      "\n",
      "--- Element 245 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 15\n",
      "Content snippet: “entity_name\": \"{entity_name}\",\n",
      "\n",
      "--- Element 246 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: “entity_type\": \"image\"\n",
      "\n",
      "--- Element 247 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: \"summary\": “concise summary of the image content, its significance, and relationship to surrounding content (max 190 words)\"\n",
      "\n",
      "--- Element 248 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 15\n",
      "Content snippet: }\n",
      "\n",
      "--- Element 249 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 15\n",
      "Content snippet: }\n",
      "\n",
      "--- Element 250 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 15\n",
      "Content snippet: 12\n",
      "\n",
      "--- Element 251 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: Context from surrounding content: {context}\n",
      "\n",
      "--- Element 252 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: Image details: - Image Path: {image_path} + Captions: {captions}\n",
      "\n",
      "--- Element 253 ---\n",
      "Category: ListItem\n",
      "Page number: 15\n",
      "Content snippet: Footnotes: {footnotes}\n",
      "\n",
      "--- Element 254 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: Focus on providing accurate, detailed visual analysis that incorporates the context and would be useful for knowledge retrieval.\n",
      "\n",
      "--- Element 255 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: Figure 7: Vision analysis prompt for context-aware image interpretation and knowledge extraction.\n",
      "\n",
      "--- Element 256 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: Table Analysis Prompt\n",
      "\n",
      "--- Element 257 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: eee table_analysis_pronpt.png 1 Please analyze this table content considering the surrounding context, and provide a JSON response with the\n",
      "\n",
      "--- Element 258 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: following structure\n",
      "\n",
      "--- Element 259 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: “detailed_description\": “A comprehensive analysis of the table including: Table structure and organization\n",
      "\n",
      "--- Element 260 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: Column headers and their meanings\n",
      "\n",
      "--- Element 261 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: Key data points and patterns\n",
      "\n",
      "--- Element 262 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: Statistical insights and trends\n",
      "\n",
      "--- Element 263 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: Relationships between data elements\n",
      "\n",
      "--- Element 264 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: Significance of the data presented in relation to surrounding context\n",
      "\n",
      "--- Element 265 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: 12 - How the table supports or illustrates concepts from the surrounding content\n",
      "\n",
      "--- Element 266 ---\n",
      "Category: ListItem\n",
      "Page number: 15\n",
      "Content snippet: 13. Always use specific names and values instead of general references.\",\n",
      "\n",
      "--- Element 267 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: 14 “entity_info\": {\n",
      "\n",
      "--- Element 268 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 15\n",
      "Content snippet: “entity_name\": \"{entity_name}\",\n",
      "\n",
      "--- Element 269 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 15\n",
      "Content snippet: “entity_type\": \"table\",\n",
      "\n",
      "--- Element 270 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: \"summary\": “concise summary of the table's purpose, key findings, and relationship to surrounding content (max 100 words)\"\n",
      "\n",
      "--- Element 271 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 15\n",
      "Content snippet: +\n",
      "\n",
      "--- Element 272 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 15\n",
      "Content snippet: }\n",
      "\n",
      "--- Element 273 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: Context from surrounding content: {context}\n",
      "\n",
      "--- Element 274 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: Table Information:\n",
      "\n",
      "--- Element 275 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: Image Path: {table_img_path} Caption; {table_caption} Body: {table_body} Footnotes: {table_footnote}\n",
      "\n",
      "--- Element 276 ---\n",
      "Category: NarrativeText\n",
      "Page number: 15\n",
      "Content snippet: Focus on extracting meaningful insights and relationships from the tabular data in the context of the surrounding content.\n",
      "\n",
      "--- Element 277 ---\n",
      "Category: Title\n",
      "Page number: 15\n",
      "Content snippet: Figure 8: Table analysis prompt for structured content decomposition and semantic understanding.\n",
      "\n",
      "--- Element 278 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 15\n",
      "Content snippet: 15\n",
      "\n",
      "--- Element 279 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 280 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: Equation Analysis Prompt\n",
      "\n",
      "--- Element 281 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: equation_analysis_prompt.png Please analyze this mathematical equation considering the surrounding context, and provide a JSON response\n",
      "\n",
      "--- Element 282 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: with the following structure:\n",
      "\n",
      "--- Element 283 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: “detailed_description\": “A comprehensive analysis of the equation including: Mathematical meaning and interpretation\n",
      "\n",
      "--- Element 284 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: Variables and their definitions in the context of surrounding content\n",
      "\n",
      "--- Element 285 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: Mathematical operations and functions used\n",
      "\n",
      "--- Element 286 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: Application domain and context based on surrounding material\n",
      "\n",
      "--- Element 287 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: Physical or theoretical significance\n",
      "\n",
      "--- Element 288 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: Relationship to other mathematical concepts mentioned in the context\n",
      "\n",
      "--- Element 289 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: Practical applications or use cases\n",
      "\n",
      "--- Element 290 ---\n",
      "Category: ListItem\n",
      "Page number: 16\n",
      "Content snippet: How the equation relates to the broader discussion or framework\n",
      "\n",
      "--- Element 291 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: Always use specific mathematical terminology\",\n",
      "\n",
      "--- Element 292 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: “entity_info\": {\n",
      "\n",
      "--- Element 293 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: “entity_name\": \"{entity_name}\"\n",
      "\n",
      "--- Element 294 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: “entity_type\": \"equation\"\n",
      "\n",
      "--- Element 295 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: \"summary\": “concise summary of the equation's purpose, significance, and role in the surrounding context (max 100 words)\"\n",
      "\n",
      "--- Element 296 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 16\n",
      "Content snippet: }\n",
      "\n",
      "--- Element 297 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 16\n",
      "Content snippet: }\n",
      "\n",
      "--- Element 298 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: Context from surrounding content: {context}\n",
      "\n",
      "--- Element 299 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: Equation Information Equation: {equation_text} Format: {equation_format}\n",
      "\n",
      "--- Element 300 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: Focus on providing mathematical insights and explaining the equation's significance within the broader context.\n",
      "\n",
      "--- Element 301 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: Figure 9: Equation analysis prompt for mathematical expression interpretation and integration.\n",
      "\n",
      "--- Element 302 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: Accuracy Evaluation Prompt\n",
      "\n",
      "--- Element 303 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 16\n",
      "Content snippet: 1\n",
      "\n",
      "--- Element 304 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: accuracy_evaluation_prompt.png\n",
      "\n",
      "--- Element 305 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: You are an expert evaluator tasked with assessing the accuracy of answers generated by a RAG (Retrieval-Augmented Generation) system.\n",
      "\n",
      "--- Element 306 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: **Task**: Evaluate whether the generated answer correctly responds to the given question based on the expected answer.\n",
      "\n",
      "--- Element 307 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: **Question**: {question} **Expected Answer**: {expected_answer}\n",
      "\n",
      "--- Element 308 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: **Generated Answer**: {generated_answer}\n",
      "\n",
      "--- Element 309 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: **Evaluation Criteriat*\n",
      "\n",
      "--- Element 310 ---\n",
      "Category: ListItem\n",
      "Page number: 16\n",
      "Content snippet: 1. **Accuracy (0 or 1)**: Does the generated answer match the factual content of the expected answer? - 1: The generated answer is factually correct and aligns with the expected answer\n",
      "\n",
      "--- Element 311 ---\n",
      "Category: ListItem\n",
      "Page number: 16\n",
      "Content snippet: 0: The generated answer is factually incorrect or contradicts the expected answer\n",
      "\n",
      "--- Element 312 ---\n",
      "Category: ListItem\n",
      "Page number: 16\n",
      "Content snippet: tInstructions**: - Focus on factual correctness, not writing style or format\n",
      "\n",
      "--- Element 313 ---\n",
      "Category: ListItem\n",
      "Page number: 16\n",
      "Content snippet: Consider partial matches: if the generated answer contains the correct information but includes additional context, it should still be considered accurate\n",
      "\n",
      "--- Element 314 ---\n",
      "Category: ListItem\n",
      "Page number: 16\n",
      "Content snippet: For numerical answers, check if the values match or are equivalent\n",
      "\n",
      "--- Element 315 ---\n",
      "Category: ListItem\n",
      "Page number: 16\n",
      "Content snippet: For list answers, check if all key elements are present\n",
      "\n",
      "--- Element 316 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: If the expected answer is \"Not answerable\" and the generated answer indicates inability to answer, consider it accurate\n",
      "\n",
      "--- Element 317 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: **Qutput Format**;\n",
      "\n",
      "--- Element 318 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: Please respond with a JSON object containing only {\n",
      "\n",
      "--- Element 319 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 16\n",
      "Content snippet: \"accuracy\": © or 1,\n",
      "\n",
      "--- Element 320 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: \"reasoning\": \"Brief explanation of your evaluation\"\n",
      "\n",
      "--- Element 321 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 16\n",
      "Content snippet: +\n",
      "\n",
      "--- Element 322 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: Figure 10: Accuracy evaluation prompt for consistent factual assessment across question types.\n",
      "\n",
      "--- Element 323 ---\n",
      "Category: Title\n",
      "Page number: 16\n",
      "Content snippet: A.4. ACCURACY EVALUATION PROMPT DESIGN\n",
      "\n",
      "--- Element 324 ---\n",
      "Category: NarrativeText\n",
      "Page number: 16\n",
      "Content snippet: Figure 10 presents the standardized prompt specifically designed for systematic factual accuracy as- sessment of generated responses across multiple domains. The prompt establishes explicit evaluation\n",
      "\n",
      "--- Element 325 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 16\n",
      "Content snippet: 16\n",
      "\n",
      "--- Element 326 ---\n",
      "Category: Title\n",
      "Page number: 17\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 327 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: classifications accompanied by concise analytical justifications. All accuracy evaluations throughout our comprehensive experimental framework were conducted using GPT-40-mini, ensuring consistent and\n",
      "\n",
      "--- Element 328 ---\n",
      "Category: Title\n",
      "Page number: 17\n",
      "Content snippet: A.5 CHALLENGES AND FUTURE DIRECTIONS FOR MULTI-MODAL RAG\n",
      "\n",
      "--- Element 329 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: While current multimodal RAG systems demonstrate promising capabilities, their limitations emerge most clearly through systematic analysis of failure cases. Understanding where and why these systems b\n",
      "\n",
      "--- Element 330 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: e Text-Centric Retrieval Bias: Systems exhibit strong preference for textual sources, even when queries explicitly demand visual information. This reveals inadequate cross-modal attention.\n",
      "\n",
      "--- Element 331 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: e Document Structure Processing Challenges: Systems struggle with complex layouts and non- linear information flows. This exposes limitations in spatial reasoning and contextual understanding.\n",
      "\n",
      "--- Element 332 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: These failure modes illuminate key insights about current multimodal AI. They provide concrete directions for architectural innovations that could substantially improve system robustness.\n",
      "\n",
      "--- Element 333 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: ‘Question: According to Figure 3, what is the sequence followed to generate the\n",
      "\n",
      "--- Element 334 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: final labels for Named Entity Revogt GPT-4o-mini®: LightRAG®: Expected ansmer The sequence is Characters > Char Embedding > Char involves inputting words and their representations a sequence where wor\n",
      "\n",
      "--- Element 335 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: —__ der in the figure is from bottom to top) culminating in the final prediction of entity labels. produce the final labels. 3 oa = MMGraphRAG @: RAG-Anything ®:\n",
      "\n",
      "--- Element 336 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: involves using a combined representation of word _...involves processing input characters through\n",
      "\n",
      "--- Element 337 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: = = embeddings and character-level convolution, which is pj-directional LSTM layers (Char LSTM-B a * | then processed through a Bi-LSTM layer, followed by and Char LSTM-F) followed by a softmax or 7 a\n",
      "\n",
      "--- Element 338 ---\n",
      "Category: Title\n",
      "Page number: 17\n",
      "Content snippet: Multimodel Document [Evidence figure in the document\n",
      "\n",
      "--- Element 339 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: Figure 11: Cross-modal noise case. All methods fail to retrieve the correct answer from the specified image, instead retrieving noisy textual evidence that misaligns with the structured visual content\n",
      "\n",
      "--- Element 340 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: ‘Question: Which models style space shows a clearer separation\n",
      "\n",
      "--- Element 341 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: GPT-4o-mini®: LighRAG®: between different styles according to Figure 2? i ht RAGS\n",
      "\n",
      "--- Element 342 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: ‘The Joint goal accuracy in the \"Train\" domain The Joint goa improved by 6.26% when using the GEM fine-tuning improved from 44,\n",
      "\n",
      "--- Element 343 ---\n",
      "Category: Title\n",
      "Page number: 17\n",
      "Content snippet: e “Train” domain\n",
      "\n",
      "--- Element 344 ---\n",
      "Category: Title\n",
      "Page number: 17\n",
      "Content snippet: from scratch to\n",
      "\n",
      "--- Element 345 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: — nw “oiat Sa [Hors] | strategy compared to training from scratch, 50.51% with GEM fine-tuning, indicating an Entutn on Domains Faeroe! |p Paan_| | 65.35% versus 59.09% improvement of 6.27% Regn Tuan \n",
      "\n",
      "--- Element 346 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: em | 1993792 resulting in an improvement of 15.597 strategy, resulting in an improvement of\n",
      "\n",
      "--- Element 347 ---\n",
      "Category: Title\n",
      "Page number: 17\n",
      "Content snippet: Multimodel Document Evidence table in the document 13.86%.\n",
      "\n",
      "--- Element 348 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: Figure 12: Ambiguous table structure case. All methods fail to correctly parse the confusing table layout with merged cells and unclear column boundaries, leading to incorrect data extraction.\n",
      "\n",
      "--- Element 349 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: Case 1: Cross-Modal Misalignment. Figure || presents a particularly revealing failure scenario where all evaluated methods consistently produce incorrect answers despite having access to the necessary\n",
      "\n",
      "--- Element 350 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: Issue 1: Retrieval Bias Toward Text. Current RAG systems demonstrate pronounced bias toward textual passages. This occurs particularly when visual content lacks exact keyword matches. The bias persist\n",
      "\n",
      "--- Element 351 ---\n",
      "Category: NarrativeText\n",
      "Page number: 17\n",
      "Content snippet: The retrieved textual information, while topically related, often operates at a different granularity level than visual content. Images may contain precise, structured data such as specific numerical \n",
      "\n",
      "--- Element 352 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 17\n",
      "Content snippet: 17\n",
      "\n",
      "--- Element 353 ---\n",
      "Category: Title\n",
      "Page number: 18\n",
      "Content snippet: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "\n",
      "--- Element 354 ---\n",
      "Category: NarrativeText\n",
      "Page number: 18\n",
      "Content snippet: detailed diagrams, or exact spatial relationships. Corresponding text typically provides general, conceptual descriptions. This semantic misalignment introduces noise that actively misleads the reason\n",
      "\n",
      "--- Element 355 ---\n",
      "Category: NarrativeText\n",
      "Page number: 18\n",
      "Content snippet: Issue 2: Rigid Spatial Processing Patterns. Current visual processing models exhibit fundamental rigidity in spatial interpretation. Most systems default to sequential scanning patterns—top-to- bottom\n",
      "\n",
      "--- Element 356 ---\n",
      "Category: NarrativeText\n",
      "Page number: 18\n",
      "Content snippet: In the observed failure case, the correct answer required integrating visual elements in reverse order from the model’s default processing sequence. The system’s inability to recognize and adapt to th\n",
      "\n",
      "--- Element 357 ---\n",
      "Category: NarrativeText\n",
      "Page number: 18\n",
      "Content snippet: Case 2: Structural Noise in Ambiguous Table Layouts. As shown in Figure 12, all methods failed when confronted with a structurally ambiguous table. The primary failure stems from the table’s confusing\n",
      "\n",
      "--- Element 358 ---\n",
      "Category: NarrativeText\n",
      "Page number: 18\n",
      "Content snippet: The case highlights two essential directions for enhancing robustness. RAG systems require layout- aware parsing mechanisms that can recognize and adapt to structural irregularities rather than imposi\n",
      "\n",
      "--- Element 359 ---\n",
      "Category: UncategorizedText\n",
      "Page number: 18\n",
      "Content snippet: 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 逐一打印文字和類別\n",
    "for i, doc in enumerate(pdf_docs, start=1):\n",
    "    print(f\"--- Element {i} ---\")\n",
    "    print(\"Category:\", doc.metadata.get(\"category\"))\n",
    "    print(\"Page number:\", doc.metadata.get(\"page_number\"))\n",
    "    print(\"Content snippet:\", doc.page_content[:200])  # 顯示前200字\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e54b79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "區塊: 2510.12323v1 [cs.AI] 14 Oct 2025\n",
      "區塊: Zirui Guo, Xubin Ren, Lingrui Xu, Jiahao Zhang, Chao Huang* The University of Hong Kong zrguol01@hku.hk xubinrencs@gmail.com chaohuang75@gmail.com\n",
      "區塊: ogur panaiyay plug,\n",
      "區塊: .\n",
      "區塊: i ' Documents. Text Encoder | = = § 1 Maitimedal voa t ' '\n",
      "區塊: o4uz jopow 14m Jonyxay,\n",
      "區塊: T =emb(s): 5 € VUEUG,, (5)\n",
      "區塊: Response = VLM(q,P(q), ¥*(2)): 6)\n",
      "區塊: GPT-40-mini 40.3. 46.9 60.3 59.2 61.0 61.0 43.8 49.6 51.2 LightRAG 53.8 56.2 59.5 61.8 65.7 85.0 59.7 46.8 58.4 MMGraphRAG 64.3 52.8 64.9 40.0 61.5 67.6 66.0 60.5 61.0 RAGAnything 61.4 67.0 61.5 60.2 663 85.0 76.3 46.0 63.4\n",
      "區塊: GPT-40-mini 35.5 44.0 246 33.1 29.5 46.8 31.1 33.5 LightRAG 40.8 341 36.2 39.4 41.0 44.4 38.3 38.9 MMGraphRAG 40.8 36.5 35.7 35.8 28.2 46.9 38.5 37.7 RAGAnything 46.6 43.5 38.7 43.9 34.0 45.7 43.6 42.8\n",
      "區塊: Chunk-only 55.8 61.5 60.1 60.7 640 816 66.2 43.5 60.0 w/o Reranker 60.9 63.5 58.8 60.2 68.6 81.7 74.7 45.4 62.4 RAGAnything 61.4 67.0 61.5 60.2 663 85.0 76.3 46.0 63.4\n",
      "區塊: 10\n",
      "區塊: 11\n",
      "區塊: 12\n",
      "區塊: Type Acad. Fin. Gov. Law. News # Docs 49 40 44 46 50 # Questions 303 288 = 148 191 172 Avg. Pages 11 192 69 58 1\n",
      "區塊: Type Res. Tut. Acad. Guid. Broch. Admin. Fin. # Docs 34 17 26 22 15 10 11 # Questions 292 138 199 155 100 81 117 Avg. Pages 39 58 35 78 30 17 87\n",
      "區塊: 13\n",
      "區塊: 14\n",
      "區塊: “entity_name\": \"{entity_name}\",\n",
      "區塊: }\n",
      "區塊: }\n",
      "區塊: 12\n",
      "區塊: “entity_name\": \"{entity_name}\",\n",
      "區塊: “entity_type\": \"table\",\n",
      "區塊: +\n",
      "區塊: }\n",
      "區塊: 15\n",
      "區塊: }\n",
      "區塊: }\n",
      "區塊: 1\n",
      "區塊: \"accuracy\": © or 1,\n",
      "區塊: +\n",
      "區塊: 16\n",
      "區塊: 17\n",
      "區塊: 18\n"
     ]
    }
   ],
   "source": [
    "# 打印特定分類區塊\n",
    "for i, doc in enumerate(pdf_docs, start=1):\n",
    "    if doc.metadata.get(\"category\") == \"UncategorizedText\":\n",
    "        print(\"區塊:\", doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee574051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UncategorizedText: 35 elements\n",
      "Title: 118 elements\n",
      "NarrativeText: 193 elements\n",
      "ListItem: 13 elements\n"
     ]
    }
   ],
   "source": [
    "# 各元素分組\n",
    "from collections import defaultdict\n",
    "\n",
    "elements_by_category = defaultdict(list)\n",
    "\n",
    "for doc in pdf_docs:\n",
    "    cat = doc.metadata.get(\"category\", \"Uncategorized\")\n",
    "    elements_by_category[cat].append(doc.page_content)\n",
    "\n",
    "# 印出每個類別的元素數量\n",
    "for cat, contents in elements_by_category.items():\n",
    "    print(f\"{cat}: {len(contents)} elements\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1af79b",
   "metadata": {},
   "source": [
    "# Metadata 結構說明\n",
    "```python\n",
    "{\n",
    "    'source': 'D:\\\\Learning-lab\\\\Test\\\\sample.pdf',  # PDF 原始檔案路徑\n",
    "    'coordinates': {                                   # 元素在頁面上的位置\n",
    "        'points': ((51.0, 631.0), (51.0, 1429.0), (99.0, 1429.0), (99.0, 631.0)),\n",
    "        'system': 'PixelSpace',\n",
    "        'layout_width': 1700,\n",
    "        'layout_height': 2200\n",
    "    },\n",
    "    'filetype': 'application/pdf',                     # 檔案類型\n",
    "    'languages': ['eng'],                              # 偵測語言\n",
    "    'last_modified': '2025-10-21T15:20:27',           # 檔案最後修改時間\n",
    "    'page_number': 1,                                  # 所屬頁碼\n",
    "    'file_directory': 'D:\\\\Learning-lab\\\\Test',       # PDF 所在資料夾\n",
    "    'filename': 'sample.pdf',                          # 檔案名稱\n",
    "    'category': 'UncategorizedText',                  # 元素類別（文字、程式碼、公式、圖片等）\n",
    "    'element_id': 'dc03f3ec8b6688c2ff97432265348f10' # 唯一元素 ID\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d648b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "source": [
    "# 抓圖片、逐頁載入、OCR\n",
    "# 載入模式有三：'single', 'paged', 'elements'。單檔、逐頁、元素\n",
    "# \"ocr_languages\": [\"eng\", \"chi_tra\"],  # OCR 英文 + 繁體中文\n",
    "\"\"\"\n",
    "舊方法：\n",
    "\n",
    "pdf_loader = UnstructuredPDFLoader(\n",
    "    pdf_path,\n",
    "    mode=\"paged\", \n",
    "    unstructured_kwargs={\n",
    "        \"ocr_languages\": [\"eng\"],  \n",
    "        \"extract_images_in_pdf\": True\n",
    "    }\n",
    ")\n",
    "\"\"\"\n",
    "# 新方式建議用指定'elements'，並設定 chunking_strategy:\"by_page\"，該方法可以提供更靈活得拆分方式\n",
    "pdf_loader = UnstructuredPDFLoader(\n",
    "    pdf_path,\n",
    "    mode=\"elements\", \n",
    "    unstructured_kwargs={\n",
    "        \"ocr_languages\": [\"eng\"],  # 需要 Tesseract\n",
    "        \"extract_images_in_pdf\": True,\n",
    "        \"chunking_strategy\":\"by_page\"\n",
    "    }\n",
    ")\n",
    "\n",
    "pdf_docs = pdf_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f158856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UncategorizedText: 35 elements\n",
      "Title: 118 elements\n",
      "NarrativeText: 193 elements\n",
      "ListItem: 13 elements\n"
     ]
    }
   ],
   "source": [
    "# 再次查看各元素分組\n",
    "from collections import defaultdict\n",
    "\n",
    "elements_by_category = defaultdict(list)\n",
    "\n",
    "for doc in pdf_docs:\n",
    "    cat = doc.metadata.get(\"category\", \"Uncategorized\")\n",
    "    elements_by_category[cat].append(doc.page_content)\n",
    "\n",
    "# 印出每個類別的元素數量\n",
    "for cat, contents in elements_by_category.items():\n",
    "    print(f\"{cat}: {len(contents)} elements\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10716d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_docs 是 mode=\"elements\" 或 chunking_strategy=\"by_page\" 後的結果\n",
    "from collections import defaultdict\n",
    "\n",
    "# 記錄每頁圖片數量\n",
    "images_per_page = defaultdict(int)\n",
    "\n",
    "for doc in pdf_docs:\n",
    "    page = doc.metadata.get(\"page_number\", 1)\n",
    "    # 檢查元素類型或 category 是否是圖片\n",
    "    if doc.metadata.get(\"element_type\") in [\"Image\", \"ImageBlock\"] or doc.metadata.get(\"category\") == \"Image\":\n",
    "        images_per_page[page] += 1\n",
    "\n",
    "# 印出結果\n",
    "for page, count in images_per_page.items():\n",
    "    print(f\"Page {page}: {count} 張圖片\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9279335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽取圖片並存檔\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "output_dir = \"extracted_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, doc in enumerate(pdf_docs, start=1):\n",
    "    if doc.metadata.get(\"element_type\") == \"Image\":\n",
    "        image = doc.metadata.get(\"image\")  # PIL Image 物件\n",
    "        if image:\n",
    "            image_path = os.path.join(output_dir, f\"page{i}_img{doc.metadata['element_id']}.png\")\n",
    "            image.save(image_path)\n",
    "            print(f\"存檔完成: {image_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02661623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: arXiv\n",
      "Page 1: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 1: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 1: ABSTRACT\n",
      "Page 1: 1 INTRODUCTION\n",
      "Page 2: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 3: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 3: 2 THE RAG-ANYTHING FRAMEWORK\n",
      "Page 3: 2.1 PRELIMINARY\n",
      "Page 3: 2.1.1 MOTIVATING RAG-ANYTHING\n",
      "Page 3: 2.2 UNIVERSAL REPRESENTATION FOR HETEROGENEOUS KNOWLEDGE\n",
      "Page 3: {ej = (tj, ey) FE ()\n",
      "Page 3: Decompose\n",
      "Page 3: ky\n",
      "Page 4: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 4: Gras oege\n",
      "Page 4: mage Caption & Graph\n",
      "Page 4: ‘Metadata Extraction\n",
      "Page 4: e Structural Knowledge Negation\n",
      "Page 4: Multi-modal Processors\n",
      "Page 4: : vu\n",
      "Page 4: tet vDB\n",
      "Page 4: OQ\n",
      "Page 4: LaTeX Equation Recognition\n",
      "Page 4: ofuz vouonb3 —oyuy a6ouz\n",
      "Page 4: Semantic Similarity Matching\n",
      "Page 4: V8 over All\n",
      "Page 4: Table Structure & Content Parsing\n",
      "Page 4: our aIqmL\n",
      "Page 4: 2.2.1 DUAL-GRAPH CONSTRUCTION FOR MULTIMODAL KNOWLEDGE\n",
      "Page 5: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 5: 2.2.2 GRAPH FUSION AND INDEX CREATION\n",
      "Page 5: 2.3 CROSS-MODAL HYBRID RETRIEVAL\n",
      "Page 6: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 6: 2.4 FROM RETRIEVAL TO SYNTHESIS\n",
      "Page 7: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 7: Table 1: Statistics of Experimental Datasets.\n",
      "Page 7: 3 EVALUATION\n",
      "Page 7: 3.1 EXPERIMENTAL SETTINGS\n",
      "Page 8: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 8: Domains Types Aca. Fin. Gov. Law. News Txt. Mm. Una.\n",
      "Page 8: Method Overall\n",
      "Page 8: Method Domains Overall\n",
      "Page 8: Res. Tut. Acad. Guid. Broch. Admin. Fin.\n",
      "Page 8: 3.2 PERFORMANCE COMPARISON\n",
      "Page 9: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 9: Domains Types Aca. Fin. Gov. Law. News Txt. Mm. Una.\n",
      "Page 9: Method Overall\n",
      "Page 9: 3.3. ARCHITECTURAL VALIDATION WITH ABLATION STUDIES\n",
      "Page 9: 3.4 CASE STUDIES\n",
      "Page 10: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 10: Deterministic Autoencoder (DAE).\n",
      "Page 10: MMGraphRAG@:\n",
      "Page 10: the Deterministic Autoencoder (DAE).\n",
      "Page 10: Multimode! Document inthe document Figure 2\n",
      "Page 10: 4 RELATED WORK\n",
      "Page 11: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 11: 5 CONCLUSION\n",
      "Page 12: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 12: REFERENCES\n",
      "Page 13: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 13: A APPENDIX\n",
      "Page 13: A.1 DATASET CHARACTERISTICS AND STATISTICS\n",
      "Page 13: Table 5: Document type distribution and statistics for the DocBench benchmark.\n",
      "Page 13: Table 6: Document type distribution and statistics for the MMLongBench benchmark.\n",
      "Page 13: A.2 ADDITIONAL CASE STUDIES\n",
      "Page 13: ablation analysis in Accuracy\n",
      "Page 13: Multimodel Document Evidence figure in the document\n",
      "Page 13: RAG-Anything(Correct’®):\n",
      "Page 14: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 14: ‘Question: Which model combination for the Evidence Inference\n",
      "Page 14: Multimode! Document. Evidence table in the document 0.708. a value of 0.506.\n",
      "Page 14: A.3> CONTEXT-AWARE MULTIMODAL PROMPTING\n",
      "Page 15: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 15: Vision Analysis Prompt\n",
      "Page 15: eee vision_analysis_prompt.png\n",
      "Page 15: = Describe the overall composition and layout\n",
      "Page 15: Identify all objects, people, text, and visual elements\n",
      "Page 15: Include technical details if relevant (charts, diagrams, etc.)\n",
      "Page 15: “entity_info\": {\n",
      "Page 15: “entity_type\": \"image\"\n",
      "Page 15: Image details: - Image Path: {image_path} + Captions: {captions}\n",
      "Page 15: Figure 7: Vision analysis prompt for context-aware image interpretation and knowledge extraction.\n",
      "Page 15: Table Analysis Prompt\n",
      "Page 15: Column headers and their meanings\n",
      "Page 15: Key data points and patterns\n",
      "Page 15: Statistical insights and trends\n",
      "Page 15: Relationships between data elements\n",
      "Page 15: 14 “entity_info\": {\n",
      "Page 15: Table Information:\n",
      "Page 15: Image Path: {table_img_path} Caption; {table_caption} Body: {table_body} Footnotes: {table_footnote}\n",
      "Page 15: Figure 8: Table analysis prompt for structured content decomposition and semantic understanding.\n",
      "Page 16: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 16: Equation Analysis Prompt\n",
      "Page 16: with the following structure:\n",
      "Page 16: Physical or theoretical significance\n",
      "Page 16: Practical applications or use cases\n",
      "Page 16: “entity_info\": {\n",
      "Page 16: “entity_name\": \"{entity_name}\"\n",
      "Page 16: “entity_type\": \"equation\"\n",
      "Page 16: Equation Information Equation: {equation_text} Format: {equation_format}\n",
      "Page 16: Figure 9: Equation analysis prompt for mathematical expression interpretation and integration.\n",
      "Page 16: Accuracy Evaluation Prompt\n",
      "Page 16: accuracy_evaluation_prompt.png\n",
      "Page 16: **Question**: {question} **Expected Answer**: {expected_answer}\n",
      "Page 16: **Generated Answer**: {generated_answer}\n",
      "Page 16: **Evaluation Criteriat*\n",
      "Page 16: **Qutput Format**;\n",
      "Page 16: \"reasoning\": \"Brief explanation of your evaluation\"\n",
      "Page 16: Figure 10: Accuracy evaluation prompt for consistent factual assessment across question types.\n",
      "Page 16: A.4. ACCURACY EVALUATION PROMPT DESIGN\n",
      "Page 17: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Page 17: A.5 CHALLENGES AND FUTURE DIRECTIONS FOR MULTI-MODAL RAG\n",
      "Page 17: Multimodel Document [Evidence figure in the document\n",
      "Page 17: e “Train” domain\n",
      "Page 17: from scratch to\n",
      "Page 17: Multimodel Document Evidence table in the document 13.86%.\n",
      "Page 18: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n"
     ]
    }
   ],
   "source": [
    "# 抓標題\n",
    "outline = []\n",
    "\n",
    "for doc in pdf_docs:\n",
    "    cat = doc.metadata.get(\"category\")\n",
    "    if cat == \"Title\":\n",
    "        outline.append({\n",
    "            \"page\": doc.metadata.get(\"page_number\"),\n",
    "            \"title\": doc.page_content.strip()\n",
    "        })\n",
    "\n",
    "# 印出導覽大綱\n",
    "for item in outline:\n",
    "    print(f\"Page {item['page']}: {item['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e33a1fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Page 1 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[UncategorizedText] Zirui Guo, Xubin Ren, Lingrui Xu, Jiahao Zhang, Chao Huang* The University of Hong Kong zrguol01@hku.hk xubinrencs@gmail.com chaohuang75@gmail.com\n",
      "[Title] ABSTRACT\n",
      "[NarrativeText] Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding Large Language Models beyond their static training limitations. However, a critical misalignment exists between current RAG capabilities and real-world information environments. Modern knowledge repositories are inher- ently multimodal, containing rich combinations of textual content, visual elements, structured tables, and mathematical expressions. Yet existing RAG frameworks are limited to textual content, creating fundamental gaps when processing multimodal documents. We present RAG-Anything, a unified framework that enables compre- hensive knowledge retrieval across all modalities. Our approach reconceptualizes multimodal content as interconnected knowledge entities rather than isolated data types. The framework introduces dual-graph construction to capture both cross- modal relationships and textual semantics within a unified representation. We develop cross-modal hybrid retrieval that combines structural knowledge naviga- tion with semantic matching. This enables effective reasoning over heterogeneous content where relevant evidence spans multiple modalities. RAG-Anything demon- strates superior performance on challenging multimodal benchmarks, achieving significant improvements over state-of-the-art methods. Performance gains become particularly pronounced on long documents where traditional approaches fail. Our framework establishes a new paradigm for multimodal knowledge access, eliminat- ing the architectural fragmentation that constrains current systems. Our framework is open-sourced at: https: //github.com/HKUDS/RAG-Anything.\n",
      "[UncategorizedText] 2510.12323v1 [cs.AI] 14 Oct 2025\n",
      "[Title] 1 INTRODUCTION\n",
      "[NarrativeText] Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding the knowledge boundaries of Large Language Models (LLM) beyond their static training limita- tions Zhang et al. (2025). By enabling dynamic retrieval and incorporation of external knowledge during inference, RAG systems transform static language models into adaptive, knowledge-aware systems. This capability has proven essential for applications requiring up-to-date information, domain-specific knowledge, or factual grounding that extends beyond pre-training corpora.\n",
      "[Title] arXiv\n",
      "[NarrativeText] However, existing RAG frameworks focus exclusively on text-only knowledge while neglecting the rich multimodal information present in real-world documents. This limitation fundamentally mis- aligns with how information exists in authentic environments. Real-world knowledge repositories are inherently heterogeneous and multimodal Abootorabi et al. (2025). They contain rich combinations of textual content, visual elements, structured tables, and mathematical expressions across diverse document formats. This textual assumption forces existing RAG systems to either discard non-textual information entirely or flatten complex multimodal content into inadequate textual approximations.\n",
      "[NarrativeText] The consequences of this limitation become particularly severe in document-intensive domains where multimodal content carries essential meaning. Academic research, financial analysis, and technical documentation represent prime examples of knowledge-rich environments. These domains fundamentally depend on visual and structured information. Critical insights are often encoded exclusively in non-textual formats. Such formats resist meaningful conversion to plain text.\n",
      "[NarrativeText] The consequences of this limitation become particularly severe in knowledge-intensive domains where multimodal content carries essential meaning. Three representative scenarios illustrate the critical\n",
      "[ListItem] Corresponding Author: Chao Huang\n",
      "--- Page 2 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[NarrativeText] need for multimodal RAG capabilities. In Scientific Research, experimental results are primarily communicated through plots, diagrams, and statistical visualizations. These contain core discoveries that remain invisible to text-only systems. Financial Analysis relies heavily on market charts, correlation matrices, and performance tables. Investment insights are encoded in visual patterns rather than textual descriptions. Additionally, Medical Literature Analysis depends on radiological images, diagnostic charts, and clinical data tables. These contain life-critical information essential for accurate diagnosis and treatment decisions. Current RAG frameworks systematically exclude these vital knowledge sources across all three scenarios. This creates fundamental gaps that render them inadequate for real-world applications requiring comprehensive information understanding. Therefore, multimodal RAG emerges as a critical advancement. It is necessary to bridge these knowledge gaps and enable truly comprehensive intelligence across all modalities of human knowledge representation.\n",
      "[NarrativeText] Addressing multimodal RAG presents three fundamental technical challenges that demand principled solutions. This makes it significantly more complex than traditional text-only approaches. The naive solution of converting all multimodal content to textual descriptions introduces severe information loss. Visual elements such as charts, diagrams, and spatial layouts contain semantic richness that cannot be adequately captured through text alone. These inherent limitations necessitate the design of effective technical components. Such components must be specifically designed to handle multimodal complexity and preserve the full spectrum of information contained within diverse content types.\n",
      "[NarrativeText] Technical Challenges. e First, the unified multimodal representation challenge requires seam- lessly integrating diverse information types. The system must preserve their unique characteristics and cross-modal relationships. This demands advanced multimodal encoders that can capture both intra-modal and inter-modal dependencies without losing essential visual semantics. e Second, the structure-aware decomposition challenge demands intelligent parsing of complex layouts. The system must maintain spatial and hierarchical relationships crucial for understanding. This requires specialized layout-aware parsing modules that can interpret document structure and preserve contex- tual positioning of multimodal elements. e Third, the cross-modal retrieval challenge necessitates sophisticated mechanisms that can navigate between different modalities. These mechanisms must reason over their interconnections during retrieval. This calls for cross-modal alignment systems capable of understanding semantic correspondences across text, images, and structured data. These challenges are amplified in long-context scenarios. Relevant evidence is dispersed across multiple modalities and sections, requiring coordinated reasoning across heterogeneous information sources.\n",
      "[NarrativeText] Our Contributions. To address these challenges, we introduce RAG-Anything, a unified framework that fundamentally reimagines multimodal knowledge representation and retrieval. Our approach employs a dual-graph construction strategy that elegantly bridges the gap between cross-modal understanding and fine-grained textual semantics. Rather than forcing diverse modalities into text- centric pipelines, RAG-Anything constructs complementary knowledge graphs that preserve both multimodal contextual relationships and detailed textual knowledge. This design enables seamless integration of visual elements, structured data, and mathematical expressions within a unified retrieval framework. The system maintains semantic integrity across modalities while ensuring efficient cross-modal reasoning capabilities throughout the process.\n",
      "[NarrativeText] Our cross-modal hybrid retrieval mechanism strategically combines structural knowledge nav- igation with semantic similarity matching. This architecture addresses the fundamental limita- tion of existing approaches that rely solely on embedding-based retrieval or keyword matching. RAG-Anything leverages explicit graph relationships to capture multi-hop reasoning patterns. It simultaneously employs dense vector representations to identify semantically relevant content that lacks direct structural connections. The framework introduces modality-aware query processing and cross-modal alignment systems. These enable textual queries to effectively access visual and structured information. This unified approach eliminates the architectural fragmentation that plagues current multimodal RAG systems. It delivers superior performance particularly on long-context documents where relevant evidence spans multiple modalities and document sections.\n",
      "[NarrativeText] Experimental Validation. To validate the effectiveness of our proposed approach, we conduct com- prehensive experiments on two challenging multimodal benchmarks: DocBench and MMLongBench. Our evaluation demonstrates that RAG-Anything achieves superior performance across diverse do- mains. The framework represents substantial improvements over state-of-the-art baselines. Notably, our performance gains become increasingly significant as content length increases. We observe particularly pronounced advantages on long-context materials. This validates our core hypothesis\n",
      "--- Page 3 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[NarrativeText] that dual-graph construction and cross-modal hybrid retrieval are essential for handling complex multimodal materials. Our ablation studies reveal that graph-based knowledge representation provides the primary performance gains. Traditional chunk-based approaches fail to capture the structural relationships critical for multimodal reasoning. Case studies further demonstrate that our framework excels at precise localization within complex layouts. The system effectively disambiguates similar terminology and navigates multi-panel visualizations through structure-aware retrieval mechanisms.\n",
      "[Title] 2 THE RAG-ANYTHING FRAMEWORK\n",
      "[Title] 2.1 PRELIMINARY\n",
      "[NarrativeText] Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for dynamically expanding the knowledge boundaries of LLMs. While LLMs demonstrate exceptional reasoning capabilities, their knowledge remains static and bounded by training data cutoffs. This creates an ever-widening gap with the rapidly evolving information landscape. RAG systems address this critical limitation by enabling LLMs to retrieve and incorporate external knowledge sources during inference. This transforms them from static repositories into adaptive, knowledge-aware systems.\n",
      "[NarrativeText] The Multimodal Reality: Beyond Text-Only RAG. Current RAG systems face a critical limitation that severely restricts their real-world deployment. Existing frameworks operate under the restrictive assumption that knowledge corpus consists exclusively of plain textual documents. This assump- tion fundamentally misaligns with how information exists in authentic environments. Real-world knowledge repositories are inherently heterogeneous and multimodal, containing rich combinations of textual content, visual elements, structured data, and mathematical expressions. These diverse knowledge sources span multiple document formats and presentation mediums, from research papers and technical slides to web pages and interactive documents.\n",
      "[Title] 2.1.1 MOTIVATING RAG-ANYTHING\n",
      "[NarrativeText] This multimodal reality introduces fundamental technical challenges that expose the inadequacy of current text-only RAG approaches. Effective multimodal RAG requires unified indexing strategies that can handle disparate data types, cross-modal retrieval mechanisms that preserve semantic relationships across modalities, and sophisticated synthesis techniques that can coherently integrate diverse information sources. These challenges demand a fundamentally different architectural approach rather than incremental improvements to existing systems.\n",
      "[NarrativeText] The RAG-Anything framework introduces a unified approach for retrieving and processing knowl- edge from heterogeneous multimodal information sources. Our system addresses the fundamental challenge of handling diverse data modalities and document formats within a retrieval pipeline. The framework comprises three core components: universal indexing for multimodal knowledge, cross-modal adaptive retrieval, and knowledge-enhanced response generation. This integrated design enables effective knowledge utilization across modalities while maintaining computational efficiency.\n",
      "[Title] 2.2 UNIVERSAL REPRESENTATION FOR HETEROGENEOUS KNOWLEDGE\n",
      "[NarrativeText] A key requirement for universal knowledge access is the ability to represent heterogeneous multimodal content in a unified, retrieval-oriented abstraction. Unlike existing pipelines that simply parse documents into text segments, RAG-Anything introduces Multimodal Knowledge Unification. This process decomposes raw inputs into atomic knowledge units while preserving their structural context and semantic alignment. For instance, RAG-Anything ensures that figures remain grounded in their captions, equations remain linked to surrounding definitions, and tables stay connected to explanatory narratives. This transforms heterogeneous files into a coherent substrate for cross-modal retrieval.\n",
      "[NarrativeText] Formally, each knowledge source k; € K (e.g., a web page) is decomposed into atomic content units:\n",
      "[Title] Decompose\n",
      "[Title] {ej = (tj, ey) FE ()\n",
      "[Title] ky\n",
      "[NarrativeText] where each unit c; consists of a modality type t; € text, image, table, equation, ... and its corre- sponding raw content x;. The content «x; represents the extracted information from the original knowledge source, processed in a modality-aware manner to preserve semantic integrity.\n",
      "--- Page 4 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[NarrativeText] : { Multimodal Knowledge Unification ' i Dual-Graph Construction for Multimodal Knowledge ' bQuery ------ 2 \\ ' ' (each document) ' Could you share insights ti === == Knowledge Graph »=--- =~ \\ '\\ onthe experimental |\n",
      "[NarrativeText] G8) Structured Content List ( ‘ . ! Knowledge Graph g MATT 77 Baron TT Dace Hierarchical Text 5 Be Extraction Fy\n",
      "[Title] ofuz vouonb3 —oyuy a6ouz\n",
      "[UncategorizedText] .\n",
      "[Title] e Structural Knowledge Negation\n",
      "[Title] mage Caption & Graph\n",
      "[Title] Gras oege\n",
      "[UncategorizedText] ogur panaiyay plug,\n",
      "[Title] ‘Metadata Extraction\n",
      "[Title] Multi-modal Processors\n",
      "[UncategorizedText] o4uz jopow 14m Jonyxay,\n",
      "[Title] : vu\n",
      "[Title] LaTeX Equation Recognition\n",
      "[Title] OQ\n",
      "[Title] tet vDB\n",
      "[Title] Semantic Similarity Matching\n",
      "[UncategorizedText] i ' Documents. Text Encoder | = = § 1 Maitimedal voa t ' '\n",
      "[Title] V8 over All\n",
      "[Title] our aIqmL\n",
      "[Title] Table Structure & Content Parsing\n",
      "[NarrativeText] | Based on the experimental | | data, the results revealed...\n",
      "[NarrativeText] Figure 1: Overview of our proposed universal RAG framework RAG-Anything.\n",
      "[NarrativeText] To ensure high-fidelity extraction, RAG-Anything leverages specialized parsers for different content types. Text is segmented into coherent paragraphs or list items. Figures are extracted with associated metadata such as captions and cross-references. Tables are parsed into structured cells with headers and values. Mathematical expressions are converted into symbolic representations. The resulting x; preserves both content and structural context within the source. This provides a faithful, modality- consistent representation. The decomposition abstracts diverse file formats into atomic units while maintaining their hierarchical order and contextual relationships. This canonicalization enables uniform processing, indexing, and retrieval of multimodal content within our framework.\n",
      "[Title] 2.2.1 DUAL-GRAPH CONSTRUCTION FOR MULTIMODAL KNOWLEDGE\n",
      "[NarrativeText] While multimodal knowledge unification provides a uniform abstraction across modalities, directly constructing a single unified graph often risks overlooking modality-specific structural signals. The proposed RAG-Anything addresses this challenge through a dual-graph construction strategy. The system first builds a cross-modal knowledge graph that faithfully grounds non-textual modalities within their contextual environment. It then constructs a text-based knowledge graph using es- tablished text-centric extraction pipelines. These complementary graphs are merged through entity alignment. This design ensures accurate cross-modal grounding and comprehensive coverage of textual semantics, enabling richer knowledge representation and robust retrieval.\n",
      "[ListItem] Cross-Modal Knowledge Graph: Non-textual content like images, tables, and equations contains rich semantic information that traditional text-only approaches often overlook. To preserve this knowledge, RAG-Anything constructs a multimodal knowledge graph where non-text atomic units are transformed into structured graph entities. RAG-Anything leverages multimodal large language models to derive two complementary textual representations from each atomic content unit. The first is a detailed description dshunk optimized for cross-modal retrieval. The second is\n",
      "[NarrativeText] an entity summary ee containing key attributes such as entity name, type, and description for graph construction. The generation process is context-aware, processing each unit with its local neighborhood C; = {cx | |k — j| < 6}, where 6 controls the contextual window size. This ensures representations accurately reflect each unit’s role within the broader document structure.\n",
      "[NarrativeText] Building on these textual representations, RAG-Anything constructs the graph structure using non- text units as anchor points. For each non-text unit c;, the graph extraction routine R(-) processes\n",
      "[NarrativeText] its description dchnk to identify fine-grained entities and relations: (V;,€)) = Rd’), (2)\n",
      "[NarrativeText] where V; and €; denote the sets of intra-chunk entities and their relations, respectively. Each atomic non-text unit is associated with a multimodal entity node vj\"™ that serves as an anchor for\n",
      "--- Page 5 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[NarrativeText] its intra-chunk entities through explicit be longs_to edges:\n",
      "[NarrativeText] V= {opm}; U U V;, (3) j E= Ue U Uttu Betongs—f0, up) swe Vj}. (4) j j\n",
      "[NarrativeText] This construction preserves modality-specific grounding while ensuring non-textual content is con- textualized by its textual neighborhood. This enables reliable cross-modal retrieval and reasoning.\n",
      "[ListItem] Text-based Knowledge Graph: For text modality chunks, we construct a traditional text-based knowledge graph following established methodologies similar to LightRAG (Guo et al., 2024) and GraphRAG (Edge et al., 2024). The extraction process operates directly on textual content x; where t; = text, leveraging named entity recognition and relation extraction techniques to identify entities and their semantic relationships. Given the rich semantic information inherent in textual content, multimodal context integration is not required for this component. The resulting text-based knowledge graph captures explicit knowledge and semantic connections present in textual portions of documents, complementing the multimodal graph’s cross-modal grounding capabilities.\n",
      "[Title] 2.2.2 GRAPH FUSION AND INDEX CREATION\n",
      "[NarrativeText] The separate cross-modal and text-based knowledge graphs capture complementary aspects of document semantics. Integrating them creates a unified representation leveraging visual-textual associations and fine-grained textual relationships for enhanced retrieval.\n",
      "[NarrativeText] e (i) Entity Alignment and Graph Fusion. To create a unified knowledge representation, we merge the multimodal knowledge graph (V, EB ) and text-based knowledge graph through entity align- ment. This process uses entity names as primary matching keys to identify semantically equivalent entities across both graph structures. The integration consolidates their representations, creating a comprehensive knowledge graph G = (V,€). This graph captures both multimodal contextual relationships and text-based semantic connections. The merged graph provides a holistic view of the document collection. This enables effective retrieval by leveraging visual-textual associations from the multimodal graph and fine-grained textual knowledge relationships from the text-based graph.\n",
      "[NarrativeText] e (ii) Dense Representation Generation. To enable efficient similarity-based retrieval, we construct a comprehensive embedding table 7 that encompasses all components generated during the indexing process. We encode dense representations for all graph entities, relationships, and atomic content chunks across modalities using an appropriate encoder. This creates a unified embedding space where each component s € entities, relations, chunks is mapped to its corresponding dense representation:\n",
      "[UncategorizedText] T =emb(s): 5 € VUEUG,, (5)\n",
      "[NarrativeText] where emb(-) denotes the embedding function tailored for each component type. Together, the unified knowledge graph G and the embedding table 7 constitute the complete retrieval index I = (G,T). This provides both structural knowledge representation and dense vector space for efficient cross-modal similarity search during the subsequent retrieval stage.\n",
      "[Title] 2.3 CROSS-MODAL HYBRID RETRIEVAL\n",
      "[NarrativeText] The retrieval stage operates on the index I = (G, T) to identify relevant knowledge components for a given user query. Traditional RAG methods face significant limitations when dealing with multimodal documents. They typically rely on semantic similarity within single modalities and fail to capture the rich interconnections between visual, mathematical, tabular, and textual elements. To address these challenges, our framework introduces a cross-modal hybrid retrieval mechanism. This mechanism leverages structural knowledge and semantic representations across heterogeneous modalities.\n",
      "[NarrativeText] Modality-Aware Query Encoding. Given a user query q, we first perform modality-aware query analysis to extract lexical cues and potential modality preferences embedded within the query. For instance, queries containing terms such as \"figure,\" \"chart,\" \"table,\" or \"equation\" provide explicit signals about the expected modality of relevant information. We then compute a unified text embedding e, using the same encoder employed during indexing, ensuring consistency between\n",
      "--- Page 6 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[NarrativeText] query and knowledge representations. This embedding-based approach enables cross-modal retrieval capabilities where textual queries can effectively access multimodal content through their shared representations, maintaining retrieval consistency while preserving cross-modal accessibility.\n",
      "[NarrativeText] Hybrid Knowledge Retrieval Architecture. Recognizing that knowledge relevance manifests through both explicit structural connections and implicit semantic relationships, we design a hybrid retrieval architecture that strategically combines two complementary mechanisms.\n",
      "[NarrativeText] e (i) Structural Knowledge Navigation. This mechanism addresses the challenge of capturing explicit relationships and multi-hop reasoning patterns. Traditional keyword-based retrieval often fails to identify knowledge connected through intermediate entities or cross-modal relationships. To overcome this limitation, we exploit the structural properties encoded within our unified knowledge graph G. We employ keyword matching and entity recognition to locate relevant graph components. The retrieval process begins with exact entity matching against query terms.\n",
      "[NarrativeText] We then perform strategic neighborhood expansion to include related entities and relationships within a specified hop distance. This structural approach proves particularly effective at uncovering high- level semantic connections and entity-relation patterns that span multiple modalities. It capitalizes on the rich cross-modal linkages established in our multimodal knowledge graph. The structural navigation yields candidate set C.,,,(q) containing relevant entities, relationships, and their associated content chunks that provide comprehensive contextual information.\n",
      "[NarrativeText] e (ii) Semantic Similarity Matching. This mechanism addresses the challenge of identifying semantically relevant knowledge that lacks explicit structural connections. While structural navigation excels at following explicit relationships, it may miss relevant content that is semantically related but not directly connected in the graph topology. To bridge this gap, we conduct dense vector similarity search between the query embedding e, and all components stored in embedding table T.\n",
      "[NarrativeText] This approach encompasses atomic content chunks across all modalities, graph entities, and relation- ship representations, enabling fine-grained semantic matching that can surface relevant knowledge even when traditional lexical or structural signals are absent. The learned embedding space captures nuanced semantic relationships and contextual similarities that complement the explicit structural signals from the navigation mechanism. This retrieval pathway returns the top-k most semantically similar chunks Cyeman(q) ranked by cosine similarity scores, ensuring comprehensive coverage of both structurally and semantically relevant knowledge.\n",
      "[NarrativeText] Candidate Pool Unification. Both retrieval pathways may return overlapping candidates with differing relevance signals. This necessitates a principled approach to unify and rank results. Retrieval candidates from both pathways are unified into a comprehensive candidate pool: C(q) = Cstru(q) U Cseman(q). Simply merging candidates would ignore distinct evidence each pathway provides. It would fail to account for redundancy between retrieved content.\n",
      "[NarrativeText] e (i) Multi-Signal Fusion Scoring. To address these challenges, we apply a sophisticated fusion scoring mechanism integrating multiple complementary relevance signals. These include structural importance derived from graph topology, semantic similarity scores from embedding space, and query- inferred modality preferences obtained through lexical analysis. This multi-faceted scoring approach ensures that final ranked candidates C*(q) effectively balance structural knowledge relationships with semantic relevance while appropriately weighting different modalities based on query characteristics.\n",
      "[NarrativeText] e (ii) Hybrid Retrieval Integration. The resulting hybrid retrieval mechanism enables our framework to leverage the complementary strengths of both knowledge graphs and dense representations. This provides comprehensive coverage of relevant multimodal knowledge for response generation.\n",
      "[Title] 2.4 FROM RETRIEVAL TO SYNTHESIS\n",
      "[NarrativeText] Effective multimodal question answering requires preserving rich visual semantics while maintaining coherent grounding across heterogeneous knowledge sources. Simple text-only approaches lose crucial visual information, while naive multimodal methods struggle with coherent cross-modal integration. Our synthesis stage addresses these challenges by systematically combining retrieved multimodal knowledge into comprehensive, evidence-grounded responses.\n",
      "[NarrativeText] e (i) Building Textual Context. Given the top-ranked retrieval candidates C*(q), we construct a structured textual context. We concatenate textual representations of all retrieved components, includ-\n",
      "--- Page 7 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[Title] Table 1: Statistics of Experimental Datasets.\n",
      "[NarrativeText] Dataset #Documents #Avg. Pages #Avg. Tokens #Doc Types # Questions DocBench 229 66 46377 5 1102 MMLongBench 135 47.5 21214 7 1082\n",
      "[NarrativeText] ing entity summaries, relationship descriptions, and chunk contents. The concatenation incorporates appropriate delimiters to indicate modality types and hierarchical origins. This approach ensures the language model can effectively parse and reason over heterogeneous knowledge components.\n",
      "[NarrativeText] e (ii) Recovering Visual Content. For multimodal chunks corresponding to visual artifacts, we perform dereferencing to recover original visual content, creating V*(q). This design maintains con- sistency with our unified embedding strategy. Textual proxies enable efficient retrieval while authentic visual content provides rich semantics necessary for sophisticated reasoning during synthesis.\n",
      "[NarrativeText] The synthesis process jointly conditions on both the assembled comprehensive textual context and dereferenced visual artifacts using a vision-language model:\n",
      "[UncategorizedText] Response = VLM(q,P(q), ¥*(2)): 6)\n",
      "[NarrativeText] where the VLM integrates information from query, textual context, and visual content. This unified conditioning enables sophisticated visual interpretation while maintaining grounding in retrieved evidence. The resulting responses are both visually informed and factually grounded.\n",
      "[Title] 3 EVALUATION\n",
      "[Title] 3.1 EXPERIMENTAL SETTINGS\n",
      "[NarrativeText] Evaluation Datasets. We conduct comprehensive evaluations on two challenging multimodal Document Question Answering (DQA) benchmarks that reflect real-world complexity and diversity. DocBench (Zou et al., 2024) provides a rigorous testbed with 229 multimodal documents spanning five critical domains: Academia, Finance, Government, Laws, and News. The dataset includes 1,102 expert-crafted question-answer pairs. These documents are notably extensive, averaging 66 pages and approximately 46,377 tokens, which presents substantial challenges for long-context understanding.\n",
      "[NarrativeText] MMLongBench (Ma et al., 2024) complements this evaluation by focusing specifically on long- context multimodal document comprehension. It features 135 documents across 7 diverse document types with 1,082 expert-annotated questions. Together, these benchmarks provide comprehensive coverage of the multimodal document understanding challenges that RAG-Anything aims to address. They ensure our evaluation captures both breadth across domains and depth in document complexity. Detailed dataset statistics and characteristics are provided in Appendix A.1.\n",
      "[NarrativeText] Baselines. We compare RAG-Anything against the following methods for performance evaluation:\n",
      "[NarrativeText] ¢ GPT-40-mini: A powerful multimodal language model with native text and image understanding capabilities. Its 128K token context window enables direct processing of entire documents. We evaluate this model as a strong baseline for long-context multimodal understanding.\n",
      "[NarrativeText] ¢ LightRAG (Guo et al., 2024): A graph-enhanced RAG system that integrates structured knowledge representation with dual-level retrieval mechanisms. It captures both fine-grained entity-relation information and broader semantic context, improving retrieval precision and response coherence.\n",
      "[ListItem] MMGraphRAG (Wan & Yu, 2025): A multimodal retrieval framework that constructs unified knowledge graphs spanning textual and visual content. This method employs spectral clustering for multimodal entity analysis and retrieves context along reasoning paths to guide generation.\n",
      "[NarrativeText] Experimental Settings. In our experiments, we implement all baselines using GPT-40-mini as the backbone LLM. Documents are parsed using MinerU (Wang et al., 2024) to extract text, im- ages, tables, and equations for downstream RAG processing. For the retrieval pipeline, we em- ploy the text-embedding-3-large model with 3072-dimensional embeddings. We use the bge-reranker-—v2-m3 model for reranking. For graph-based RAG methods, we enforce a com- bined entity-and-relation token limit of 20,000 tokens and a chunk token limit of 12,000 tokens.\n",
      "--- Page 8 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[NarrativeText] Table 2: Accuracy (%) on DocBench Dataset. Performance results with best scores highlighted in dark blue and second-best in light blue. Domain categories include Academia (Aca.), Finance (Fin.), Government (Gov.), Legal Documents (Law), and News Articles (News). Document types are categorized as Text-only (Txt.), Multimodal (Mm.), and Unanswerable queries (Una.).\n",
      "[Title] Domains Types Aca. Fin. Gov. Law. News Txt. Mm. Una.\n",
      "[Title] Method Overall\n",
      "[UncategorizedText] GPT-40-mini 40.3. 46.9 60.3 59.2 61.0 61.0 43.8 49.6 51.2 LightRAG 53.8 56.2 59.5 61.8 65.7 85.0 59.7 46.8 58.4 MMGraphRAG 64.3 52.8 64.9 40.0 61.5 67.6 66.0 60.5 61.0 RAGAnything 61.4 67.0 61.5 60.2 663 85.0 76.3 46.0 63.4\n",
      "[NarrativeText] Table 3: Accuracy (%) on MMLongBench across different domains and overall performance. Best re- sults are highlighted in dark blue and second-best in light blue.. Domain categories include Research Reports/Introductions (Res.), Tutorials/Workshops (Tut.), Academic Papers (Acad.), Guidebooks (Guid.), Brochures (Broch.), Administration/Industry Files (Admin.), and Financial Reports (Fin.).\n",
      "[Title] Method Domains Overall\n",
      "[Title] Res. Tut. Acad. Guid. Broch. Admin. Fin.\n",
      "[UncategorizedText] GPT-40-mini 35.5 44.0 246 33.1 29.5 46.8 31.1 33.5 LightRAG 40.8 341 36.2 39.4 41.0 44.4 38.3 38.9 MMGraphRAG 40.8 36.5 35.7 35.8 28.2 46.9 38.5 37.7 RAGAnything 46.6 43.5 38.7 43.9 34.0 45.7 43.6 42.8\n",
      "[NarrativeText] Outputs are constrained to a one-sentence format. For the baseline GPT-40-mini in our QA scenario, documents are concatenated into image form with a maximum of 50 pages per document, rendered at 144 dpi. Finally, all query results are evaluated for accuracy by GPT—40-mini.\n",
      "[Title] 3.2 PERFORMANCE COMPARISON\n",
      "[NarrativeText] Superior Performance and Cross-Domain Generalization. RAG-Anything demonstrates superior overall performance over baselines through its unified multimodal framework. Unlike LightRAG, which is restricted to text-only content processing, RAG-Anything treats text, images, tables, and equations as first-class entities. MMGraphRAG only adds basic image processing while treating tables and equations as plain text, missing crucial structural information. RAG-Anything introduces a comprehensive dual-graph construction strategy that preserves structural relationships across all modalities. This unified approach enables superior performance across both evaluation benchmarks.\n",
      "[NarrativeText] Enhanced Long-Context Performance. RAG-Anything demonstrates superior performance on long-context documents. The framework excels where relevant evidence is dispersed across multiple modalities and sections. It achieves the best results in information-dense domains such as Research Reports and Financial Reports on MMLongBench. These improvements stem from the structured context injection mechanism. This mechanism integrates dual-graph construction for cross-page entity alignment. It combines semantic retrieval with structural navigation. The framework also employs modality-aware processing for efficient context window utilization. Unlike baselines that cannot uniformly process diverse modalities, RAG-Anything effectively captures scattered multimodal evidence. Its cross-modal hybrid retrieval architecture combines structural knowledge navigation with semantic similarity matching. This enables the framework to leverage both explicit relationships and implicit semantic connections across modalities.\n",
      "[NarrativeText] To systematically evaluate model performance across varying document lengths, we conducted comprehensive experiments on both datasets. As illustrated in Figure 2, RAG-Anything and MM- GraphRAG exhibit comparable performance on shorter documents. However, RAG-Anything’s advantages become increasingly pronounced as document length grows. On DocBench, the perfor- mance gap expands dramatically to over 13 points for documents exceeding 100 pages (68.2% vs.\n",
      "--- Page 9 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[NarrativeText] DocBench Accuracy DocBench QA Counts MMLongBench Accuracy MMLongBench QA Counts 100. 100; = RAGAnything 350] = RAGAnything 700: <2 MMGraphrac —e MMGraphRAG s 3 s 3 pa] a > 3 id = ig = G 150; | i 3” & 3° § 300 8 100, g {200 20 20; o 50, i S00 1-10 11:50 51-100101-200 200+ © F10 11'5051-10001:200200+ 1-10 11-50 51-100101°200 200+ 0° Io 11-5051-10001-200200+ Page Range Page Range Page Range Page Range\n",
      "[NarrativeText] Figure 2: Performance evaluation across documents of varying lengths.\n",
      "[NarrativeText] Table 4: Ablation study results on DocBench. The “Chunk-only” variant bypasses dual-graph construction and relies solely on traditional chunk-based retrieval, while “w/o Reranker” eliminates cross-modal reranking but preserves the core graph-based architecture.\n",
      "[Title] Domains Types Aca. Fin. Gov. Law. News Txt. Mm. Una.\n",
      "[Title] Method Overall\n",
      "[UncategorizedText] Chunk-only 55.8 61.5 60.1 60.7 640 816 66.2 43.5 60.0 w/o Reranker 60.9 63.5 58.8 60.2 68.6 81.7 74.7 45.4 62.4 RAGAnything 61.4 67.0 61.5 60.2 663 85.0 76.3 46.0 63.4\n",
      "[NarrativeText] 54.6% for 101-200 pages; 68.8% vs. 55.0% for 200+ pages). On MMLongBench, RAG-Anything demonstrates consistent improvements across all length categories, achieving accuracy gains of 3.4 points for 11-50 pages, 9.3 points for 51-100 pages, and 7.9 points for 101-200 pages. These findings confirm that our dual-graph construction and cross-modal hybrid retrieval mechanism is particularly effective for long-document reasoning tasks.\n",
      "[Title] 3.3. ARCHITECTURAL VALIDATION WITH ABLATION STUDIES\n",
      "[NarrativeText] To isolate and quantify the contributions of key architectural components in RAG-Anything, we conducted systematic ablation studies examining two critical design choices. Given that our approach fundamentally differs from existing methods through dual-graph construction and hybrid retrieval, we specifically evaluated: i) Chunk-only, which bypasses graph construction entirely and relies solely on traditional chunk-based retrieval, and ii) w/o Reranker, which eliminates the cross-modal reranking component while preserving the core graph-based architecture.\n",
      "[NarrativeText] As demonstrated in Table 4, the results validate our architectural design through striking performance variations. e Graph Construction is Essential. The chunk-only variant achieves merely 60.0% accuracy with substantial cross-domain drops. This demonstrates that traditional chunking fails to capture structural and cross-modal relationships essential for multimodal documents. e Reranking Provides Marginal Gains. Removing the reranker yields only a modest decline to 62.4%, while the full model achieves 63.4% accuracy. This indicates that cross-modal reranking provides valuable refinement, but primary gains stem from our graph-based retrieval and cross-modal integration.\n",
      "[Title] 3.4 CASE STUDIES\n",
      "[NarrativeText] Multimodal documents contain rich structural information within each modality. Understanding these intra-modal structures is crucial for accurate reasoning. We analyze two representative cases from DocBench to demonstrate how RAG-Anything leverages these structures. These cases highlight a key limitation of existing methods. Baselines either rely on superficial textual cues or flatten complex visual elements into plain text. In contrast, RAG-Anything builds modality-aware graphs that preserve essential relationships (e.g., table header++celliounit edges; panel<>captionaxis edges). This enables precise reasoning over complex document layouts.\n",
      "[NarrativeText] e Case 1: Multi-panel Figure Interpretation. This case examines a common scenario in academic literature. Researchers often need to compare results across different experimental conditions. These results are typically presented in multi-panel visualizations. Figure 3 shows a challenging t-SNE\n",
      "--- Page 10 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[NarrativeText] ‘Question: Which model’ style space shows a separation\n",
      "[NarrativeText] between different styles according to GPT-4o-mini®: LighRAG®: According to Figure 2, the VAE According to Figure 2, the Variational Autoencoder model's style space shows a clearer (VAE) shows a clearer separation between different separation between different styles. styles in its style space compared to the\n",
      "[Title] Deterministic Autoencoder (DAE).\n",
      "[Title] MMGraphRAG@:\n",
      "[NarrativeText] According to Figure 2, the model's style space RAG-Anything(Correct®):\n",
      "[NarrativeText] shows a clearer separation between different styles The DAE model's style space shows a clearer\n",
      "[NarrativeText] in the Variational Autoencoder (VAE) compared to separation between different styles according to\n",
      "[Title] the Deterministic Autoencoder (DAE).\n",
      "[Title] Multimode! Document inthe document Figure 2\n",
      "[NarrativeText] Figure 3: Multi-panel figure interpretation case. The query requires identifying cluster separation patterns from the style-space panel, while avoiding confusion from the adjacent content-space panel.\n",
      "[NarrativeText] visualization with multiple subpanels. The query requires distinguishing between two related but distinct panels. RAG-Anything constructs a visual-layout graph where panels, axis titles, legends, and captions become nodes. Key edges encode semantic relationships. Panels contain specific plots. Captions provide contextual information. Subfigures relate hierarchically. This structure guides the retriever to focus on the style-space panel for comparing cluster separation patterns. The system avoids confusion from the adjacent content space panel. This panel shows less clear distinctions.\n",
      "[NarrativeText] juestic Vhat was Novo Nordisk's total amount sj id salaries in 2020? A a\n",
      "[NarrativeText] Q unt d salar LightRAG®: Novo Nordisk spent DKK 11,503 million ts spent on wages and salaries in on wages and salaries in 2020. a 2020 was DKK 32,928 million. : ern MMGraphRAG®: RAG-Anything(Correct®): : Novo Nordisk spent a total of 's total amount spent on wages : oa nanan 11,503 million DKK on wages and salaries in 2020 was DKK 26,778 million. Multimodel Document Evidence table in the document and salaries in 2020.\n",
      "[NarrativeText] Figure 4: Financial table navigation case. The query involves locating the specific intersection of “Wages and salaries” row and “2020” column amid similar terminological entries.\n",
      "[NarrativeText] e Case 2: Financial Table Navigation. This case addresses a common challenge in financial document analysis. Analysts must extract specific metrics from tables with similar terminology and multiple time periods. Figure 4 shows this scenario. The query involves resolving ambiguous financial terms and selecting the correct column for a specified year.\n",
      "[NarrativeText] RAG-Anything transforms the financial report table into a structured graph. Each row header, column header (year), data cell, and unit becomes a node. The edges capture key relationships: row-of, column-of, header-applies-to, and unit-of. This structure enables precise navigation. The retriever focuses on the row “Wages and salaries” and the column for “2020”. It directs attention to the target cell (26,778 million). The system successfully disambiguates nearby entries like “Share-based payments.” Competing methods treat tables as linear text. They often confuse numerical spans and years. This leads to significantly inaccurate answers. RAG-Anything explicitly models relationships within the table. It achieves precise selection and numeric grounding. This ensures accurate responses.\n",
      "[NarrativeText] e Key Insights. Both cases demonstrate how RAG-Anything’s structure-aware design delivers targeted advantages. Our approach transforms documents into explicit graph representations. These graphs capture intra-modal relationships that traditional methods miss. In figures, connections between panels, captions, and axes enable panel-level comparisons. This goes beyond keyword matching. In tables, row—column-unit graphs ensure accurate identification through modeling.\n",
      "[NarrativeText] This structure-aware retrieval design reduces confusion from repeated terminology and complex layouts. Traditional RAG systems struggle with these scenarios due to lack of structural understanding. Even MMGraphRAG fails here because it only considers image modality entities. It ignores other modality entities like table cells, row headers, and column headers. RAG-Anything’s comprehensive graph representation captures all modality-specific entities and their relationships. This enables precise, modality-specific grounding that leads to consistent improvements in document Q&A tasks requiring fine-grained localization. Additional cases are available in Appendix A.2.\n",
      "[Title] 4 RELATED WORK\n",
      "[NarrativeText] e Graph-Enhanced Retrieval-Augmented Generation. Large language models struggle with long-context inputs and multi-hop queries, failing to precisely locate dispersed evidence (Zhang et al.,\n",
      "[UncategorizedText] 10\n",
      "--- Page 11 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[NarrativeText] 2025). Graph structures address this limitation by introducing explicit relational modeling, improving both retrieval efficiency and reasoning accuracy (Bei et al., 2025).\n",
      "[NarrativeText] Since GraphRAG (Edge et al., 2024), research has evolved along two complementary directions. First, graph construction approaches optimize structures for retrieval efficiency, ranging from Ligh- tRAG’s (Guo et al., 2024) sparsified indices to neural models like GNN-RAG (Mavromatis & Karypis, 2024) and memory-augmented variants like HippoRAG (Jimenez Gutierrez et al., 2024). Second, knowledge aggregation approaches integrate information for multi-level reasoning through hier- archical methods like RAPTOR (Sarthi et al., 2024) and ArchRAG (Wang et al., 2025). Despite these advances, existing systems remain text-centric with homogeneous inputs. This limits their applicability to multimodal documents and constrains robust reasoning over heterogeneous content. RAG-Anything addresses this gap by extending GraphRAG to all modalities.\n",
      "[NarrativeText] e Multimodal Retrieval-Augmented Generation. Multimodal RAG represents a natural evolution from text-based RAG systems, addressing the need to integrate external knowledge from diverse data modalities for comprehensive response generation (Abootorabi et al., 2025). However, current approaches are fundamentally constrained by their reliance on modality-specific architectures. Exist- ing methods demonstrate these constraints across domains: VideoRAG (Ren et al., 2025) employs dual-channel architectures for video understanding while MM-VID (Lin et al., 2023) converts videos to text, losing visual information; VisRAG (Yu et al., 2025) preserves document layouts as images but misses granular relationships; MMGraphRAG (Wan & Yu, 2025) links scene graphs with textual representations but suffers from structural blindness—treating tables and formulas as plain text without proper entity extraction, losing structural information for reasoning.\n",
      "[NarrativeText] The fundamental problem underlying these limitations is architectural fragmentation. Current systems require specialized processing pipelines for each modality. This creates poor generalizability as new modalities demand custom architectures and fusion mechanisms. Such fragmentation introduces cross-modal alignment difficulties, modality biases, and information bottlenecks. These issues systematically compromise system performance and scalability. RAG-Anything addresses this fragmentation through a unified graph-based framework. Our approach processes all modalities with consistent structured modeling. This eliminates architectural constraints while preserving multimodal information integrity. The result is seamless cross-modal reasoning across heterogeneous content.\n",
      "[Title] 5 CONCLUSION\n",
      "[NarrativeText] RAG-Anything introduces a paradigm shift in multimodal retrieval through its unified graph-based framework. Our core technical innovation is the dual-graph construction strategy that seamlessly integrates cross-modal and text-based knowledge graphs. Rather than forcing diverse modalities into text-centric pipelines that lose critical structural information, our approach fundamentally reconcep- tualizes multimodal content as interconnected knowledge entities with rich semantic relationships. The hybrid retrieval mechanism strategically combines structural navigation with semantic matching, enabling precise reasoning over complex document layouts. Comprehensive evaluation demonstrates superior performance on long-context documents, particularly those exceeding 100 pages where traditional methods fail. This work establishes a new foundation for multimodal RAG systems that can handle the heterogeneous nature of diverse information landscapes.\n",
      "[NarrativeText] Our analysis in Appendix A.5 reveals critical challenges facing current multimodal RAG systems. Two fundamental issues emerge through systematic failure case examination. First, systems exhibit text-centric retrieval bias, preferentially accessing textual sources even when queries explicitly require visual information. Second, rigid spatial processing patterns fail to adapt to non-standard document layouts. These limitations manifest in cross-modal misalignment scenarios and structurally ambiguous tables. The findings highlight the need for adaptive spatial reasoning and layout-aware parsing mechanisms to handle real-world multimodal document complexity.\n",
      "[UncategorizedText] 11\n",
      "--- Page 12 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[Title] REFERENCES\n",
      "[NarrativeText] Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammad- khani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin Asgari. Ask in any modality: A comprehensive survey on multimodal retrieval-augmented genera- tion. arXiv preprint arXiv:2502.08826, 2025.\n",
      "[NarrativeText] Yuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi Chen, Sheng Zhou, Hao Chen, Yong Li, Jiajun Bu, Shirui Pan, Yizhou Yu, et al. Graphs meet ai agents: Taxonomy, progress, and future opportunities. arXiv preprint arXiv:2506.18019, 2025.\n",
      "[NarrativeText] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024.\n",
      "[NarrativeText] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval- augmented generation. arXiv preprint arXiv:2410.05779, 2024.\n",
      "[NarrativeText] Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neuro- biologically inspired long-term memory for large language models. NeurIPS, 37:59532-59569, 2024.\n",
      "[NarrativeText] Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, and Lijuan Wang. Mm-vid: Advancing video understanding with gpt-4v(ision). arXiv preprint arXiv:2310.19773, 2023.\n",
      "[NarrativeText] Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. Advances in Neural Information Processing Systems, 37:95963-96010, 2024.\n",
      "[NarrativeText] Costas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model reasoning. arXiv preprint arXiv:2405.20139, 2024.\n",
      "[NarrativeText] Xubin Ren, Lingrui Xu, Long Xia, Shuaigiang Wang, Dawei Yin, and Chao Huang. Vide- orag: Retrieval-augmented generation with extreme long-context videos. arXiv preprint arXiv:2502.01549, 2025.\n",
      "[NarrativeText] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning. Raptor: Recursive abstractive processing for tree-organized retrieval. In The Twelfth International Conference on Learning Representations, 2024.\n",
      "[NarrativeText] Xueyao Wan and Hang Yu. Mmgraphrag: Bridging vision and language with interpretable multimodal knowledge graphs. arXiv preprint arXiv:2507.20804, 2025.\n",
      "[NarrativeText] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document content extraction. arXiv preprint arXiv:2409. 18839, 2024.\n",
      "[NarrativeText] Shu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, and Yuchi Ma. Archrag: Attributed community- based hierarchical retrieval-augmented generation. arXiv preprint arXiv:2502.09891, 2025.\n",
      "[NarrativeText] Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, and Maosong Sun. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. arXiv preprint arXiv:2410.10594, 2025.\n",
      "[NarrativeText] Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen, Yilin Xiao, Chuang Zhou, Yi Chang, and Xiao Huang. A survey of graph retrieval-augmented generation for customized large language models. arXiv preprint arXiv:2501.13958, 2025.\n",
      "[NarrativeText] Anni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, and Dong Yu. Docbench: A benchmark for evaluating lIm-based document reading systems. arXiv preprint arXiv:2407.10701, 2024.\n",
      "[UncategorizedText] 12\n",
      "--- Page 13 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[Title] A APPENDIX\n",
      "[NarrativeText] This appendix provides comprehensive supporting materials for our experimental evaluation and implementation details. Section A.1 presents detailed dataset statistics for the DocBench and MMLongBench multi-modal benchmarks, including document type distributions and complexity metrics. Section A.2 showcases additional case studies that demonstrate RAG-Anything’s structure- aware capabilities across diverse multimodal content understanding tasks. Section A.3 documents the complete set of multimodal analysis prompts for vision, table, and equation processing that enable context-aware interpretation. Section A.4 provides the standardized accuracy evaluation prompt used for consistent response assessment across all experimental conditions.\n",
      "[Title] A.1 DATASET CHARACTERISTICS AND STATISTICS\n",
      "[Title] Table 5: Document type distribution and statistics for the DocBench benchmark.\n",
      "[UncategorizedText] Type Acad. Fin. Gov. Law. News # Docs 49 40 44 46 50 # Questions 303 288 = 148 191 172 Avg. Pages 11 192 69 58 1\n",
      "[Title] Table 6: Document type distribution and statistics for the MMLongBench benchmark.\n",
      "[UncategorizedText] Type Res. Tut. Acad. Guid. Broch. Admin. Fin. # Docs 34 17 26 22 15 10 11 # Questions 292 138 199 155 100 81 117 Avg. Pages 39 58 35 78 30 17 87\n",
      "[NarrativeText] Tables 5 and 6 present the distribution of document types across the DocBench and MMLong- Bench benchmarks. e DocBench encompasses medium- to long-length documents spanning various domains, including legal, governmental, and financial files. Financial reports represent the most extensive category, averaging 192 pages per document, while the News category consists of concise single-page newspapers. e MMLongBench demonstrates a broader spectrum of document types and lengths. Research reports, tutorials, and academic papers maintain moderate lengths of 35-58 pages on average, while guidebooks extend to approximately 78 pages. Brochures and administrative files remain relatively compact, whereas financial reports again emerge as the longest category.\n",
      "[NarrativeText] Collectively, these two benchmarks provide comprehensive coverage ranging from brief news arti- cles to extensive technical and financial documentation. This establishes diverse and challenging evaluation contexts for multimodal document understanding tasks.\n",
      "[Title] A.2 ADDITIONAL CASE STUDIES\n",
      "[NarrativeText] Cueto: Which GCAN submodel component moval LightRAG®: resulted in the lowest agcuracy for Twitter15 Removing the source tweet from the GCAN model attention mechanism from the resulted in the Iowest accuracy for Twitter15, as\n",
      "[NarrativeText] SA WA wR OG WC WAL GCAN sub-model resulted in the lowest accuracy for Twitter!5. MMGraphRAG@: = os ‘model resulted in the lowest accuracy for\n",
      "[NarrativeText] indicated by a significant drop in performance when using the sub-model that excluded both source tweet embeddings and dual co-attention,\n",
      "[Title] RAG-Anything(Correct’®):\n",
      "[NarrativeText] The removal of the source tweet embeddings and dua configuration \"-S-A,\" resulted in the lowest accuracy for Twitter15.\n",
      "[NarrativeText] ‘o-attention, indicated as model\n",
      "[NarrativeText] ‘Twitterl5, as indicated by signi\n",
      "[NarrativeText] Figure 4: GCA drops in the ablation analysis in Figure 4.\n",
      "[Title] ablation analysis in Accuracy\n",
      "[Title] Multimodel Document Evidence figure in the document\n",
      "[NarrativeText] Figure 5: Visual reasoning case. RAG-Anything correctly identifies \"-S-A\" as the lowest accuracy configuration, while baselines misinterpret spatial relationships.\n",
      "[NarrativeText] e Demonstrating Visual Reasoning Capabilities. Figure 5 illustrates how RAG-Anything handles complex visual reasoning tasks involving chart interpretation. The query asks which GCAN sub- model component removal yields the lowest accuracy on Twitter15. Traditional approaches struggle\n",
      "[UncategorizedText] 13\n",
      "--- Page 14 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[NarrativeText] with spatial relationships between visual elements. RAG-Anything addresses this challenge by constructing a structured graph representation of the bar plot. Bars, axis labels, and legends become interconnected nodes. These are linked by semantic relations such as bar-of and label-applies-to.\n",
      "[NarrativeText] This graph-based approach enables precise alignment between visual and textual elements. The system correctly identifies the bar labeled \"-S-A\" (removing source tweet embeddings and dual co-attention) and its corresponding accuracy value as the lowest performer. Baseline methods that flatten visual information often misinterpret spatial relationships. They frequently conflate nearby components. RAG-Anything’s structured representation preserves critical visual-textual associations. This leads to accurate query resolution and proper attribution of performance drops to \"-S-A\".\n",
      "[Title] ‘Question: Which model combination for the Evidence Inference\n",
      "[NarrativeText] dataset has the highest AUPRC value? GPT -4o-mini: LightRAG®:\n",
      "[NarrativeText] The model combination for the Evidence Inference The model combination \"BERT + LSTM - dataset with the highest AUPRC value is the BERT- Attention\" has the highest AUPRC value for the to-BERT model, achieving an AUPRC of 0.455. Evidence Inference dataset at 0.429, MMGraphRAG®: RAG-Anything(Correct®): The model combination for the Evidence Inference The model combination for the Evidence dataset with the highest AUPRC values the \"Bett-To- Jy erence dataset with the highest AUPRC\n",
      "[NarrativeText] = Bert\" model, which achieved an AUPRC score of value is GloVe + LSTM - Attention, achieving\n",
      "[Title] Multimode! Document. Evidence table in the document 0.708. a value of 0.506.\n",
      "[NarrativeText] Figure 6: Tabular navigation case. RAG-Anything locates the highest AUPRC value (0.506), while the compared approaches struggle with structural ambiguity.\n",
      "[NarrativeText] e Handling Complex Tabular Structures. Figure 6 showcases RAG-Anything’s ability to navigate intricate tabular data where structural disambiguation is crucial. The query seeks the model combi- nation achieving the highest AUPRC value for the Evidence Inference dataset—a task complicated by repeated row labels across multiple datasets within the same table. This scenario highlights a fundamental limitation of conventional approaches that struggle with structural ambiguity in data.\n",
      "[NarrativeText] RAG-Anything overcomes this by parsing the table into a comprehensive relational graph where headers and data cells become nodes connected through explicit row-of and column-of relationships. This structured representation enables the system to correctly isolate the Evidence Inference dataset context and identify \"GloVe + LSTM — Attention\" with a score of 0.506 as the optimal configuration. By explicitly preserving hierarchical table constraints that other methods often collapse or misinterpret, RAG-Anything ensures reliable reasoning across complex multi-dataset tabular structures.\n",
      "[Title] A.3> CONTEXT-AWARE MULTIMODAL PROMPTING\n",
      "[NarrativeText] These three prompts orchestrate structured, context-aware multimodal analysis with JSON-formatted outputs. They systematically guide the model to extract comprehensive descriptions of visual, tabular, and mathematical content while maintaining explicit alignment with surrounding information.\n",
      "[NarrativeText] Vision Analysis Prompt. Figure 7 orchestrates comprehensive image-context integration. The prompt directs the model to systematically capture compositional elements, object relationships, visual attributes, stylistic features, dynamic actions, and technical components (e.g., charts), while es- tablishing explicit connections to accompanying text. This approach transcends superficial description, enabling contextually-grounded interpretations that enhance knowledge retrieval and substantiation.\n",
      "[NarrativeText] Table Analysis Prompt. Figure 8 structures systematic tabular content decomposition across multiple analytical dimensions: structural organization, column semantics, critical values, statistical patterns, and contextual relevance. Through precise terminology and numerical accuracy requirements, the prompt eliminates ambiguous generalizations and ensures faithful preservation of key indicators while maintaining coherent alignment with surrounding discourse.\n",
      "[NarrativeText] Equation Analysis Prompt. Figure 9 prioritizes semantic interpretation over syntactic restatement of mathematical expressions. The prompt instructs comprehensive analysis of variable definitions, operational logic, theoretical foundations, inter-formula relationships, and practical applications. This methodology ensures mathematical content becomes integral to broader argumentative frameworks, supporting enhanced retrieval accuracy, analytical traceability, and reasoning coherence.\n",
      "[UncategorizedText] 14\n",
      "--- Page 15 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[Title] Vision Analysis Prompt\n",
      "[Title] eee vision_analysis_prompt.png\n",
      "[NarrativeText] 1 Please analyze this image in detail, considering the surrounding context. Provide a JSON response with the following structure\n",
      "[NarrativeText] \"detailed_description\": “A comprehensive and detailed visual description of the image following these guidelines:\n",
      "[Title] = Describe the overall composition and layout\n",
      "[Title] Identify all objects, people, text, and visual elements\n",
      "[NarrativeText] Explain relationships between elements and how they relate to the surrounding context\n",
      "[NarrativeText] Note colors, lighting, and visual style\n",
      "[NarrativeText] Describe any actions or activities shown\n",
      "[UncategorizedText] 12\n",
      "[Title] Include technical details if relevant (charts, diagrams, etc.)\n",
      "[NarrativeText] Reference connections to the surrounding content when relevant\n",
      "[NarrativeText] 14 - Always use specific names instead of pronouns\",\n",
      "[Title] “entity_info\": {\n",
      "[UncategorizedText] “entity_name\": \"{entity_name}\",\n",
      "[Title] “entity_type\": \"image\"\n",
      "[NarrativeText] \"summary\": “concise summary of the image content, its significance, and relationship to surrounding content (max 190 words)\"\n",
      "[UncategorizedText] }\n",
      "[UncategorizedText] }\n",
      "[NarrativeText] Context from surrounding content: {context}\n",
      "[Title] Image details: - Image Path: {image_path} + Captions: {captions}\n",
      "[ListItem] Footnotes: {footnotes}\n",
      "[NarrativeText] Focus on providing accurate, detailed visual analysis that incorporates the context and would be useful for knowledge retrieval.\n",
      "[Title] Figure 7: Vision analysis prompt for context-aware image interpretation and knowledge extraction.\n",
      "[Title] Table Analysis Prompt\n",
      "[NarrativeText] eee table_analysis_pronpt.png 1 Please analyze this table content considering the surrounding context, and provide a JSON response with the\n",
      "[NarrativeText] following structure\n",
      "[NarrativeText] “detailed_description\": “A comprehensive analysis of the table including: Table structure and organization\n",
      "[Title] Column headers and their meanings\n",
      "[Title] Key data points and patterns\n",
      "[Title] Statistical insights and trends\n",
      "[Title] Relationships between data elements\n",
      "[NarrativeText] Significance of the data presented in relation to surrounding context\n",
      "[NarrativeText] 12 - How the table supports or illustrates concepts from the surrounding content\n",
      "[ListItem] 13. Always use specific names and values instead of general references.\",\n",
      "[Title] 14 “entity_info\": {\n",
      "[UncategorizedText] “entity_name\": \"{entity_name}\",\n",
      "[UncategorizedText] “entity_type\": \"table\",\n",
      "[NarrativeText] \"summary\": “concise summary of the table's purpose, key findings, and relationship to surrounding content (max 100 words)\"\n",
      "[UncategorizedText] +\n",
      "[UncategorizedText] }\n",
      "[NarrativeText] Context from surrounding content: {context}\n",
      "[Title] Table Information:\n",
      "[Title] Image Path: {table_img_path} Caption; {table_caption} Body: {table_body} Footnotes: {table_footnote}\n",
      "[NarrativeText] Focus on extracting meaningful insights and relationships from the tabular data in the context of the surrounding content.\n",
      "[Title] Figure 8: Table analysis prompt for structured content decomposition and semantic understanding.\n",
      "[UncategorizedText] 15\n",
      "--- Page 16 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[Title] Equation Analysis Prompt\n",
      "[NarrativeText] equation_analysis_prompt.png Please analyze this mathematical equation considering the surrounding context, and provide a JSON response\n",
      "[Title] with the following structure:\n",
      "[NarrativeText] “detailed_description\": “A comprehensive analysis of the equation including: Mathematical meaning and interpretation\n",
      "[NarrativeText] Variables and their definitions in the context of surrounding content\n",
      "[NarrativeText] Mathematical operations and functions used\n",
      "[NarrativeText] Application domain and context based on surrounding material\n",
      "[Title] Physical or theoretical significance\n",
      "[NarrativeText] Relationship to other mathematical concepts mentioned in the context\n",
      "[Title] Practical applications or use cases\n",
      "[ListItem] How the equation relates to the broader discussion or framework\n",
      "[NarrativeText] Always use specific mathematical terminology\",\n",
      "[Title] “entity_info\": {\n",
      "[Title] “entity_name\": \"{entity_name}\"\n",
      "[Title] “entity_type\": \"equation\"\n",
      "[NarrativeText] \"summary\": “concise summary of the equation's purpose, significance, and role in the surrounding context (max 100 words)\"\n",
      "[UncategorizedText] }\n",
      "[UncategorizedText] }\n",
      "[NarrativeText] Context from surrounding content: {context}\n",
      "[Title] Equation Information Equation: {equation_text} Format: {equation_format}\n",
      "[NarrativeText] Focus on providing mathematical insights and explaining the equation's significance within the broader context.\n",
      "[Title] Figure 9: Equation analysis prompt for mathematical expression interpretation and integration.\n",
      "[Title] Accuracy Evaluation Prompt\n",
      "[Title] accuracy_evaluation_prompt.png\n",
      "[NarrativeText] You are an expert evaluator tasked with assessing the accuracy of answers generated by a RAG (Retrieval-Augmented Generation) system.\n",
      "[UncategorizedText] 1\n",
      "[NarrativeText] **Task**: Evaluate whether the generated answer correctly responds to the given question based on the expected answer.\n",
      "[Title] **Question**: {question} **Expected Answer**: {expected_answer}\n",
      "[Title] **Generated Answer**: {generated_answer}\n",
      "[Title] **Evaluation Criteriat*\n",
      "[ListItem] 1. **Accuracy (0 or 1)**: Does the generated answer match the factual content of the expected answer? - 1: The generated answer is factually correct and aligns with the expected answer\n",
      "[ListItem] 0: The generated answer is factually incorrect or contradicts the expected answer\n",
      "[ListItem] tInstructions**: - Focus on factual correctness, not writing style or format\n",
      "[ListItem] Consider partial matches: if the generated answer contains the correct information but includes additional context, it should still be considered accurate\n",
      "[ListItem] For numerical answers, check if the values match or are equivalent\n",
      "[ListItem] For list answers, check if all key elements are present\n",
      "[NarrativeText] If the expected answer is \"Not answerable\" and the generated answer indicates inability to answer, consider it accurate\n",
      "[Title] **Qutput Format**;\n",
      "[NarrativeText] Please respond with a JSON object containing only {\n",
      "[UncategorizedText] \"accuracy\": © or 1,\n",
      "[Title] \"reasoning\": \"Brief explanation of your evaluation\"\n",
      "[UncategorizedText] +\n",
      "[Title] Figure 10: Accuracy evaluation prompt for consistent factual assessment across question types.\n",
      "[Title] A.4. ACCURACY EVALUATION PROMPT DESIGN\n",
      "[NarrativeText] Figure 10 presents the standardized prompt specifically designed for systematic factual accuracy as- sessment of generated responses across multiple domains. The prompt establishes explicit evaluation criteria that prioritize content correctness over stylistic considerations, producing binary accuracy\n",
      "[UncategorizedText] 16\n",
      "--- Page 17 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[NarrativeText] classifications accompanied by concise analytical justifications. All accuracy evaluations throughout our comprehensive experimental framework were conducted using GPT-40-mini, ensuring consistent and reliable assessment standards across diverse question categories and specialized domains.\n",
      "[Title] A.5 CHALLENGES AND FUTURE DIRECTIONS FOR MULTI-MODAL RAG\n",
      "[NarrativeText] While current multimodal RAG systems demonstrate promising capabilities, their limitations emerge most clearly through systematic analysis of failure cases. Understanding where and why these systems break down is crucial for advancing the field beyond current performance plateaus. Examining failure patterns helps identify fundamental architectural bottlenecks and design principles for more robust multimodal systems. Our investigation reveals two critical failure patterns exposing deeper systemic issues in multimodal RAG architectures. These patterns are not merely edge cases but reflect fundamental challenges in cross-modal information integration and structural reasoning:\n",
      "[NarrativeText] e Text-Centric Retrieval Bias: Systems exhibit strong preference for textual sources, even when queries explicitly demand visual information. This reveals inadequate cross-modal attention.\n",
      "[NarrativeText] e Document Structure Processing Challenges: Systems struggle with complex layouts and non- linear information flows. This exposes limitations in spatial reasoning and contextual understanding.\n",
      "[NarrativeText] These failure modes illuminate key insights about current multimodal AI. They provide concrete directions for architectural innovations that could substantially improve system robustness.\n",
      "[NarrativeText] ‘Question: According to Figure 3, what is the sequence followed to generate the\n",
      "[NarrativeText] final labels for Named Entity Revogt GPT-4o-mini®: LightRAG®: Expected ansmer The sequence is Characters > Char Embedding > Char involves inputting words and their representations a sequence where word representations are first HRCI 2 Nogl Eeatures > Word Representation > Word LSTMEE/B = into a Word CRF layer, followed by Word LSTM-B, _ processed through a convolution layer, followed by a Word LSTM-F, and character-level representations, __Bi-LSTM layer and then a softmax or CRF layer to\n",
      "[NarrativeText] —__ der in the figure is from bottom to top) culminating in the final prediction of entity labels. produce the final labels. 3 oa = MMGraphRAG @: RAG-Anything ®:\n",
      "[NarrativeText] involves using a combined representation of word _...involves processing input characters through\n",
      "[NarrativeText] = = embeddings and character-level convolution, which is pj-directional LSTM layers (Char LSTM-B a * | then processed through a Bi-LSTM layer, followed by and Char LSTM-F) followed by a softmax or 7 a softmax or CRF layer to produce the final labels. CRF layer that produces the final labels.\n",
      "[Title] Multimodel Document [Evidence figure in the document\n",
      "[NarrativeText] Figure 11: Cross-modal noise case. All methods fail to retrieve the correct answer from the specified image, instead retrieving noisy textual evidence that misaligns with the structured visual content.\n",
      "[NarrativeText] ‘Question: Which models style space shows a clearer separation\n",
      "[NarrativeText] GPT-4o-mini®: LighRAG®: between different styles according to Figure 2? i ht RAGS\n",
      "[NarrativeText] ‘The Joint goal accuracy in the \"Train\" domain The Joint goa improved by 6.26% when using the GEM fine-tuning improved from 44,\n",
      "[Title] e “Train” domain\n",
      "[Title] from scratch to\n",
      "[NarrativeText] — nw “oiat Sa [Hors] | strategy compared to training from scratch, 50.51% with GEM fine-tuning, indicating an Entutn on Domains Faeroe! |p Paan_| | 65.35% versus 59.09% improvement of 6.27% Regn Tuan 95755825 476 Le pe eunigot | Ye tom guie nar oo, | MMGraphRAG®®: RAG-Anything®: on I new domain | GEM | 5354 96.27 | 50.69 96.42 The Joint goal ac cy in the \"Train\" domain The Joint Goal Accuracy in the \"Train\" domain = z Totes Ne Fee | Hel oe improved from 44.24% when training from scratch to improved from 44.24% with training from ewning oat | Mite | 1917 7522 aa ‘hen using the GEM fine-tuning strategy, scratch to 58.10% using the GEM fine-tuning\n",
      "[NarrativeText] em | 1993792 resulting in an improvement of 15.597 strategy, resulting in an improvement of\n",
      "[Title] Multimodel Document Evidence table in the document 13.86%.\n",
      "[NarrativeText] Figure 12: Ambiguous table structure case. All methods fail to correctly parse the confusing table layout with merged cells and unclear column boundaries, leading to incorrect data extraction.\n",
      "[NarrativeText] Case 1: Cross-Modal Misalignment. Figure || presents a particularly revealing failure scenario where all evaluated methods consistently produce incorrect answers despite having access to the necessary information. This universal failure across different architectures suggests fundamental limitations in how current systems handle noisy, heterogeneous multimodal data—a critical challenge as real-world applications inevitably involve imperfect, inconsistent information sources. The failure exposes two interconnected systemic issues that compound each other:\n",
      "[NarrativeText] Issue 1: Retrieval Bias Toward Text. Current RAG systems demonstrate pronounced bias toward textual passages. This occurs particularly when visual content lacks exact keyword matches. The bias persists even when queries contain explicit instructions to prioritize visual sources. This reveals a fundamental weakness in cross-modal attention mechanisms.\n",
      "[NarrativeText] The retrieved textual information, while topically related, often operates at a different granularity level than visual content. Images may contain precise, structured data such as specific numerical values,\n",
      "[UncategorizedText] 17\n",
      "--- Page 18 ---\n",
      "[Title] RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "[NarrativeText] detailed diagrams, or exact spatial relationships. Corresponding text typically provides general, conceptual descriptions. This semantic misalignment introduces noise that actively misleads the reasoning process. The system attempts to reconcile incompatible levels of detail and specificity.\n",
      "[NarrativeText] Issue 2: Rigid Spatial Processing Patterns. Current visual processing models exhibit fundamental rigidity in spatial interpretation. Most systems default to sequential scanning patterns—top-to- bottom and left-to-right—that mirror natural reading conventions. While effective for simple text documents, this approach creates systematic failures with structurally complex real-world content. Many documents require non-conventional processing strategies. Tables demand column-wise interpretation, technical diagrams follow specific directional flows, and scientific figures embed critical information in unexpectedly positioned annotations. These structural variations are prevalent in professional documents, making adaptive spatial reasoning essential.\n",
      "[NarrativeText] In the observed failure case, the correct answer required integrating visual elements in reverse order from the model’s default processing sequence. The system’s inability to recognize and adapt to this structural requirement led to systematic misinterpretation. This represents a fundamental architectural limitation where spatial reasoning remains static regardless of document context or query intent. When spatial processing patterns are misaligned with document structure, the extracted information becomes not merely incomplete but actively misleading. This structural noise compounds other processing errors and can lead to confident but entirely incorrect conclusions.\n",
      "[NarrativeText] Case 2: Structural Noise in Ambiguous Table Layouts. As shown in Figure 12, all methods failed when confronted with a structurally ambiguous table. The primary failure stems from the table’s confusing design: the GEM row lacks dedicated cell boundaries, and the \"Joint\" and \"Slot\" columns merge without clear separation. These structural irregularities create parsing ambiguities that system- atically mislead extraction algorithms. This failure pattern reveals a critical vulnerability in current RAG systems. When table structures deviate from standard formatting conventions—through merged cells, unclear boundaries, or non-standard layouts—extraction methods consistently misinterpret cell relationships and conflate distinct data values. This exposes the brittleness of current approaches when faced with real-world document variations that deviate from clean, structured formats.\n",
      "[NarrativeText] The case highlights two essential directions for enhancing robustness. RAG systems require layout- aware parsing mechanisms that can recognize and adapt to structural irregularities rather than imposing rigid formatting assumptions. Additionally, integrating visual processing capabilities could significantly improve noise resilience, as visual models can leverage spatial relationships and contextual design cues that are lost in purely structural representations.\n",
      "[UncategorizedText] 18\n"
     ]
    }
   ],
   "source": [
    "# 重建版面\n",
    "from operator import itemgetter\n",
    "\n",
    "# 假設 pdf_docs 是 mode=\"elements\" 的結果\n",
    "pages = {}\n",
    "\n",
    "for doc in pdf_docs:\n",
    "    page = doc.metadata.get(\"page_number\", 1)\n",
    "    coords = doc.metadata.get(\"coordinates\", {})\n",
    "    y_top = coords.get(\"points\", [(0,0)])[0][1]  # 上方座標\n",
    "    x_left = coords.get(\"points\", [(0,0)])[0][0]  # 左方座標\n",
    "\n",
    "    if page not in pages:\n",
    "        pages[page] = []\n",
    "    pages[page].append((y_top, x_left, doc.metadata.get(\"category\"), doc.page_content))\n",
    "\n",
    "# 依 y_top 排序，每頁重建版面\n",
    "for page_num, elements in pages.items():\n",
    "    print(f\"--- Page {page_num} ---\")\n",
    "    elements_sorted = sorted(elements, key=itemgetter(0, 1))\n",
    "    for y, x, cat, content in elements_sorted:\n",
    "        print(f\"[{cat}] {content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86ec43bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK ===\n",
      "\n",
      "detailed diagrams, or exact spatial relationships. Corresponding text typically provides general, conceptual descriptions. This semantic misalignment introduces noise that actively misleads the reasoning process. The system attempts to reconcile incompatible levels of detail and specificity.\n",
      "Issue 2: Rigid Spatial Processing Patterns. Current visual processing models exhibit fundamental rigidity in spatial interpretation. Most systems default to sequential scanning patterns—top-to- bottom and left-to-right—that mirror natural reading conventions. While effective for simple text documents, this approach creates systematic failures with structurally complex real-world content. Many documents require non-conventional processing strategies. Tables demand column-wise interpretation, technical diagrams follow specific directional flows, and scientific figures embed critical information in unexpectedly positioned annotations. These structural variations are prevalent in professional documents, making adaptive spatial reasoning essential.\n",
      "In the observed failure case, the correct answer required integrating visual elements in reverse order from the model’s default processing sequence. The system’s inability to recognize and adapt to this structural requirement led to systematic misinterpretation. This represents a fundamental architectural limitation where spatial reasoning remains static regardless of document context or query intent. When spatial processing patterns are misaligned with document structure, the extracted information becomes not merely incomplete but actively misleading. This structural noise compounds other processing errors and can lead to confident but entirely incorrect conclusions.\n",
      "Case 2: Structural Noise in Ambiguous Table Layouts. As shown in Figure 12, all methods failed when confronted with a structurally ambiguous table. The primary failure stems from the table’s confusing design: the GEM row lacks dedicated cell boundaries, and the \"Joint\" and \"Slot\" columns merge without clear separation. These structural irregularities create parsing ambiguities that system- atically mislead extraction algorithms. This failure pattern reveals a critical vulnerability in current RAG systems. When table structures deviate from standard formatting conventions—through merged cells, unclear boundaries, or non-standard layouts—extraction methods consistently misinterpret cell relationships and conflate distinct data values. This exposes the brittleness of current approaches when faced with real-world document variations that deviate from clean, structured formats.\n",
      "The case highlights two essential directions for enhancing robustness. RAG systems require layout- aware parsing mechanisms that can recognize and adapt to structural irregularities rather than imposing rigid formatting assumptions. Additionally, integrating visual processing capabilities could significantly improve noise resilience, as visual models can leverage spatial relationships and contextual design cues that are lost in purely structural representations.\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# 依 category 加上格式化標記，可以讓內容更接近原 PDF：\n",
    "for y, x, cat, content in elements_sorted:\n",
    "    if cat == \"Title\":\n",
    "        print(f\"\\n=== {content} ===\\n\")\n",
    "    elif cat == \"ListItem\":\n",
    "        print(f\"• {content}\")\n",
    "    else:\n",
    "        print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
